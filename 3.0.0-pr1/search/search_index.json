{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Welcome", "text": ""}, {"location": "#welcome-to-dbsnapper", "title": "Welcome to DBSnapper", "text": "<p>\ud83e\udd16 NEW in v3.0: AI-Powered Database Management</p> <p>DBSnapper v3.0 introduces comprehensive AI integration capabilities! Manage your database snapshots through natural language conversations with AI assistants like Claude Desktop and Claude Code.</p> <ul> <li>AI Integrations - Natural language database snapshot management</li> <li>Model Context Protocol (MCP) - Full integration with AI assistants</li> <li>Intelligent Operations - Auto-discovery, validation, and error resolution</li> <li>Enhanced Security - AI-assisted sanitization and configuration validation</li> </ul> <p>Explore AI Features \u2192</p> <p>Platform Integrations</p> <ul> <li>VSCode Extension - Available on the Visual Studio Marketplace for in-editor database snapshot management</li> <li>Terraform Provider - Manage DBSnapper resources with Infrastructure as Code</li> <li>GitHub Actions - Automate database snapshots in your CI/CD pipelines</li> </ul> <p>v3.0.0-pr1 Preview Release</p> <p>This is a preview release showcasing DBSnapper v3.0's AI integration capabilities. This preview includes all core functionality plus the new AI features.</p> <p> DBSnapper Architecture Overview </p> <p>DBSnapper revolutionizes the way development teams handle database snapshotting, bringing de-identified production data into the heart of development and testing workflows. It stands as a robust alternative to traditional, often cumbersome methods for creating development and test fixtures. With DBSnapper, you get to leverage real, production-grade data, stripped of its sensitive elements, to power your development and testing environments.</p>"}, {"location": "#quick-start", "title": "Quick Start", "text": "<p>Ready to get started? Here's your path to creating your first database snapshot:</p> <ol> <li>Install DBSnapper - Get the CLI tool set up</li> <li>Quick Start Guide - Create your first snapshot in minutes</li> <li>Sign up for DBSnapper Cloud - Share snapshots with your team</li> </ol>"}, {"location": "#sign-up-for-dbsnapper-cloud", "title": "Sign Up for DBSnapper Cloud", "text": "<p>Sign Up for the DBSnapper Cloud and get started with a safer, simpler way to manage your database snapshots.</p>"}, {"location": "#releases-and-integrations", "title": "Releases and Integrations", "text": "<ul> <li> <p>   The DBSnapper Agent interacts with your databases and communicates with the DBSnapper Cloud.</p> </li> <li> <p> - DBSnapper Extension for VSCode, allowing you to load database snapshots directly from your editor.</p> </li> <li> <p>   The DBSnapper Terraform Provider allows you to manage DBSnapper resources using the Terraform platform and Infrastructure as Code.</p> </li> <li> <p>   The DBSnapper GitHub Action allows you to include DBSnapper in your CI/CD pipelines for automated management of database snapshots.</p> </li> <li> <p>   DBSnapper supports Okta OIDC for Single Sign-On (SSO) authentication and group sharing.</p> </li> </ul>"}, {"location": "#key-features", "title": "Key Features", "text": "<p>DBSnapper provides comprehensive database snapshot management with these core capabilities:</p>"}, {"location": "#database-support-storage", "title": "\ud83d\uddc4\ufe0f Database Support &amp; Storage", "text": "<ul> <li>PostgreSQL and MySQL databases with more engines coming soon</li> <li>Bring Your Own Storage - Use Amazon S3, Cloudflare R2, or your preferred cloud storage</li> <li>Presigned URLs for secure upload and download operations</li> </ul>"}, {"location": "#security-compliance", "title": "\ud83d\udd12 Security &amp; Compliance", "text": "<ul> <li>Data Sanitization - Remove or mask sensitive information</li> <li>SSO Integration - Okta OIDC support with group-based sharing</li> <li>Private-cloud First - Your data stays in your infrastructure</li> </ul>"}, {"location": "#ai-integration-new-in-v30", "title": "\ud83e\udd16 AI Integration (New in v3.0)", "text": "<ul> <li>Natural Language Interface - Manage snapshots through AI conversations</li> <li>Model Context Protocol (MCP) - Full integration with Claude Desktop/Code</li> <li>Intelligent Discovery - AI-powered database tool configuration</li> <li>Smart Validation - AI-assisted configuration and error resolution</li> </ul>"}, {"location": "#developer-experience", "title": "\u2699\ufe0f Developer Experience", "text": "<ul> <li>Zero-Config Operation - Complex operations in a single command</li> <li>Terraform Provider - Infrastructure as Code support</li> <li>GitHub Actions - CI/CD pipeline integration</li> <li>VSCode Extension - In-editor snapshot management</li> <li>Docker-Enabled - Leverages containerization for database tools</li> </ul>"}, {"location": "#advanced-capabilities", "title": "\ud83d\udcca Advanced Capabilities", "text": "<ul> <li>Database Subsetting - Create smaller, relationally-complete snapshots</li> <li>Ephemeral Sanitization - No need for temporary databases</li> <li>Templating Engine - Environment variable support for sensitive data</li> </ul>"}, {"location": "#core-capabilities", "title": "Core Capabilities", "text": ""}, {"location": "#subsetting-smaller-snapshots-relationally-complete", "title": "\ud83d\udce6 Subsetting - Smaller snapshots, relationally complete", "text": "<p>Create smaller, more manageable relationally-complete snapshots of your database. Perfect for development and testing when you only need a subset of production data.</p> <p>Key Benefits:</p> <ul> <li>Efficient Data Management - Work with relevant data only, reducing resource usage</li> <li>Maintains Data Integrity - Preserves relational structure and referential integrity</li> <li>Customizable Criteria - Define specific tables, rows, or data sets to include</li> </ul>"}, {"location": "#snapshotting-simplified-database-backups", "title": "Snapshotting - Simplified database backups", "text": "<p>DBSnapper offers an efficient and powerful solution for snapshotting databases that simplifies the snapshotting process for different database platforms.</p> <p>DBSnapper's snapshotting capability is not just about capturing data; it's a strategic tool that integrates into and enhances the entire software development lifecycle. From creating realistic test environments and aiding in AI model training to providing essential support in CI/CD pipelines, DBSnapper stands as an indispensable asset for any software development team aiming to streamline and improve their database management and utilization in a modern startup environment.</p> <ul> <li> <p>Real-world Test Cases: Utilizing de-identified data snapshots, you can create more effective and realistic test cases. This helps in identifying potential issues in a more accurate production-like environment.</p> </li> <li> <p>Seamless Integration with CI/CD Pipelines: DBSnapper can be easily integrated into CI/CD pipelines such as GitHub Actions, automating the process of generating snapshots for your team and ensuring the team is using the latest and most accurate data for testing.</p> </li> <li> <p>Training AI Models: For AI and machine learning initiatives, having access to diverse, real-world data sets is crucial. DBSnapper's ability to provide de-identified snapshots of real operational data can significantly enhance the training process of AI models, leading to more accurate models.</p> </li> </ul>"}, {"location": "#sanitization-de-identification-and-sensitive-data-removal", "title": "Sanitization - De-identification and sensitive data removal", "text": "<p>DBSnapper enables you to de-identify and sanitize your production data, removing sensitive information such as personal details, financial data, and other confidential information. This is crucial for ensuring compliance with data protection regulations and maintaining the privacy and security of your users.</p> <ul> <li> <p>Data Provenance During Sanitization: The DBSnapper tools are designed to give you full control over your data, ensuring that no sensitive or proprietary data leaves your environment.</p> </li> <li> <p>Adherence to GDPR and Other Regulations: In the era of stringent data protection laws like the GDPR, CCPA, and others, DBSnapper's sanitization feature ensures that your data handling practices are compliant, reducing the risk of legal complications and hefty fines.</p> </li> <li> <p>Maintaining Data Utility Post-Sanitization: Despite the removal of sensitive data, the utility and integrity of the dataset are preserved, making it suitable for development, testing, and analysis without compromising privacy.</p> </li> </ul>"}, {"location": "#share-securely-distribute-snapshots-via-dbsnapper-cloud", "title": "Share - Securely distribute snapshots via DBSnapper Cloud", "text": "<p>The sharing aspect of DBSnapper is made possible through the DBSnapper Cloud, a critical feature for secure storage and distribution of database snapshots. It's designed for seamless collaboration within your team or for integration with automated processes.</p> <ul> <li> <p>SSO-Aware Team Sharing: DBSnapper Cloud supports Single Sign-On (SSO), and is SSO-Group aware, allowing you to easily share snapshots with your team members, using the groups you've already set up in your SSO provider.</p> </li> <li> <p>Flexibility of Storage Choices: With DBSnapper, you have the flexibility to 'Bring Your Own Cloud Storage Provider'. This means you can choose the cloud storage that best aligns with your company\u2019s policies and data management strategies, ensuring that your data remains within your approved PaaS vendor.</p> </li> <li> <p>Easy Access for Team Members: Shared snapshots are easily accessible to authorized team members. This facilitates collaboration, as team members can work with the same datasets in a synchronized manner.</p> </li> <li> <p>Integration with Automated Processes: The DBSnapper Cloud is designed for integration with automated processes, such as CI/CD pipelines, making it simpler to incorporate database snapshots into your development and deployment workflows.</p> </li> </ul>"}, {"location": "ai-integrations/", "title": "AI Integrations", "text": "<p>New in v3.0.0-pr1 Preview</p> <p>DBSnapper v3.0.0-pr1 introduces AI integration capabilities through Model Context Protocol (MCP) support. This preview release demonstrates the core AI functionality with comprehensive MCP server integration for AI assistants.</p>"}, {"location": "ai-integrations/#overview", "title": "Overview", "text": "<p>DBSnapper's AI integration capabilities enable developers and teams to interact with their database snapshots, targets, and configurations through natural language conversations with AI assistants. Instead of memorizing command-line syntax, you can simply tell your AI assistant what you want to accomplish.</p> <p>Key Benefits:</p> <ul> <li>Natural Language Interface: Interact with DBSnapper using plain English</li> <li>Intelligent Discovery: AI assistants can help discover and configure database tools automatically</li> <li>Context-Aware Operations: AI can understand your workflow and suggest appropriate actions</li> <li>Enhanced Productivity: Faster snapshot operations through conversational interfaces</li> <li>Error Prevention: AI validation helps catch configuration issues before they cause problems</li> </ul>"}, {"location": "ai-integrations/#model-context-protocol-mcp-integration", "title": "Model Context Protocol (MCP) Integration", "text": "<p>The Model Context Protocol provides a standardized way for AI applications to access external tools and data sources. DBSnapper's MCP server exposes comprehensive functionality through this protocol.</p>"}, {"location": "ai-integrations/#supported-ai-applications", "title": "Supported AI Applications", "text": "<ul> <li>Claude Desktop - Anthropic's desktop AI application</li> <li>Claude Code - Anthropic's VS Code-integrated AI assistant  </li> <li>Any MCP-compatible AI client - The standard protocol ensures broad compatibility</li> </ul>"}, {"location": "ai-integrations/#core-capabilities", "title": "Core Capabilities", "text": ""}, {"location": "ai-integrations/#target-management", "title": "\ud83c\udfaf Target Management", "text": "<ul> <li>List and explore all configured database targets</li> <li>Get detailed information about specific targets including snapshots</li> <li>Safely update target configurations with validation</li> <li>Access sanitized configuration data</li> </ul>"}, {"location": "ai-integrations/#snapshot-operations", "title": "\ud83d\udcf8 Snapshot Operations", "text": "<ul> <li>Create new database snapshots with natural language commands</li> <li>Load snapshots into target databases</li> <li>Browse available snapshots with rich metadata</li> <li>Access snapshot creation history and statistics</li> </ul>"}, {"location": "ai-integrations/#sanitization-security", "title": "\ud83d\udd12 Sanitization &amp; Security", "text": "<ul> <li>Create sanitized snapshots with sensitive data removed</li> <li>Validate configurations including sanitization queries</li> <li>Access sanitization templates for common use cases</li> <li>Real-time validation with detailed error reporting</li> </ul>"}, {"location": "ai-integrations/#database-tools-management", "title": "\ud83d\udee0\ufe0f Database Tools Management", "text": "<ul> <li>Automatically discover database client tools (pg_dump, mysql, etc.)</li> <li>Validate database tools configuration</li> <li>Resolve PATH and configuration issues automatically</li> </ul>"}, {"location": "ai-integrations/#quick-start", "title": "Quick Start", "text": "<ol> <li>Install DBSnapper v3.0+ with MCP support</li> <li>Configure your AI application to use the DBSnapper MCP server  </li> <li>Start conversing with your AI assistant about database operations</li> </ol>"}, {"location": "ai-integrations/#configuration-example", "title": "Configuration Example", "text": "<p>For Claude Desktop/Code, add to your configuration:</p> <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"dbsnapper\",\n      \"args\": [\"mcp\"],\n      \"description\": \"DBSnapper database snapshot management\"\n    }\n  }\n}\n</code></pre>"}, {"location": "ai-integrations/#example-conversations", "title": "Example Conversations", "text": ""}, {"location": "ai-integrations/#getting-started", "title": "Getting Started", "text": "<pre><code>You: \"What database targets do I have configured?\"\n\nAI: I'll check your configured database targets.\n\nYou have 3 targets configured:\n- production-db (PostgreSQL, cloud target)\n- staging-db (PostgreSQL, local target)  \n- analytics-db (MySQL, local target)\n</code></pre>"}, {"location": "ai-integrations/#creating-and-managing-snapshots", "title": "Creating and Managing Snapshots", "text": "<pre><code>You: \"Create a sanitized snapshot of my production database for development\"\n\nAI: I'll create a sanitized snapshot of your production database that's safe for development use.\n\n\u2705 Sanitized snapshot created successfully!\n- Sanitized snapshot: 1705123456_production-db.san.zip\n- Original size: 2.1 GB, Sanitized size: 1.8 GB  \n- PII data removed, email addresses anonymized\n- Ready for development team use\n</code></pre>"}, {"location": "ai-integrations/#configuration-validation", "title": "Configuration Validation", "text": "<pre><code>You: \"Check if all my database configurations are valid\"\n\nAI: I'll validate your complete DBSnapper configuration.\n\n\u2705 Configuration validation completed!\n\nConfiguration Validation: PASSED\n  Target: production-db: PASS\n  Target: staging-db: PASS\n  Target: analytics-db: PASS\n  Global Configuration: PASS\n\nAll targets and configurations are valid and ready for use.\n</code></pre>"}, {"location": "ai-integrations/#tool-discovery", "title": "Tool Discovery", "text": "<pre><code>You: \"My snapshots are failing - can you help fix the database tools?\"\n\nAI: I'll discover and configure your database tools automatically.\n\n\u2705 Database tools discovered and configured!\n- PostgreSQL tools: /opt/homebrew/bin/pg_dump, pg_restore, psql\n- MySQL tools: /opt/homebrew/bin/mysql, mysqldump\n- Configuration saved permanently\n- All tools validated and ready for use\n</code></pre>"}, {"location": "ai-integrations/#available-mcp-tools", "title": "Available MCP Tools", "text": ""}, {"location": "ai-integrations/#target-management_1", "title": "Target Management", "text": "<ul> <li><code>list_targets</code> - List all configured database targets</li> <li><code>get_target_details</code> - Get detailed target information with snapshots</li> <li><code>update_target</code> - Safely update target configurations (dry-run supported)</li> </ul>"}, {"location": "ai-integrations/#snapshot-operations_1", "title": "Snapshot Operations", "text": "<ul> <li><code>build_snapshot</code> - Create new database snapshots</li> <li><code>load_snapshot</code> - Load snapshots into target databases</li> <li><code>list_snapshots</code> - Browse available snapshots with metadata</li> </ul>"}, {"location": "ai-integrations/#sanitization-security_1", "title": "Sanitization &amp; Security", "text": "<ul> <li><code>sanitize_snapshot</code> - Create sanitized snapshots with sensitive data removed</li> <li><code>validate_config</code> - Validate targets, queries, and database connections</li> </ul>"}, {"location": "ai-integrations/#database-tools", "title": "Database Tools", "text": "<ul> <li><code>discover_database_tools</code> - Auto-discover and configure database client tools</li> <li><code>validate_database_tools</code> - Validate database tools configuration</li> <li><code>get_config_info</code> - Get comprehensive configuration information</li> </ul>"}, {"location": "ai-integrations/#mcp-resources", "title": "MCP Resources", "text": "<p>DBSnapper provides rich contextual information through MCP resources:</p>"}, {"location": "ai-integrations/#configuration-resources", "title": "Configuration Resources", "text": "<ul> <li><code>dbsnapper://config</code> - Sanitized configuration including targets and profiles</li> <li><code>dbsnapper://targets</code> - Target list with summary statistics</li> <li><code>dbsnapper://validation-results</code> - Real-time validation status</li> </ul>"}, {"location": "ai-integrations/#snapshot-resources", "title": "Snapshot Resources", "text": "<ul> <li><code>dbsnapper://snapshots</code> - Comprehensive snapshot information with metadata</li> <li><code>dbsnapper://build-history</code> - Snapshot creation history with success/failure details</li> </ul>"}, {"location": "ai-integrations/#sanitization-resources", "title": "Sanitization Resources", "text": "<ul> <li><code>dbsnapper://sanitization-queries</code> - Query configuration with execution status</li> <li><code>dbsnapper://query-templates</code> - Common sanitization templates (PII, GDPR, etc.)</li> </ul>"}, {"location": "ai-integrations/#available-in-v300-pr1", "title": "Available in v3.0.0-pr1", "text": "<p>\u2705 Model Context Protocol (MCP) Server - Full MCP server implementation with HTTP and SSE transport - Complete tool exposure for all core DBSnapper operations - Resource access for configuration and snapshot data - Compatible with Claude Desktop, Claude Code, and other MCP clients</p> <p>\u2705 Core AI Operations - Target management through natural language - Snapshot creation, loading, and sanitization  - Configuration validation and database tool discovery - Real-time error handling and troubleshooting assistance</p>"}, {"location": "ai-integrations/#roadmap-future-ai-capabilities", "title": "Roadmap: Future AI Capabilities", "text": ""}, {"location": "ai-integrations/#full-v30-release-enhanced-ai-features", "title": "Full v3.0 Release - Enhanced AI Features", "text": "<ul> <li>Intelligent Workflow Automation - AI-driven snapshot scheduling and management</li> <li>Smart Query Generation - AI-assisted sanitization query creation</li> <li>Anomaly Detection - AI-powered monitoring of snapshot operations</li> <li>Performance Optimization - AI recommendations for snapshot and database performance</li> </ul>"}, {"location": "ai-integrations/#v31-advanced-analytics-integrations", "title": "v3.1+ - Advanced Analytics &amp; Integrations", "text": "<ul> <li>Database Insights - AI analysis of database structure and content patterns</li> <li>Usage Analytics - Intelligent reporting on snapshot usage and patterns  </li> <li>Platform Integrations - GitHub Copilot, Slack/Teams bots, CI/CD pipelines</li> <li>Custom AI Agents - Build domain-specific AI assistants for your database workflows</li> </ul>"}, {"location": "ai-integrations/#getting-started_1", "title": "Getting Started", "text": "<p>Ready to embrace AI-powered database management? Here's how to get started:</p> <ol> <li>Install DBSnapper v3.0+ with the latest AI features</li> <li>Follow the MCP Setup Guide for detailed configuration</li> <li>Explore Example Workflows to see AI integration in action</li> <li>Join the Community to share feedback and use cases</li> </ol>"}, {"location": "ai-integrations/#technical-deep-dive", "title": "Technical Deep Dive", "text": "<p>For developers interested in the technical implementation:</p> <ul> <li>MCP Command Reference - Complete technical documentation and setup guide</li> <li>GitHub Repository - Source code and technical details</li> <li>Contributing Guide - Help build the future of AI-powered database tools</li> </ul> <p>Ready to Experience AI-Powered Database Management?</p> <p>DBSnapper v3.0's AI integration represents just the beginning. We're building toward a future where managing database snapshots is as natural as having a conversation. Get started today and be part of the AI-powered database revolution.</p>"}, {"location": "configuration/", "title": "Configuration Settings", "text": "<p>DBSnapper's flexible configuration system adapts to your workflow, from simple local development to enterprise-scale team collaboration. This guide helps you get started quickly and provides comprehensive reference for advanced features.</p>"}, {"location": "configuration/#getting-started-with-configuration", "title": "Getting Started with Configuration", "text": ""}, {"location": "configuration/#initialize-your-configuration", "title": "Initialize Your Configuration", "text": "<p>The fastest way to get started is to let DBSnapper create a configuration file for you:</p> <pre><code># Create configuration file with sensible defaults\ndbsnapper config init\n\n# Verify your setup\ndbsnapper config check\n</code></pre> <p>This creates a configuration file at <code>~/.config/dbsnapper/dbsnapper.yml</code> with everything you need to get started.</p>"}, {"location": "configuration/#configuration-file-locations", "title": "Configuration File Locations", "text": "<p>DBSnapper looks for configuration in this priority order:</p> <ol> <li><code>./dbsnapper.yml</code> (current directory - perfect for project-specific configs)</li> <li><code>~/.config/dbsnapper/dbsnapper.yml</code> (user config directory - most common)</li> <li>Environment variables (see Environment Variables - great for CI/CD)</li> </ol>"}, {"location": "configuration/#your-first-target", "title": "Your First Target", "text": "<p>After initialization, add your first database target:</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code># Auto-generated by 'dbsnapper config init'\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: ~/.dbsnapper\n\n# Add your database targets here\ntargets:\n  my_app:\n    snapshot:\n      src_url: \"postgresql://user:pass@localhost:5432/production_db\"\n      dst_url: \"postgresql://user:pass@localhost:5432/dev_db\"\n</code></pre> <p>Ready to test? Run <code>dbsnapper targets</code> to verify your configuration and database connectivity.</p>"}, {"location": "configuration/#common-configuration-patterns", "title": "Common Configuration Patterns", "text": "\ud83d\ude80 Local Development\u2601\ufe0f Team with Cloud Storage\ud83c\udfe2 Enterprise with SSO <p>Perfect for individual developers working with local databases:</p> <pre><code>secret_key: your-32-char-key\n\ntargets:\n  local_app:\n    snapshot:\n      src_url: \"postgresql://postgres:postgres@localhost:5432/myapp\"\n      dst_url: \"postgresql://postgres:postgres@localhost:5432/myapp_dev\"\n</code></pre> <p>Enable team collaboration with cloud storage and sanitization:</p> <pre><code>secret_key: your-32-char-key\nauthtoken: your-dbsnapper-cloud-token\n\nstorage_profiles:\n  team_s3:\n    provider: s3\n    bucket: team-snapshots\n    awscli_profile: production\n\ntargets:\n  production_app:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod.db:5432/app\"\n      dst_url: \"postgresql://user:pass@dev.db:5432/app_dev\"\n    storage_profile: team_s3\n    sanitize:\n      query_file: \"remove_pii.sql\"\n</code></pre> <p>Full enterprise setup with SSO and team sharing:</p> <pre><code>secret_key: your-32-char-key\nauthtoken: your-dbsnapper-cloud-token\n\n# SSO Integration\nsso:\n  okta:\n    provider_url: https://your-org.okta.com\n    client_id: your-client-id\n    redirect_url: http://localhost:8080/callback\n\n# Performance optimization\ndefaults:\n  cpus: 4  # Use 4 cores for all operations\n\nstorage_profiles:\n  production_s3:\n    provider: s3\n    bucket: prod-snapshots\n    awscli_profile: production\n\n  sanitized_s3:\n    provider: s3  \n    bucket: sanitized-snapshots\n    awscli_profile: team-access\n\ntargets:\n  user_database:\n    cpus: 8  # Override for this large database\n    snapshot:\n      src_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@prod:5432/users\"\n      dst_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@dev:5432/users_dev\"\n    storage_profile: production_s3\n    sanitize:\n      storage_profile: sanitized_s3\n      override_query: \"UPDATE users SET email = CONCAT('user', id, '@example.com')\"\n    # Share with development and QA teams\n    sso_groups: [\"developers\", \"qa-team\"]\n</code></pre>"}, {"location": "configuration/#key-configuration-concepts", "title": "Key Configuration Concepts", "text": ""}, {"location": "configuration/#security-encryption", "title": "\ud83d\udd11 Security &amp; Encryption", "text": "<ul> <li><code>secret_key</code> - 32-character hex string for encrypting sensitive data (auto-generated by <code>config init</code>)</li> <li><code>authtoken</code> - Your DBSnapper Cloud authentication token (optional, enables team features)</li> <li>Environment variable templates - Use <code>{{</code>VAR_NAME<code>| env}}</code> to avoid storing credentials in config files</li> </ul>"}, {"location": "configuration/#performance-configuration", "title": "\u26a1 Performance Configuration", "text": "<p>New in v3.0: Configure CPU usage for optimal performance:</p> <pre><code># Global default (applies to all targets)\ndefaults:\n  cpus: 4\n\n# Per-target override for large databases  \ntargets:\n  large_db:\n    cpus: 8  # Use more cores for this target\n</code></pre>"}, {"location": "configuration/#database-targets", "title": "\ud83d\uddc4\ufe0f Database Targets", "text": "<p>A target defines a complete snapshot workflow:</p> <ul> <li><code>src_url</code> - Source database (where to snapshot from)</li> <li><code>dst_url</code> - Destination database (where to load snapshots)</li> <li>Storage profiles - Where to store snapshots in the cloud</li> <li>Sanitization rules - How to remove sensitive data</li> <li>Schema filtering - Which database schemas to include (PostgreSQL only)</li> </ul>"}, {"location": "configuration/#storage-profiles", "title": "\u2601\ufe0f Storage Profiles", "text": "<p>Configure cloud storage for team sharing and backup:</p> <pre><code>storage_profiles:\n  my_s3:\n    provider: s3\n    bucket: my-snapshots\n    awscli_profile: default  # Use AWS CLI credentials\n</code></pre> <p>Supported providers: Amazon S3, Cloudflare R2, MinIO, DigitalOcean Spaces, and other S3-compatible services.</p>"}, {"location": "configuration/#need-more-detail", "title": "Need More Detail?", "text": "<p>This getting started section covers the most common configurations. For comprehensive reference documentation, continue reading below, or jump to specific sections:</p> <ul> <li>Environment Variables - Configure via environment variables</li> <li>Database Targets - Complete target configuration options</li> <li>Storage Profiles - Cloud storage setup</li> <li>Sanitization - Data privacy and security</li> <li>SSO Integration - Team authentication setup</li> </ul>"}, {"location": "configuration/#complete-configuration-reference", "title": "Complete Configuration Reference", "text": "<p>Complete Configuration Example</p> <pre><code># Core Settings\nauthtoken: jREZkinFQpSJb4TWZAKioHuCD7KG2GV3xZqiUwbZfeJVTS7V\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: ~/.dbsnapper\nenv: development\ndebug: true\ntrace: false\n\n\n# Docker Configuration\ndocker:\n  images:\n    mysql: mysql:9\n    postgres: postgres:latest\n\n# Global Overrides\noverride:\n  san_query: RFJPUCBUQUJMRSBJRiBFWElTVFMgZGJzbmFwcGVyX2luZm87...\n  dst_db_url: postgres://postgres:postgres@localhost:15432/override_db\n\n# Default Settings\ndefaults:\n  shared_target_dst_db_url: postgres://postgres:postgres@localhost:15432/shared_default\n\n# SSO Configuration\nsso:\n  okta:\n    provider_url: https://dev-89837244.okta.com\n    client_id: 0oag95e2z5BhLZ5AI5d7\n    redirect_url: http://localhost:8080/callback\n\n# Storage Profiles\nstorage_profiles:\n  aws-s3-prod:\n    provider: s3\n    awscli_profile: default\n    region: us-west-2\n    bucket: dbsnapper-production\n    prefix: snapshots\n\n  cloudflare-r2:\n    provider: r2\n    awscli_profile: r2_profile\n    bucket: dbsnapper-r2\n    prefix: sanitized\n\n# Target Definitions\ntargets:\n  myapp-prod:\n    name: \"Production Database\"\n    snapshot:\n      src_url: postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@prod-host:5432/myapp\n      dst_url: postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@dev-host:5432/myapp_dev\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n        exclude_schemas: [\"temp_logs\"]\n    storage_profile: aws-s3-prod\n\n    sanitize:\n      dst_url: postgres://user:pass@localhost:5432/myapp_sanitized\n      query_file: sanitize.sql\n      override_query: \"UPDATE users SET email = 'user@example.com';\"\n\n    subset:\n      src_url: postgres://user:pass@prod-host:5432/myapp\n      dst_url: postgres://user:pass@localhost:5432/myapp_subset\n      subset_tables:\n        - table: public.users\n          where: \"created_at &gt; '2023-01-01'\"\n        - table: public.orders\n          percent: 20\n      copy_tables:\n        - public.settings\n      excluded_tables:\n        - public.temp_data\n      added_relationships:\n        - fk_table: public.orders\n          fk_columns: user_id\n          ref_table: public.users\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: public.audit_log\n          ref_table: public.users\n\n  shared-team-target:\n    name: \"Team Shared Target\"\n    share:\n      dst_url: postgres://user:pass@localhost:5432/team_shared\n      storage_profile_name: cloudflare-r2\n</code></pre>"}, {"location": "configuration/#configuration-sections", "title": "Configuration Sections", "text": ""}, {"location": "configuration/#global-settings", "title": "Global Settings", "text": ""}, {"location": "configuration/#core-settings", "title": "Core Settings", "text": "<pre><code># Required: Encryption key for sensitive data\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\n\n# Optional: Authentication token for DBSnapper Cloud\nauthtoken: jREZkinFQpSJb4TWZAKioHuCD7KG2GV3...\n\n# Optional: Local storage location (default: ~/.dbsnapper)\nworking_directory: ~/.dbsnapper\n\n# Optional: Environment mode (development/staging/production)\nenv: development\n</code></pre> <p>Core Settings Details:</p> <code>secret_key</code> (required string[32]) 16-byte hexadecimal string (32 characters) used for encrypting sensitive configuration data. Generated automatically with <code>dbsnapper config init</code>. <code>authtoken</code> (optional string) Authentication token for DBSnapper Cloud integration. Enables cloud targets, storage profiles, and team sharing features. <code>working_directory</code> (optional string) Directory for storing local snapshots and temporary files. Defaults to <code>~/.dbsnapper</code>. <code>env</code> (optional string) Environment mode that configures API endpoints automatically: - <code>development</code> \u2192 <code>http://app.dbsnapper.local:3000/api/v3</code> - <code>staging</code> \u2192 <code>https://stg4777.dbsnapper.com/api/v3</code> - <code>production</code> \u2192 <code>https://app.dbsnapper.com/api/v3</code>"}, {"location": "configuration/#docker-configuration", "title": "Docker Configuration", "text": "<p>Configure Docker images for database engines:</p> <pre><code>docker:\n  images:\n    mysql: mysql:9\n    postgres: postgres:latest\n</code></pre> <p>Supported Database Images:</p> <ul> <li><code>mysql</code>: MySQL database engine</li> <li><code>postgres</code>: PostgreSQL database engine</li> </ul>"}, {"location": "configuration/#environment-variables", "title": "Environment Variables", "text": "<p>DBSnapper supports comprehensive environment variable configuration using the <code>DBSNAPPER_</code> prefix. Convert configuration keys by replacing periods with double underscores <code>__</code>.</p>"}, {"location": "configuration/#common-environment-variables", "title": "Common Environment Variables", "text": "<pre><code># Core Settings\nDBSNAPPER_SECRET_KEY=c614a689a559d1b517c28a5e4fcdc059\nDBSNAPPER_AUTHTOKEN=jREZkinFQpSJb4TWZAKioHuCD7KG2GV3...\nDBSNAPPER_WORKING_DIRECTORY=/opt/dbsnapper\nDBSNAPPER_ENV=production\n\n# Automation &amp; CI\nDBSNAPPER_NO_CONFIRM=true          # Skip confirmation prompts (i.e. when loading snapshots)\n\n# Configuration Overrides\nDBSNAPPER_OVERRIDE__SAN_QUERY=RFJPUCBUQUJMRSBJRi...\nDBSNAPPER_OVERRIDE__DST_DB_URL=postgres://user:pass@localhost:5432/override_db\nDBSNAPPER_DEFAULTS__SHARED_TARGET_DST_DB_URL=postgres://user:pass@localhost:5432/shared_default\n\n# Docker Images\nDBSNAPPER_DOCKER__IMAGES__POSTGRES=postgres:16-alpine\nDBSNAPPER_DOCKER__IMAGES__MYSQL=mysql:8-oracle\n</code></pre>"}, {"location": "configuration/#configuration-priority", "title": "Configuration Priority", "text": "<p>Environment variables take precedence over configuration file settings, allowing for deployment-specific overrides without modifying configuration files.</p>"}, {"location": "configuration/#url-templates", "title": "URL Templates", "text": "<p>DBSnapper supports Go template syntax in all connection URLs, enabling dynamic configuration with environment variables:</p> <pre><code>targets:\n  dynamic-target:\n    snapshot:\n      src_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@{{`DB_HOST` | env}}:5432/{{`DB_NAME` | env}}\"\n      dst_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@localhost:5432/{{`DB_NAME` | env}}_snapshot\"\n</code></pre> <p>Template Functions:</p> <ul> <li><code>{{</code>ENV_VAR<code>| env}}</code> - Substitute environment variable value</li> <li><code>{{</code>CONSTANT<code>}}</code> - Use literal string value</li> </ul> <p>Supported URL Fields:</p> <ul> <li><code>snapshot.src_url</code> and <code>snapshot.dst_url</code></li> <li><code>sanitize.dst_url</code></li> <li><code>subset.src_url</code> and <code>subset.dst_url</code></li> <li><code>share.dst_url</code></li> </ul>"}, {"location": "configuration/#targets", "title": "Targets", "text": "<p>Targets define database connections and operations. Each target can support multiple operations: snapshot, sanitize, subset, and share.</p>"}, {"location": "configuration/#basic-target-configuration", "title": "Basic Target Configuration", "text": "<pre><code>targets:\n  myapp-prod:\n    name: \"Production Database\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod-host:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev-host:5432/myapp_dev\" # Will be created if it doesn't exist or overwritten if it does.\n    storage_profile: aws-s3-production\n</code></pre>"}, {"location": "configuration/#schema-configuration", "title": "Schema Configuration", "text": "<p>PostgreSQL Only Feature: Control which database schemas are included in snapshots. Schema filtering is only supported for PostgreSQL databases - MySQL uses standard dump without filtering.</p> <pre><code>targets:\n  postgres-target:\n    name: \"PostgreSQL with Schema Filtering\"\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/myapp\"\n      dst_url: \"postgresql://user:pass@host:5432/myapp_snap\"\n      schema_config:\n        # Option 1: Include only specific schemas\n        include_schemas: [\"public\", \"app_data\", \"reports\"]\n\n        # Option 2: Exclude specific schemas (alternative to include_schemas)\n        # exclude_schemas: [\"temp_logs\", \"analytics\", \"debug\"]\n\n        # Option 3: Control default behavior\n        use_default_schema: false # true = default schema only, false = all schemas\n</code></pre> <p>Schema Configuration Behavior:</p> Configuration Build Behavior Load Behavior No <code>schema_config</code> Builds <code>public</code> schema only Loads <code>public</code> schema only <code>use_default_schema: true</code> Builds default schema (<code>public</code>) Loads default schema from snapshot <code>use_default_schema: false</code> Builds all available schemas Loads all schemas present in snapshot <code>include_schemas: [\"public\", \"app\"]</code> Builds only listed schemas that exist Loads only listed schemas from snapshot <code>exclude_schemas: [\"temp\", \"logs\"]</code> Builds all schemas except listed ones Loads all schemas except excluded ones <p>Schema Configuration Options:</p> <code>include_schemas</code> (optional array) Include only specified schemas in build/load operations. Takes precedence over <code>exclude_schemas</code>. Only existing schemas will be processed. <code>exclude_schemas</code> (optional array) Exclude specified schemas from build/load operations. Processes all other available schemas. <code>use_default_schema</code> (optional boolean) Controls default schema behavior: - <code>true</code>: Process only the default schema (<code>public</code> for PostgreSQL) - <code>false</code>: Process all available schemas in the database - Not set: Defaults to <code>public</code> schema only <p>Important Notes:</p> <ul> <li>Dynamic Analysis: Schema filtering analyzes available schemas in the source database at build time</li> <li>Load Consistency: Load operations respect the schemas that were included in the snapshot</li> <li>Non-existent Schemas: Requesting schemas that don't exist will show warnings but won't fail the operation</li> <li>MySQL Limitation: Schema filtering is PostgreSQL-only; MySQL targets ignore <code>schema_config</code></li> <li>Validation: Schemas cannot appear in both <code>include_schemas</code> and <code>exclude_schemas</code></li> </ul> <p>Schema Validation</p> <p>The system prevents schemas from appearing in both <code>include_schemas</code> and <code>exclude_schemas</code> lists. Configuration validation will fail if this occurs.</p> <p>!!! tip \"Best Practices\" - Use <code>include_schemas</code> for explicit control over which schemas to process - Use <code>exclude_schemas</code> when you want most schemas except a few (logs, temp data) - Set <code>use_default_schema: false</code> to capture all schemas in complex applications - Test schema configuration with small databases first to verify expected behavior</p>"}, {"location": "configuration/#sanitization-configuration", "title": "Sanitization Configuration", "text": "<p>Configure data sanitization for security and compliance:</p> <pre><code>targets:\n  sanitized-target:\n    sanitize:\n      dst_url: \"postgresql://user:pass@localhost:5432/myapp_sanitized\"\n      query_file: \"sanitization.sql\"\n      override_query: \"UPDATE users SET email = CONCAT('user', id, '@example.com');\"\n</code></pre> <p>Sanitization Priority System</p> <p>DBSnapper uses a three-level priority system for sanitization queries:</p> <ol> <li>Target-level <code>override_query</code> (Highest Priority)</li> <li>Global <code>override.san_query</code> (Medium Priority)</li> <li>Target <code>query_file</code> (Lowest Priority)</li> </ol> <p>Base64 Encoding Support</p> <p>All sanitization queries support base64 encoding for complex SQL:</p> <pre><code># Base64 encoded query (automatically detected and decoded)\noverride:\n  san_query: \"VVBEQVRFIHVzZXJzIFNFVCBlbWFpbCA9ICd1c2VyQGV4YW1wbGUuY29tJzs=\"\n</code></pre>"}, {"location": "configuration/#subset-configuration", "title": "Subset Configuration", "text": "<p>Create smaller, referentially intact database subsets:</p> <pre><code>targets:\n  subset-target:\n    subset:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_subset\"\n\n      # Tables to subset with specific criteria\n      subset_tables:\n        - table: public.users\n          where: \"created_at &gt; '2023-01-01'\"\n        - table: public.orders\n          percent: 20\n\n      # Tables to copy completely\n      copy_tables:\n        - public.settings\n        - public.configuration\n\n      # Tables to exclude entirely\n      excluded_tables:\n        - public.temp_data\n        - public.audit_logs\n\n      # Add missing foreign key relationships\n      added_relationships:\n        - fk_table: public.orders\n          fk_columns: user_id\n          ref_table: public.users\n          ref_columns: id\n\n      # Exclude problematic relationships (circular dependencies)\n      excluded_relationships:\n        - fk_table: public.audit_log\n          ref_table: public.users\n</code></pre> <p>Subset Configuration Details:</p> <code>subset_tables</code> Initial tables of interest with filtering criteria. Use either <code>where</code> clause or <code>percent</code> (not both). <code>copy_tables</code> Tables copied in their entirety to maintain referential integrity. <code>excluded_tables</code> Tables completely excluded from the subset. <code>added_relationships</code> Foreign key relationships not defined in database schema but needed for referential integrity. <code>excluded_relationships</code> Relationships to ignore (useful for breaking circular dependencies)."}, {"location": "configuration/#share-configuration", "title": "Share Configuration", "text": "<p>Configure targets for team sharing via cloud storage:</p> <pre><code>targets:\n  shared-target:\n    name: \"Team Shared Target\"\n    share:\n      dst_url: \"postgresql://user:pass@localhost:5432/shared_db\"\n      storage_profile_name: \"s3-sanitized-bucket\"\n</code></pre> <p>Share Configuration Details:</p> <code>dst_url</code> Database connection where shared snapshots will be loaded. <code>storage_profile_name</code> Reference to storage profile containing shared snapshots."}, {"location": "configuration/#storage-profiles_1", "title": "Storage Profiles", "text": "<p>Configure cloud storage for snapshot backup and sharing:</p>"}, {"location": "configuration/#amazon-s3", "title": "Amazon S3", "text": "<pre><code>storage_profiles:\n  s3-production:\n    provider: s3\n    region: us-west-2\n    bucket: dbsnapper-prod-snapshots\n    prefix: team-snapshots\n\n    # Option 1: Use AWS CLI profile\n    awscli_profile: production\n\n    # Option 2: Direct credentials (not recommended for production)\n    access_key: AKIA...\n    secret_key: xyz...\n</code></pre>"}, {"location": "configuration/#cloudflare-r2", "title": "Cloudflare R2", "text": "<pre><code>storage_profiles:\n  r2-sanitized:\n    provider: r2\n    bucket: dbsnapper-r2-sanitized\n    prefix: sanitized\n\n    # Option 1: Use AWS CLI profile configured for R2\n    awscli_profile: r2_profile\n\n    # Option 2: Direct credentials with account ID\n    access_key: your-r2-access-key\n    secret_key: your-r2-secret-key\n    account_id: your-cloudflare-account-id\n</code></pre>"}, {"location": "configuration/#minio", "title": "MinIO", "text": "<pre><code>storage_profiles:\n  minio-local:\n    provider: minio\n    endpoint: http://localhost:9000\n    bucket: dbsnapper-minio\n    prefix: snapshots\n    access_key: minioadmin\n    secret_key: minioadmin\n</code></pre>"}, {"location": "configuration/#digitalocean-spaces", "title": "DigitalOcean Spaces", "text": "<pre><code>storage_profiles:\n  do-spaces:\n    provider: dospaces\n    endpoint: https://nyc3.digitaloceanspaces.com\n    bucket: dbsnapper-do-spaces\n    prefix: production\n    access_key: your-spaces-key\n    secret_key: your-spaces-secret\n</code></pre>"}, {"location": "configuration/#override-system", "title": "Override System", "text": "<p>Global overrides provide system-wide defaults and overrides:</p> <pre><code>override:\n  # Global sanitization query (base64 encoded)\n  san_query: \"VVBEQVRFIHVzZXJzIFNFVCBlbWFpbCA9ICd1c2VyQGV4YW1wbGUuY29tJzs=\"\n\n  # Global destination database override\n  dst_db_url: \"postgresql://postgres:postgres@localhost:5432/global_override\"\n</code></pre> <p>Override Priority:</p> <ul> <li>Override settings apply to all operations unless specifically overridden in target configuration</li> <li>Target-level settings always take precedence over global overrides</li> </ul>"}, {"location": "configuration/#defaults-system", "title": "Defaults System", "text": "<p>Configure default values for shared and team operations:</p> <pre><code>defaults:\n  # Default destination for shared team snapshots\n  shared_target_dst_db_url: \"postgresql://user:pass@localhost:5432/shared_default\"\n</code></pre>"}, {"location": "configuration/#sso-configuration", "title": "SSO Configuration", "text": "<p>Configure Single Sign-On for team sharing and authentication:</p> <pre><code>sso:\n  okta:\n    provider_url: https://dev-89837244.okta.com\n    client_id: 0oag95e2z5BhLZ5AI5d7\n    redirect_url: http://localhost:8080/callback\n    # Tokens are encrypted automatically when stored\n    access_token: $aes$fdff26aae3438c29...\n    refresh_token: $aes$6d42e5fd73fea9de...\n    expires: 1715292878\n</code></pre>"}, {"location": "configuration/#configuration-modes", "title": "Configuration Modes", "text": ""}, {"location": "configuration/#local-mode-standalone", "title": "Local Mode (Standalone)", "text": "<p>Local mode uses only the configuration file without cloud integration:</p> <pre><code># Minimal local configuration\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: ~/.dbsnapper\n\ntargets:\n  local-target:\n    name: \"Local Development\"\n    snapshot:\n      src_url: \"postgresql://user:pass@localhost:5432/myapp\"\n      dst_url: \"postgresql://user:pass@localhost:5432/myapp_dev\"\n</code></pre> <p>Local Mode Features:</p> <ul> <li>Local snapshot storage only</li> <li>No cloud integration</li> <li>No team sharing capabilities</li> <li>Faster operations (no network overhead)</li> </ul>"}, {"location": "configuration/#cloud-mode", "title": "Cloud Mode", "text": "<p>Cloud mode enables full feature set with DBSnapper Cloud integration:</p> <pre><code># Cloud configuration\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nauthtoken: jREZkinFQpSJb4TWZAKioHuCD7KG2GV3...\nenv: production\n\nstorage_profiles:\n  cloud-storage:\n    provider: s3\n    bucket: dbsnapper-cloud\n    awscli_profile: production\n\ntargets:\n  cloud-target:\n    name: \"Production with Cloud\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp\"\n    storage_profile: cloud-storage\n</code></pre> <p>Cloud Mode Features:</p> <ul> <li>Cloud snapshot storage and backup</li> <li>Team sharing with SSO integration</li> <li>Cloud target management</li> <li>Enhanced security with encryption</li> <li>Cross-team collaboration</li> </ul>"}, {"location": "configuration/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "configuration/#credential-management", "title": "Credential Management", "text": "<p>Best Practices:</p> <ul> <li>Use AWS CLI profiles instead of hardcoded credentials</li> <li>Store sensitive values as environment variables</li> <li>Use DBSnapper's encryption for stored tokens</li> <li>Regularly rotate authentication tokens</li> </ul> <p>Example Secure Configuration:</p> <pre><code># Use environment variables for sensitive data\nauthtoken: # Set via DBSNAPPER_AUTHTOKEN\nsecret_key: # Set via DBSNAPPER_SECRET_KEY\n\ntargets:\n  secure-target:\n    snapshot:\n      # Use templates with environment variables\n      src_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@{{`DB_HOST` | env}}/myapp\"\n      dst_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@localhost/myapp_dev\"\n\nstorage_profiles:\n  secure-s3:\n    provider: s3\n    # Use AWS CLI profile instead of credentials\n    awscli_profile: dbsnapper-production\n    bucket: secure-snapshots\n</code></pre>"}, {"location": "configuration/#encryption", "title": "Encryption", "text": "<p>DBSnapper automatically encrypts sensitive configuration values:</p> <ul> <li>SSO tokens (access_token, refresh_token)</li> <li>Optionally other sensitive strings using <code>$aes$</code> prefix</li> <li>All encryption uses AES with the configured <code>secret_key</code></li> </ul>"}, {"location": "configuration/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "configuration/#configuration-validation", "title": "Configuration Validation", "text": "<p>Validate your configuration:</p> <pre><code>dbsnapper config check\n</code></pre>"}, {"location": "configuration/#common-issues", "title": "Common Issues", "text": "<p>Invalid Schema Configuration</p> <pre><code>Error: schema 'analytics' cannot be in both include and exclude lists\n</code></pre> <ul> <li>Remove schema from one of the lists</li> <li>Use either <code>include_schemas</code> OR <code>exclude_schemas</code>, not both for the same schema</li> </ul> <p>Template Parsing Errors</p> <pre><code>Error: template parsing failed for src_url\n</code></pre> <ul> <li>Check template syntax: <code>{{</code>VARIABLE<code>| env}}</code></li> <li>Ensure environment variables exist</li> <li>Verify no typos in variable names</li> </ul> <p>Storage Profile Issues</p> <pre><code>Error: storage profile 'missing-profile' not found\n</code></pre> <ul> <li>Verify storage profile name matches configuration</li> <li>Check storage profile is properly configured</li> <li>Ensure required credentials are provided</li> </ul>"}, {"location": "configuration/#debug-mode", "title": "Debug Mode", "text": "<p>For troubleshooting, you can enable additional logging by adding <code>debug: true</code> to your configuration file or using environment variables as needed.</p>"}, {"location": "configuration/#migration-guide", "title": "Migration Guide", "text": ""}, {"location": "configuration/#upgrading-from-earlier-versions", "title": "Upgrading from Earlier Versions", "text": "<p>v2.6.0+ Override System:</p> <ul> <li>Added <code>override.san_query</code> and <code>override.dst_db_url</code></li> <li>Migrate target-specific overrides to global overrides if desired</li> </ul> <p>v2.7.0+ Defaults System:</p> <ul> <li>Added <code>defaults.shared_target_dst_db_url</code></li> <li>Configure default shared target destination</li> </ul> <p>Schema Configuration:</p> <ul> <li>New <code>schema_config</code> section replaces database-specific schema filtering</li> <li>Update existing schema filtering configurations</li> </ul>"}, {"location": "configuration/#see-also", "title": "See Also", "text": "<ul> <li>Build Command - Creating database snapshots</li> <li>Load Command - Restoring snapshots</li> <li>Cloud Storage Setup</li> <li>Database Engines</li> <li>Sanitization</li> </ul>"}, {"location": "how-it-works/", "title": "How DBSnapper Works", "text": "<p>DBSnapper transforms database snapshot management from a manual, error-prone process into an automated, secure workflow that integrates seamlessly with modern development practices.</p>"}, {"location": "how-it-works/#the-developer-problem", "title": "The Developer Problem", "text": "<p>Traditional approaches to database snapshots face critical challenges:</p> <ul> <li>Security risks - Full production data exposed in development environments</li> <li>Compliance violations - PII and sensitive data in the wrong hands</li> <li>Manual overhead - Complex dump/restore processes that developers avoid</li> <li>Team bottlenecks - Database admins become gatekeepers for realistic test data</li> <li>Infrastructure sprawl - Multiple database instances for different teams and purposes</li> </ul> <p>DBSnapper solves these problems with a three-step workflow that keeps your data secure while making realistic datasets accessible to your entire team.</p> <p> DBSnapper Architecture - Bring Your Own Infrastructure Security Model </p>"}, {"location": "how-it-works/#the-dbsnapper-workflow", "title": "The DBSnapper Workflow", "text": ""}, {"location": "how-it-works/#step-1-snapshot-capture-production-reality", "title": "Step 1: Snapshot - Capture Production Reality", "text": "<p>Create point-in-time database snapshots with zero infrastructure changes.</p> <p>DBSnapper connects to your existing databases using read-only credentials and creates compressed snapshots using native database tools (<code>pg_dump</code>, <code>mysqldump</code>). These snapshots capture the current state of your data, including schema, indexes, and relationships.</p> <p>Key Benefits:</p> <ul> <li>No infrastructure changes - Works with existing database setups</li> <li>High-performance capture - Multi-core processing with configurable CPU usage</li> <li>Schema filtering - Capture only the data you need (PostgreSQL)</li> <li>Native compatibility - Uses standard database tools for maximum compatibility</li> </ul> <p>Modern Integration:</p> <ul> <li>CI/CD friendly - Automated snapshot creation in GitHub Actions</li> <li>Container-ready - Run in Docker, Kubernetes, or serverless environments</li> <li>Cloud storage - Automatic upload to your S3, R2, or other cloud storage</li> </ul>"}, {"location": "how-it-works/#step-2-sanitize-remove-sensitive-data", "title": "Step 2: Sanitize - Remove Sensitive Data", "text": "<p>Transform production data into development-safe datasets automatically.</p> <p>DBSnapper's sanitization engine removes or masks Personally Identifiable Information (PII) and other sensitive data using configurable SQL queries. The process maintains referential integrity while ensuring compliance with data protection regulations.</p> <p>Advanced Sanitization Features:</p> <ul> <li>Three-tier priority system - Target-specific, global, or file-based sanitization rules</li> <li>Ephemeral processing - No persistent storage of sensitive data during sanitization</li> <li>Template support - Use environment variables for dynamic sanitization</li> <li>Base64 encoding - Handle complex SQL queries safely in configuration files</li> </ul> <p>Compliance &amp; Security:</p> <ul> <li>GDPR/CCPA ready - Built-in patterns for common compliance requirements</li> <li>Audit trails - Track what data was sanitized and when</li> <li>Your infrastructure only - Sensitive data never leaves your environment</li> <li>Encryption at rest - All configuration and snapshots encrypted with your keys</li> </ul>"}, {"location": "how-it-works/#step-3-share-distribute-securely", "title": "Step 3: Share - Distribute Securely", "text": "<p>Enable your entire team to access realistic, sanitized data instantly.</p> <p>Share sanitized snapshots through DBSnapper Cloud with fine-grained access control. Team members can discover, download, and load snapshots directly from their development environment without involving database administrators.</p> <p>Team Collaboration Features:</p> <ul> <li>SSO integration - Use your existing Okta, Auth0, or other SSO provider</li> <li>Group-based access - Share snapshots with specific teams automatically</li> <li>Presigned URLs - Secure, time-limited access to snapshot downloads</li> <li>Multi-environment support - Different snapshots for staging, development, and testing</li> </ul> <p>Developer Experience:</p> <ul> <li>IDE integration - Load snapshots directly from VS Code</li> <li>Command-line access - Simple <code>dbsnapper load</code> commands</li> <li>Terraform provider - Infrastructure as Code for snapshot management</li> <li>API access - Programmatic integration with your existing tools</li> </ul>"}, {"location": "how-it-works/#key-architectural-principles", "title": "Key Architectural Principles", "text": ""}, {"location": "how-it-works/#security-by-design", "title": "\ud83d\udd12 Security by Design", "text": "<p>Your Data Stays Yours - DBSnapper follows a \"bring your own infrastructure\" model. Your production data never leaves your environment during processing, and sanitized snapshots are stored in your own cloud storage accounts.</p>"}, {"location": "how-it-works/#performance-optimized", "title": "\u26a1 Performance Optimized", "text": "<p>Multi-Core Processing - Configure CPU usage per target for optimal performance. PostgreSQL operations can leverage multiple cores for both snapshot creation and restoration.</p>"}, {"location": "how-it-works/#developer-first-experience", "title": "\ud83d\udd27 Developer-First Experience", "text": "<p>Zero-Config Operation - Complex database operations reduced to single commands. Whether you're using Docker, CI/CD pipelines, or your local machine, DBSnapper adapts to your workflow.</p>"}, {"location": "how-it-works/#enterprise-ready", "title": "\ud83c\udfd7\ufe0f Enterprise Ready", "text": "<p>Scale with Your Team - From individual developers to large engineering organizations, DBSnapper scales with role-based access, audit trails, and integration with your existing security infrastructure.</p>"}, {"location": "how-it-works/#modern-development-integration", "title": "Modern Development Integration", "text": ""}, {"location": "how-it-works/#continuous-integration", "title": "Continuous Integration", "text": "<p>Automate snapshot creation in your CI/CD pipelines:</p> <pre><code># GitHub Actions example\n- uses: dbsnapper/install-dbsnapper-agent-action@v1\n- run: dbsnapper sanitize production-db --new-set\n</code></pre>"}, {"location": "how-it-works/#infrastructure-as-code", "title": "Infrastructure as Code", "text": "<p>Manage snapshots with Terraform:</p> <pre><code>resource \"dbsnapper_target\" \"api_db\" {\n  name = \"production-api\"\n  sanitization_rules = [\"remove_pii.sql\"]\n  storage_profile = \"team-s3\"\n}\n</code></pre>"}, {"location": "how-it-works/#ai-assistant-integration", "title": "AI Assistant Integration", "text": "<p>Use the MCP server for AI-powered database operations:</p> <pre><code>dbsnapper mcp  # Start MCP server for Claude Desktop\n</code></pre>"}, {"location": "how-it-works/#next-steps", "title": "Next Steps", "text": "<p>Ready to implement this workflow in your environment?</p> <ol> <li>Install DBSnapper - Get started with the method that fits your setup</li> <li>Quick Start Guide - Choose your path and get running in minutes</li> <li>Join DBSnapper Cloud - Enable team collaboration and secure sharing</li> </ol> <p>Questions? Check our Philosophy page to understand the principles behind DBSnapper's design, or dive into the Configuration Guide for advanced setup options.</p>"}, {"location": "installation/", "title": "Installation", "text": "<p>Get DBSnapper running in your environment with the installation method that best fits your workflow. Whether you're developing locally, deploying in containers, or setting up CI/CD pipelines, we've got you covered.</p> <p> DBSnapper Terminal User Interface </p>"}, {"location": "installation/#choose-your-installation-method", "title": "Choose Your Installation Method", "text": "\ud83d\udc33 Docker (Recommended)\ud83c\udf7a macOS (Homebrew)\ud83d\udc27 Linux (Package Managers)\ud83d\udce6 Direct Binary Download <p>Perfect for: Teams, CI/CD, consistent environments, and trying DBSnapper quickly.</p> <p>The Docker approach includes all database tools and dependencies pre-installed, making it the most reliable way to run DBSnapper across different environments.</p> <p>Perfect for: Local development on macOS with automatic updates.</p> <p>Perfect for: Linux servers, development machines, and automated deployments.</p> <p>Perfect for: Custom deployments, air-gapped environments, or specific architecture needs.</p>"}, {"location": "installation/#pull-the-latest-image", "title": "Pull the Latest Image", "text": "<pre><code>docker pull ghcr.io/dbsnapper/dbsnapper:latest\n</code></pre>"}, {"location": "installation/#quick-start-with-docker", "title": "Quick Start with Docker", "text": "<pre><code># Interactive shell with all tools included\ndocker run -it --rm ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n\n# Or run commands directly\ndocker run --rm ghcr.io/dbsnapper/dbsnapper:latest dbsnapper --version\n</code></pre>"}, {"location": "installation/#cicd-integration", "title": "CI/CD Integration", "text": "<p>Use in GitHub Actions:</p> <pre><code>steps:\n  - uses: dbsnapper/install-dbsnapper-agent-action@v1\n    with:\n      version: latest\n  - run: dbsnapper build production-db\n</code></pre>"}, {"location": "installation/#production-deployment", "title": "Production Deployment", "text": "<p>For production workloads, mount configuration and working directories:</p> <pre><code>docker run -d \\\n  -v ~/.config/dbsnapper:/root/.config/dbsnapper \\\n  -v ~/.dbsnapper:/root/.dbsnapper \\\n  -e DBSNAPPER_SECRET_KEY=$SECRET_KEY \\\n  -e DBSNAPPER_AUTHTOKEN=$AUTH_TOKEN \\\n  ghcr.io/dbsnapper/dbsnapper:latest\n</code></pre>"}, {"location": "installation/#install-via-homebrew", "title": "Install via Homebrew", "text": "<pre><code># Add the DBSnapper tap\nbrew tap dbsnapper/tap\n\n# Install DBSnapper\nbrew install dbsnapper\n</code></pre>"}, {"location": "installation/#update-dbsnapper", "title": "Update DBSnapper", "text": "<pre><code>brew upgrade dbsnapper\n</code></pre>"}, {"location": "installation/#install-database-tools", "title": "Install Database Tools", "text": "<p>You'll also need PostgreSQL and/or MySQL client tools:</p> <pre><code># PostgreSQL tools\nbrew install postgresql\n\n# MySQL tools\nbrew install mysql-client\n</code></pre>"}, {"location": "installation/#debianubuntu-deb-packages", "title": "Debian/Ubuntu (.deb packages)", "text": "<pre><code># Download latest release\nTAG=$(curl -s https://api.github.com/repos/dbsnapper/dbsnapper/releases/latest | grep tag_name | cut -d '\"' -f 4 | sed 's/v//')\nARCH=linux_x86_64  # or linux_arm64\n\nwget https://github.com/dbsnapper/dbsnapper/releases/download/v${TAG}/dbsnapper_${ARCH}.deb\n\n# Install\nsudo dpkg -i dbsnapper_${ARCH}.deb\n</code></pre>"}, {"location": "installation/#redhatcentosfedora-rpm-packages", "title": "RedHat/CentOS/Fedora (.rpm packages)", "text": "<pre><code># Download latest release\nTAG=$(curl -s https://api.github.com/repos/dbsnapper/dbsnapper/releases/latest | grep tag_name | cut -d '\"' -f 4 | sed 's/v//')\nARCH=linux_x86_64  # or linux_arm64\n\nwget https://github.com/dbsnapper/dbsnapper/releases/download/v${TAG}/dbsnapper_${ARCH}.rpm\n\n# Install\nsudo rpm -i dbsnapper_${ARCH}.rpm\n</code></pre>"}, {"location": "installation/#alpine-linux-apk-packages", "title": "Alpine Linux (.apk packages)", "text": "<pre><code># Download latest release\nTAG=$(curl -s https://api.github.com/repos/dbsnapper/dbsnapper/releases/latest | grep tag_name | cut -d '\"' -f 4 | sed 's/v//')\nARCH=linux_x86_64  # or linux_arm64\n\nwget https://github.com/dbsnapper/dbsnapper/releases/download/v${TAG}/dbsnapper_${ARCH}.apk\n\n# Install\nsudo apk add --allow-untrusted dbsnapper_${ARCH}.apk\n</code></pre>"}, {"location": "installation/#install-database-client-tools", "title": "Install Database Client Tools", "text": "<pre><code># Debian/Ubuntu\nsudo apt-get update\nsudo apt-get install postgresql-client mysql-client\n\n# RedHat/CentOS/Fedora\nsudo dnf install postgresql mysql\n\n# Alpine\nsudo apk add postgresql-client mysql-client\n</code></pre>"}, {"location": "installation/#download-from-github-releases", "title": "Download from GitHub Releases", "text": "<p>Visit the DBSnapper Releases page to download the binary for your platform:</p> <ul> <li>macOS: <code>dbsnapper_darwin_universal.tar.gz</code> (Intel &amp; Apple Silicon)</li> <li>Linux x64: <code>dbsnapper_linux_x86_64.tar.gz</code></li> <li>Linux ARM64: <code>dbsnapper_linux_arm64.tar.gz</code></li> </ul>"}, {"location": "installation/#installation-steps", "title": "Installation Steps", "text": "<pre><code># Download (replace with your platform)\nwget https://github.com/dbsnapper/dbsnapper/releases/latest/download/dbsnapper_linux_x86_64.tar.gz\n\n# Extract\ntar -xzf dbsnapper_linux_x86_64.tar.gz\n\n# Make executable and move to PATH\nchmod +x dbsnapper\nsudo mv dbsnapper /usr/local/bin/\n</code></pre>"}, {"location": "installation/#verify-installation", "title": "Verify Installation", "text": "<p>After installation, verify DBSnapper is working correctly:</p> <pre><code># Check version\ndbsnapper --version\n\n# Check configuration and environment\ndbsnapper config check\n</code></pre> <p>You should see output similar to:</p> <pre><code>DBSnapper Agent - Version: 3.0.0 (abc123) Build Date: 2024-12-01T10:30:00Z\nDBSnapper Cloud: Standalone Mode\n\nChecking DBSnapper Configuration\n  \u2705 Config file not found - use 'dbsnapper config init' to create\n  \ud83d\udd35 Postgres Local Engine (pglocal)\n    \u2705 psql found at /usr/bin/psql\n    \u2705 pg_dump found at /usr/bin/pg_dump\n    \u2705 pg_restore found at /usr/bin/pg_restore\n  \ud83d\udd35 MySQL Local Engine (mylocal)\n    \u2705 mysqldump found at /usr/bin/mysqldump\n    \u2705 mysql found at /usr/bin/mysql\n  \ud83d\udd35 Postgres Docker Engine (pgdocker)\n    \u2705 Docker client connected\n    \u26a0\ufe0f  No docker images configured - Docker engines will use defaults\n  \ud83d\udd35 MySQL Docker Engine (mydocker)\n    \u2705 Docker client connected\n    \u26a0\ufe0f  No docker images configured - Docker engines will use defaults\n  \u2705 All supported database engines available\n  \u26a0\ufe0f  DBSnapper Cloud not configured - use 'dbsnapper auth token' or sign up at https://app.dbsnapper.com\n\n  \u2705 Installation OK\n</code></pre>"}, {"location": "installation/#development-environment-setup", "title": "Development Environment Setup", "text": ""}, {"location": "installation/#ide-integration", "title": "IDE Integration", "text": "<p>VS Code Extension - Install the DBSnapper Extension for in-editor snapshot management:</p> <ol> <li>Open VS Code</li> <li>Go to Extensions (Ctrl+Shift+X)</li> <li>Search for \"DBSnapper\"</li> <li>Click Install</li> </ol>"}, {"location": "installation/#mcp-server-setup", "title": "MCP Server Setup", "text": "<p>For AI assistant integration (Claude Desktop, etc.):</p> <pre><code># Start the MCP server (runs on default port)\ndbsnapper mcp\n\n# The server will be available for AI assistant integration\n</code></pre>"}, {"location": "installation/#container-orchestration", "title": "Container Orchestration", "text": ""}, {"location": "installation/#kubernetes-deployment", "title": "Kubernetes Deployment", "text": "<p>Deploy DBSnapper in Kubernetes for scheduled operations:</p> <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: dbsnapper-backup\nspec:\n  schedule: \"0 2 * * *\" # Daily at 2 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: dbsnapper\n              image: ghcr.io/dbsnapper/dbsnapper:latest\n              command: [\"dbsnapper\", \"build\", \"production-db\"]\n              env:\n                - name: DBSNAPPER_SECRET_KEY\n                  valueFrom:\n                    secretKeyRef:\n                      name: dbsnapper-secret\n                      key: secret-key\n                - name: DBSNAPPER_AUTHTOKEN\n                  valueFrom:\n                    secretKeyRef:\n                      name: dbsnapper-secret\n                      key: auth-token\n          restartPolicy: OnFailure\n</code></pre>"}, {"location": "installation/#docker-compose", "title": "Docker Compose", "text": "<p>For local development or small deployments:</p> <pre><code>version: \"3.8\"\nservices:\n  dbsnapper:\n    image: ghcr.io/dbsnapper/dbsnapper:latest\n    environment:\n      - DBSNAPPER_SECRET_KEY=${SECRET_KEY}\n      - DBSNAPPER_AUTHTOKEN=${AUTH_TOKEN}\n    volumes:\n      - ./config:/root/.config/dbsnapper\n      - ./snapshots:/root/.dbsnapper\n    command: [\"tail\", \"-f\", \"/dev/null\"] # Keep running for manual commands\n</code></pre>"}, {"location": "installation/#performance-considerations", "title": "Performance Considerations", "text": ""}, {"location": "installation/#cpu-configuration", "title": "CPU Configuration", "text": "<p>DBSnapper v3.0 supports configurable CPU usage for optimal performance:</p> <pre><code># Global default\ndefaults:\n  cpus: 4\n\n# Per-target override\ntargets:\n  large_database:\n    cpus: 8 # Use more cores for large databases\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/large_db\"\n</code></pre>"}, {"location": "installation/#memory-requirements", "title": "Memory Requirements", "text": "<ul> <li>Minimum: 512MB RAM</li> <li>Recommended: 2GB+ RAM for large databases</li> <li>Container limits: Set appropriate memory limits in production deployments</li> </ul>"}, {"location": "installation/#troubleshooting-installation", "title": "Troubleshooting Installation", "text": ""}, {"location": "installation/#common-issues", "title": "Common Issues", "text": "<p>\"Command not found\" after installation:</p> <pre><code># Check if binary is in PATH\nwhich dbsnapper\n\n# If not found, check installation location and add to PATH\nexport PATH=$PATH:/usr/local/bin\n</code></pre> <p>Docker permission denied:</p> <pre><code># Add user to docker group (requires logout/login)\nsudo usermod -aG docker $USER\n\n# Or use rootless Docker\n</code></pre> <p>Database tools not found:</p> <pre><code># Use Docker mode instead of local tools\ndbsnapper config check --prefer-docker\n\n# Or install database client packages (see platform sections above)\n</code></pre>"}, {"location": "installation/#verify-database-connectivity", "title": "Verify Database Connectivity", "text": "<p>Test your database connections:</p> <pre><code># Test PostgreSQL connection\npsql \"postgresql://user:pass@host:5432/dbname\" -c \"SELECT version();\"\n\n# Test MySQL connection\nmysql -h host -u user -p -e \"SELECT VERSION();\"\n</code></pre>"}, {"location": "installation/#next-steps", "title": "Next Steps", "text": "<p>With DBSnapper installed, you're ready to:</p> <ol> <li>Initialize your first configuration - Start with our guided quick start</li> <li>Set up DBSnapper Cloud - Enable team collaboration</li> <li>Configure your first target - Connect to your databases</li> </ol> <p>Need help? Check our troubleshooting guide or join the community discussions.</p>"}, {"location": "philosophy/", "title": "Why DBSnapper?", "text": "<p>Modern development teams need realistic data for building, testing, and debugging applications. However, traditional approaches to database snapshots create significant challenges around security, compliance, and team productivity. DBSnapper solves these challenges with a developer-first approach that prioritizes security, privacy, and seamless workflow integration.</p>"}, {"location": "philosophy/#the-development-data-problem", "title": "The Development Data Problem", "text": "<p>Every development team faces the same fundamental challenge: How do you give developers access to realistic data without compromising security or privacy?</p>"}, {"location": "philosophy/#traditional-approaches-fall-short", "title": "Traditional Approaches Fall Short", "text": "<p>\ud83d\udeab Synthetic/Fake Data:</p> <ul> <li>Doesn't capture real-world edge cases and data relationships</li> <li>Misses complex validation logic that only emerges with production data</li> <li>Provides false confidence in application behavior</li> </ul> <p>\ud83d\udeab Production Database Access:</p> <ul> <li>Massive security risk exposing sensitive customer data</li> <li>Compliance violations with regulations like GDPR, HIPAA, and SOX</li> <li>Performance impact on production systems</li> </ul> <p>\ud83d\udeab Manual Database Copies:</p> <ul> <li>Time-intensive process that becomes a team bottleneck</li> <li>Inconsistent sanitization leading to data leaks</li> <li>Difficult to maintain and update regularly</li> </ul> <p>\ud83d\udeab IT-Managed Snapshot Processes:</p> <ul> <li>Creates dependency on database administrators</li> <li>Slow iteration cycles that impede development velocity</li> <li>Limited access controls for different team members</li> </ul>"}, {"location": "philosophy/#the-modern-reality", "title": "The Modern Reality", "text": "<p>Today's development teams need:</p> <ul> <li>Autonomous access to realistic data without IT bottlenecks</li> <li>Compliance-ready processes that protect sensitive information</li> <li>Team collaboration that scales from startups to enterprises</li> <li>Integration with modern development toolchains (CI/CD, containers, IaC)</li> </ul>"}, {"location": "philosophy/#dbsnappers-solution-developer-autonomy-with-security", "title": "DBSnapper's Solution: Developer Autonomy with Security", "text": "<p>DBSnapper transforms database snapshot management from a manual, risky process into an automated, secure workflow that integrates with your existing development practices.</p>"}, {"location": "philosophy/#security-by-design", "title": "\ud83d\udee1\ufe0f Security by Design", "text": "<p>Your Data Never Leaves Your Control:</p> <ul> <li>Agent runs in your infrastructure, connecting directly to your databases</li> <li>Sensitive data processing happens entirely within your environment</li> <li>Zero trust architecture - DBSnapper Cloud never touches your production data</li> </ul> <p>Bring Your Own Infrastructure:</p> <ul> <li>Use your existing cloud storage (S3, R2, etc.) for snapshot storage</li> <li>Leverage your security policies and access controls</li> <li>Maintain audit trails within your compliance framework</li> </ul> <p>Encryption Everywhere:</p> <ul> <li>AES-256 encryption for all stored configurations and snapshots</li> <li>TLS 1.2+ for all network communications</li> <li>Template-based configuration to avoid storing credentials</li> </ul>"}, {"location": "philosophy/#performance-scalability", "title": "\u26a1 Performance &amp; Scalability", "text": "<p>Multi-Core Database Operations:</p> <ul> <li>Configurable CPU usage for optimal performance on your infrastructure</li> <li>Parallel processing for PostgreSQL operations (<code>pg_dump</code>, <code>pg_restore</code>)</li> <li>Scales from laptop development to enterprise database sizes</li> </ul> <p>Intelligent Compression &amp; Transfer:</p> <ul> <li>Compressed snapshots typically 10-50% of original database size</li> <li>Presigned URLs for direct, secure cloud storage operations</li> <li>Schema filtering to capture only relevant data (PostgreSQL)</li> </ul>"}, {"location": "philosophy/#team-collaboration", "title": "\ud83d\udc65 Team Collaboration", "text": "<p>SSO-Integrated Team Access:</p> <ul> <li>Use your existing identity provider (Okta, Auth0, Azure AD)</li> <li>Group-based permissions align with your organizational structure</li> <li>Automatic sharing of sanitized snapshots with authorized team members</li> </ul> <p>Multi-Environment Support:</p> <ul> <li>Different snapshot sets for development, staging, and testing</li> <li>Environment-specific sanitization rules and access controls</li> <li>Consistent data across distributed teams</li> </ul>"}, {"location": "philosophy/#developer-experience", "title": "\ud83d\udd27 Developer Experience", "text": "<p>Seamless Workflow Integration:</p> <ul> <li>VS Code Extension - Load snapshots directly from your editor</li> <li>GitHub Actions - Automated snapshot creation in CI/CD pipelines</li> <li>Terraform Provider - Infrastructure as Code for snapshot management</li> <li>MCP Server - AI assistant integration for intelligent database operations</li> </ul> <p>Zero-Config Operation:</p> <ul> <li>Complex database operations simplified to single commands</li> <li>Docker-based approach eliminates local tool dependencies</li> <li>Automatic environment detection and configuration</li> </ul>"}, {"location": "philosophy/#core-principles", "title": "Core Principles", "text": "<p>These principles guide every architectural decision and feature in DBSnapper:</p>"}, {"location": "philosophy/#1-data-sovereignty-above-all", "title": "1. Data Sovereignty Above All", "text": "<p>Your production data is your most valuable and sensitive asset. DBSnapper never requires transferring this data outside your approved infrastructure. Processing happens where your data already lives, maintaining your existing security boundaries and compliance requirements.</p>"}, {"location": "philosophy/#2-developer-autonomy-without-risk", "title": "2. Developer Autonomy Without Risk", "text": "<p>Developers should be able to access realistic data without creating security risks or compliance violations. DBSnapper enables self-service access to sanitized, realistic datasets that maintain data utility while eliminating privacy concerns.</p>"}, {"location": "philosophy/#3-infrastructure-flexibility", "title": "3. Infrastructure Flexibility", "text": "<p>Every organization has different infrastructure, cloud providers, and security requirements. DBSnapper integrates with your existing tools rather than forcing you into new dependencies or vendor relationships.</p>"}, {"location": "philosophy/#4-collaboration-scales-with-teams", "title": "4. Collaboration Scales with Teams", "text": "<p>From individual developers to large engineering organizations, DBSnapper grows with your team. Features like SSO integration, group-based sharing, and audit trails support enterprise requirements without overwhelming smaller teams.</p>"}, {"location": "philosophy/#5-automation-reduces-human-error", "title": "5. Automation Reduces Human Error", "text": "<p>Manual processes create consistency problems and security risks. DBSnapper emphasizes automation and repeatability, reducing the chance of accidental data exposure while improving team productivity.</p>"}, {"location": "philosophy/#real-world-benefits", "title": "Real-World Benefits", "text": "<p>These principles translate into tangible improvements for development teams:</p>"}, {"location": "philosophy/#faster-development-velocity", "title": "Faster Development Velocity", "text": "<ul> <li>Developers get realistic test data instantly, not weeks later</li> <li>No waiting for database administrators or IT approval processes</li> <li>Automated snapshot refreshes keep development environments current</li> </ul>"}, {"location": "philosophy/#better-application-quality", "title": "Better Application Quality", "text": "<ul> <li>Testing against production-like data catches edge cases early</li> <li>Complex data relationships and constraints are preserved</li> <li>Performance testing with realistic data volumes and distribution</li> </ul>"}, {"location": "philosophy/#reduced-security-risk", "title": "Reduced Security Risk", "text": "<ul> <li>Automated sanitization eliminates manual privacy failures</li> <li>Encrypted storage and transmission protect data at every step</li> <li>Audit trails provide compliance evidence and change tracking</li> </ul>"}, {"location": "philosophy/#lower-infrastructure-costs", "title": "Lower Infrastructure Costs", "text": "<ul> <li>Use existing cloud storage instead of specialized database hosting</li> <li>Eliminate duplicate database environments for different teams</li> <li>Reduce IT overhead for database snapshot management</li> </ul>"}, {"location": "philosophy/#enhanced-compliance-posture", "title": "Enhanced Compliance Posture", "text": "<ul> <li>Built-in data sanitization supports GDPR, HIPAA, and other regulations</li> <li>Your own infrastructure maintains data residency requirements</li> <li>Comprehensive logging and audit capabilities</li> </ul>"}, {"location": "philosophy/#modern-development-integration", "title": "Modern Development Integration", "text": "<p>DBSnapper integrates with the tools and practices that define modern software development:</p>"}, {"location": "philosophy/#container-native-architecture", "title": "Container-Native Architecture", "text": "<pre><code># Works seamlessly in Docker, Kubernetes, and serverless environments\ndocker run ghcr.io/dbsnapper/dbsnapper:latest dbsnapper build production-db\n</code></pre>"}, {"location": "philosophy/#infrastructure-as-code", "title": "Infrastructure as Code", "text": "<pre><code># Terraform provider for managing DBSnapper resources\nresource \"dbsnapper_target\" \"api\" {\n  name = \"production-api\"\n  sanitization_rules = [\"remove_pii.sql\"]\n  storage_profile = \"team-s3\"\n}\n</code></pre>"}, {"location": "philosophy/#cicd-pipeline-integration", "title": "CI/CD Pipeline Integration", "text": "<pre><code># GitHub Actions for automated snapshots\n- uses: dbsnapper/install-dbsnapper-agent-action@v1\n- run: dbsnapper sanitize production-db --new-set\n</code></pre>"}, {"location": "philosophy/#ide-integration", "title": "IDE Integration", "text": "<p>The VS Code extension brings database snapshot management directly into your development environment, eliminating context switching and friction.</p>"}, {"location": "philosophy/#the-result-secure-compliant-fast-development", "title": "The Result: Secure, Compliant, Fast Development", "text": "<p>DBSnapper enables development teams to work with realistic data while maintaining the highest security and compliance standards. By following these principles, we deliver:</p> <ul> <li>Reduced time-to-market through faster access to quality test data</li> <li>Lower security risk through automated, consistent sanitization processes</li> <li>Improved team productivity by eliminating IT bottlenecks</li> <li>Enhanced application quality through realistic development and testing environments</li> <li>Simplified compliance through built-in data protection and audit capabilities</li> </ul>"}, {"location": "philosophy/#experience-the-difference", "title": "Experience the Difference", "text": "<p>Ready to transform your team's approach to development data?</p> <ol> <li>Start with our Quick Start Guide - Get running in 5 minutes</li> <li>Install DBSnapper - Choose the method that fits your environment</li> <li>Join DBSnapper Cloud - Enable team collaboration features</li> </ol> <p>Questions about how DBSnapper fits your use case? Check out How DBSnapper Works for detailed technical information, or explore our comprehensive configuration guide for customization options.</p> <p>DBSnapper: Bringing realistic, secure data to every development team.</p>"}, {"location": "quick-start/", "title": "Quick Start Guide", "text": "<p>Get DBSnapper running in minutes with the approach that fits your needs. Whether you're exploring DBSnapper for the first time or setting up your team's database snapshot workflow, we've got you covered.</p>"}, {"location": "quick-start/#choose-your-path", "title": "Choose Your Path", "text": "\ud83d\ude80 Try DBSnapper (5 minutes)\u2699\ufe0f Set Up for Your Team\ud83d\udd27 Integrate with Your Workflow <p>Perfect for: First-time users who want to see DBSnapper in action quickly.</p> <p>What you'll accomplish:</p> <ul> <li>\u2705 Run DBSnapper in a container (no installation needed)</li> <li>\u2705 Create your first database snapshot</li> <li>\u2705 Load a snapshot into a development database</li> </ul> <p>Prerequisites: Docker installed and a test database available</p> <p>Perfect for: Teams who want to share snapshots and use cloud storage.</p> <p>What you'll accomplish:</p> <ul> <li>\u2705 Set up DBSnapper Cloud integration</li> <li>\u2705 Configure team storage profiles</li> <li>\u2705 Create shareable, sanitized snapshots</li> <li>\u2705 Enable SSO-based team access</li> </ul> <p>Prerequisites: DBSnapper Cloud account and cloud storage access</p> <p>Perfect for: Developers who want DBSnapper integrated into their daily workflow.</p> <p>What you'll accomplish:</p> <ul> <li>\u2705 Set up VSCode extension for in-editor access</li> <li>\u2705 Configure GitHub Actions for automated snapshots</li> <li>\u2705 Use Terraform provider for infrastructure as code</li> <li>\u2705 Enable MCP server for AI assistant integration</li> </ul> <p>Prerequisites: DBSnapper installed, VSCode/GitHub/Terraform as needed</p>"}, {"location": "quick-start/#step-1-run-dbsnapper-with-docker", "title": "Step 1: Run DBSnapper with Docker", "text": "<p>Use our pre-built container with all tools included:</p> <pre><code>docker run -it --rm ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n</code></pre>"}, {"location": "quick-start/#step-2-initialize-configuration", "title": "Step 2: Initialize Configuration", "text": "<pre><code>dbsnapper config init\n</code></pre>"}, {"location": "quick-start/#step-3-configure-a-test-target", "title": "Step 3: Configure a Test Target", "text": "<p>Edit the generated config to add your database:</p> <pre><code># Edit ~/.config/dbsnapper/dbsnapper.yml\ntargets:\n  my_test:\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/source_db\"\n      dst_url: \"postgresql://user:pass@host:5432/test_db\"\n</code></pre>"}, {"location": "quick-start/#step-4-create-your-first-snapshot", "title": "Step 4: Create Your First Snapshot", "text": "<pre><code>dbsnapper build my_test\n</code></pre>"}, {"location": "quick-start/#step-5-load-the-snapshot", "title": "Step 5: Load the Snapshot", "text": "<pre><code>dbsnapper load my_test 0\n</code></pre> <p>\ud83c\udf89 Success! You've created and loaded your first snapshot. Ready to explore more features?</p>"}, {"location": "quick-start/#step-1-install-dbsnapper", "title": "Step 1: Install DBSnapper", "text": "<p>Choose your preferred installation method:</p> <pre><code># macOS with Homebrew\nbrew install dbsnapper/tap/dbsnapper\n\n# Or use Docker\ndocker pull ghcr.io/dbsnapper/dbsnapper:latest\n</code></pre>"}, {"location": "quick-start/#step-2-connect-to-dbsnapper-cloud", "title": "Step 2: Connect to DBSnapper Cloud", "text": "<pre><code>dbsnapper config init\ndbsnapper auth token YOUR_AUTH_TOKEN\n</code></pre>"}, {"location": "quick-start/#step-3-configure-team-storage", "title": "Step 3: Configure Team Storage", "text": "<p>Add your team's storage profile in the cloud dashboard or config:</p> <pre><code>storage_profiles:\n  team_storage:\n    provider: s3\n    bucket: your-team-snapshots\n    awscli_profile: production\n</code></pre>"}, {"location": "quick-start/#step-4-create-team-targets", "title": "Step 4: Create Team Targets", "text": "<p>Configure targets with sanitization and sharing:</p> <pre><code>targets:\n  production_app:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/app\"\n      dst_url: \"postgresql://user:pass@dev:5432/app_dev\"\n    storage_profile: team_storage\n    sanitize:\n      query_file: \"sanitize.sql\"\n    # Enable team sharing via SSO groups\n    sso_groups: [\"developers\", \"qa-team\"]\n</code></pre>"}, {"location": "quick-start/#step-5-build-and-share-snapshots", "title": "Step 5: Build and Share Snapshots", "text": "<pre><code># Build original and sanitized snapshots\ndbsnapper sanitize production_app -n\n\n# Team members can now access via shared targets\ndbsnapper targets --shared\n</code></pre> <p>\ud83c\udf89 Success! Your team can now access sanitized snapshots securely.</p>"}, {"location": "quick-start/#step-1-install-vscode-extension", "title": "Step 1: Install VSCode Extension", "text": "<p>Install the DBSnapper Extension from the VS Code Marketplace.</p> <p>Configure your workspace settings:</p> <pre><code>{\n    \"dbsnapper.configPath\": \"~/.config/dbsnapper/dbsnapper.yml\",\n    \"dbsnapper.autoRefresh\": true\n}\n</code></pre>"}, {"location": "quick-start/#step-2-set-up-github-actions", "title": "Step 2: Set Up GitHub Actions", "text": "<p>Add the DBSnapper GitHub Action to your workflow:</p> <pre><code># .github/workflows/db-snapshots.yml\nname: Database Snapshots\non: \n  schedule:\n    - cron: '0 2 * * *'  # Daily at 2 AM\n\njobs:\n  snapshot:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: dbsnapper/install-dbsnapper-agent-action@v1\n        with:\n          version: latest\n      - name: Create Snapshot\n        env:\n          DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n          DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n        run: dbsnapper build production-db\n</code></pre>"}, {"location": "quick-start/#step-3-use-terraform-provider", "title": "Step 3: Use Terraform Provider", "text": "<p>Manage DBSnapper resources with Infrastructure as Code:</p> <pre><code>terraform {\n  required_providers {\n    dbsnapper = {\n      source = \"dbsnapper/dbsnapper\"\n    }\n  }\n}\n\nresource \"dbsnapper_target\" \"production\" {\n  name = \"production-api\"\n  snapshot = {\n    src_url = var.database_url\n    dst_url = var.dev_database_url\n  }\n  storage_profile = \"team-s3\"\n}\n</code></pre>"}, {"location": "quick-start/#step-4-enable-mcp-server-optional", "title": "Step 4: Enable MCP Server (Optional)", "text": "<p>For AI assistant integration:</p> <pre><code># Start MCP server\ndbsnapper mcp\n\n# Configure your AI assistant to use the MCP server\n# Server runs on default port for Claude Desktop/other clients\n</code></pre> <p>\ud83c\udf89 Success! DBSnapper is now integrated into your development workflow.</p>"}, {"location": "quick-start/#whats-next", "title": "What's Next?", "text": "<p>Now that you have DBSnapper running, explore these powerful features:</p>"}, {"location": "quick-start/#data-sanitization", "title": "\ud83d\udee1\ufe0f Data Sanitization", "text": "<p>Remove sensitive data from your snapshots: - Sanitization Guide - Learn sanitization strategies - Configuration Examples - Set up automated data cleaning</p>"}, {"location": "quick-start/#database-subsetting", "title": "\ud83d\udcca Database Subsetting", "text": "<p>Create smaller, focused snapshots: - Subset Introduction - Understand subsetting concepts - Configuration Guide - Set up intelligent data filtering</p>"}, {"location": "quick-start/#team-collaboration", "title": "\u2601\ufe0f Team Collaboration", "text": "<p>Share snapshots securely with your team: - DBSnapper Cloud - Central snapshot management - SSO Setup - Configure team authentication</p>"}, {"location": "quick-start/#performance-optimization", "title": "\u26a1 Performance Optimization", "text": "<p>Optimize for your workload: - CPU Configuration - Multi-core snapshot processing - Storage Profiles - Efficient cloud storage</p>"}, {"location": "quick-start/#need-help", "title": "Need Help?", "text": ""}, {"location": "quick-start/#common-issues", "title": "Common Issues", "text": "<p>\"Connection failed\" - Check your database URLs and network access</p> <p>\"Tools not found\" - Install database client tools or use Docker mode: <pre><code># Check what's available\ndbsnapper config check\n\n# Use Docker-based tools\ndocker run -it ghcr.io/dbsnapper/dbsnapper:latest\n</code></pre></p> <p>\"Permission denied\" - Ensure database user has required permissions: - <code>SELECT</code> access on source database - <code>CREATE DATABASE</code> access on destination server</p>"}, {"location": "quick-start/#get-support", "title": "Get Support", "text": "<ul> <li>Configuration Reference - Complete configuration options</li> <li>Command Reference - Full CLI documentation  </li> <li>GitHub Issues - Report bugs or request features</li> <li>Community Discussions - Get help from the community</li> </ul> <p>Ready to dive deeper? Check out our comprehensive configuration guide for advanced features and customization options.</p>"}, {"location": "release-notes/", "title": "Release Notes", "text": "<p>Full release notes</p> <p>For a complete list of changes, see the DBSnapper Releases Page which will include all changes, bug fixes, and enhancements.</p>"}, {"location": "release-notes/#v270-team-sharing-for-sso-groups", "title": "v2.7.0 - Team Sharing for SSO Groups", "text": "<p>This release introduces the ability to share targets and snapshots with your team members via their assigned groups in their SSO provider (currently Okta). You can specify the groups on the Targets page and the agent will include these targets and snapshots for any members of the group.</p> <p>Other Changes:</p> <ul> <li>Using presigned URLs for snapshot uploads and downloads from cloud storage.</li> <li>Added flags <code>--original</code> and <code>--destdb</code> to the <code>targets</code> and <code>target</code> commands so these can be used when <code>pull</code>ing or <code>load</code>ing snapshots in the terminal UI.</li> <li>Specify a default destination database (used for loading shared targets) in the configuration file like so:</li> </ul> <pre><code>defaults:\n  shared_target_dst_db_url: postgres://postgres:postgres@localhost:5432/dbsnapper_dst_db_default\n</code></pre>"}, {"location": "release-notes/#v260-overrides-sanitization-query", "title": "v2.6.0 - Overrides: Sanitization Query", "text": "<p>You may now specify a sanitization query that can be used to override all sanitization operations. This can be provided in the configuration file or as an environment variable and must be base-64 encoded:</p> <pre><code>override:\n  san_query: RFJPUCBUQUJMRSBJRiBFWElTVFMgZGJzbmFwcGVyX2luZm87CkNSRUFURSBUQUJMRSBkYnNuYXBwZXJfaW5mbyAoY3JlYXRlZF9hdCB0aW1lc3RhbXAsIHRhZ3MgdGV4dCBbXSk7CklOU0VSVCBJTlRPIGRic25hcHBlcl9pbmZvIChjcmVhdGVkX2F0LCB0YWdzKQpWQUxVRVMgKE5PVygpLCAne3F1ZXJ5OnNhbl9xdWVyeV9vdmVycmlkZSwgbG9jYXRpb246Y2xvdWR9Jyk7\n</code></pre> <p>Or via an environment variable:</p> <pre><code>DBSNAPPER_OVERRIDE__SAN_QUERY=RFJPUCBUQUJMRSBJRiBFWElTVFMgZGJzbmFwcGVyX2luZm87CkNSRUFURSBUQUJMRSBkYnNuYXBwZXJfaW5mbyAoY3JlYXRlZF9hdCB0aW1lc3RhbXAsIHRhZ3MgdGV4dCBbXSk7CklOU0VSVCBJTlRPIGRic25hcHBlcl9pbmZvIChjcmVhdGVkX2F0LCB0YWdzKQpWQUxVRVMgKE5PVygpLCAne3F1ZXJ5OnNhbl9xdWVyeV9vdmVycmlkZV9FTlYsIGxvY2F0aW9uOmNsb3VkfScpOw== dbsnapper sanitize dvdrental-san -n\n</code></pre> <p>Note: Remember to base-64 encode the query before providing it in the configuration file or as an environment variable.</p>"}, {"location": "release-notes/#v253-detect-terminal-tty", "title": "v2.5.3 - Detect Terminal (TTY)", "text": "<ul> <li>TTY detection has been added to the <code>targets</code> and <code>target</code> commands. If a TTY is detected, the interactive Terminal UI will load, otherwise a non-interactive ascii table will be displayed.</li> </ul>"}, {"location": "release-notes/#v252-bugfixes", "title": "v2.5.2 - Bugfixes", "text": "<ul> <li>Fix parsing snapshot filename with multiple underscores.</li> </ul>"}, {"location": "release-notes/#v251-decrypt-authtoken-when-provided-via-environment-bugfixes", "title": "v2.5.1 - Decrypt Authtoken When Provided via Environment + bugfixes", "text": "<ul> <li>Will now attempt to decrypt the authtoken if an encrypted authtoken is provided via the DBSNAPPER_AUTHTOKEN environment variable.</li> </ul>"}, {"location": "release-notes/#v250-docker-in-docker-changes-environment-variables-destination-db-override-flag", "title": "v2.5.0 - Docker in Docker Changes + Environment Variables + Destination DB Override Flag", "text": "<p>This release fixes some issues when running DBSnapper agent in a Docker-in-Docker (DinD) environment and adds some additional nice features.</p>"}, {"location": "release-notes/#docker-in-docker-ephemeral-sanitization-changes", "title": "Docker in Docker Ephemeral Sanitization Changes", "text": "<p>This was caused by limitations in the way Docker manages DinD containers and directory mounts. In summary - when creating a container to do dumps and restores in a DinD environment, the mountpoint was relative to the Host and not the DinD container. This caused the container to not be able to access the mounted directory. We now use the local database tools which are installed in the dbsnapper agent container to do the dumps and restores, avoiding this issue.</p> <p>The ephemeral container is still started as normal in a DinD environment and will be placed in the <code>dbsnapper</code> network. Using a non-default Docker network is necessary to ensure the Docker internal DNS resolver is used to resolve container hostnames correctly.</p>"}, {"location": "release-notes/#notes-ephemeral-sanitization-in-dind-environments", "title": "Notes - Ephemeral sanitization in DinD environments:", "text": "<ul> <li>DBSnapper, by default, specifies <code>dbsnapper</code> for the docker network for any containers created.</li> <li>If you are running the DBSnapper docker image and planning to do an ephemeral sanitization, you'll need to</li> <li>Mount the docker socket to the container <code>-v /var/run/docker.sock:/var/run/docker.sock</code></li> <li>Specify the <code>dbsnapper</code> network for the container <code>--network dbsnapper</code></li> </ul>"}, {"location": "release-notes/#environment-variables", "title": "Environment Variables", "text": "<p>This release also improves handling of environment variables. You can now execute the docker container without the need to initialize the configuration file first. You can pass the environment variables directly to the docker run command. For example:</p>"}, {"location": "release-notes/#interactive-agent-example", "title": "Interactive Agent Example", "text": "<pre><code>docker run -it -e DBSNAPPER_SECRET=XXX -e DBSNAPPER_AUTHTOKEN=YYY -v /var/run/docker.sock:/var/run/docker.sock --rm --network dbsnapper --pull always ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n</code></pre> <p>This will start the DBSnapper agent container in interactive mode with the following settings:</p> <ul> <li>With the <code>DBSNAPPER_SECRET</code> and <code>DB_SNAPPER_AUTHTOKEN</code> environment variables set. Both are required to execute the agent without a configuration file.</li> <li>With the docker socket mounted to the container <code>-v /var/run/docker.sock:/var/run/docker.sock</code> (for ephemeral sanitization)</li> <li>In the <code>dbsnapper</code> network <code>--network dbsnapper</code></li> <li>Will pull the latest DBSnapper image <code>--pull always</code></li> <li>Will remove the container after it exits <code>--rm</code></li> </ul>"}, {"location": "release-notes/#one-liner-snapshot-example", "title": "One-Liner Snapshot Example", "text": "<p>A one-liner that will launch the DBSnapper agent container, build a snapshot, upload it to the cloud, and exit:</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -e DBSNAPPER_SECRET_KEY=XXX -e DBSNAPPER_AUTHTOKEN=YYY --rm --network dbsnapper  --pull always ghcr.io/dbsnapper/dbsnapper:latest dbsnapper build dvdrental-cloud\n</code></pre> <p>This command will output:</p> <pre><code>Creating working directory at: /root/.dbsnapper\nDBSnapper Agent - Version: 2.5.0\nDBSnapper Cloud: Enabled\n\nSTART: Build Snapshot for target: dvdrental-cloud with engine: postgres-local\n--&gt; Zipping snapshot to /root/.dbsnapper/1715957602_dvdrental-cloud.zip\n--&gt; Uploading snapshot to Cloud Storage Profile: cloudflare-r2 - r2://dbsnapper-r2/dbs-production-bucket/e9b33f60-d791-4d73-b9d2-30f9adad2fde.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: e9b33f60-d791-4d73-b9d2-30f9adad2fde\nFINISH: Building DB Snapshot for target: dvdrental-cloud\n</code></pre>"}, {"location": "release-notes/#one-liner-sanitize-example", "title": "One-Liner Sanitize Example", "text": "<p>A one-liner that will launch the DBSnapper agent container, create a new original and sanitized snapshot set, upload them to cloud storage, and exit:</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -e DBSNAPPER_SECRET_KEY=XXX -e DBSNAPPER_AUTHTOKEN=YYY  --rm --network dbsnapper --pull always ghcr.io/dbsnapper/dbsnapper:latest dbsnapper sanitize dvdrental-cloud -n\n</code></pre> <p>Which outputs:</p> <pre><code>latest: Pulling from dbsnapper/dbsnapper\nDigest: sha256:2d0b3c054942bdea88bde16e8b49a69d1249a0ea20b78b69508b23e4ee23f92d\nStatus: Image is up to date for ghcr.io/dbsnapper/dbsnapper:latest\nCreating working directory at: /root/.dbsnapper\nDBSnapper Agent - Version: 2.5.0 (c2d2ec052fe4) Build Date: 2024-05-17T16:21:43Z\nDBSnapper Cloud: Enabled\n\nRunning ephemeral sanitization\nCreating a new snapshot set\n--&gt; Building NEW original snapshot\n--&gt; Zipping snapshot to /root/.dbsnapper/1715963288_dvdrental-san.zip\n--&gt; Created ephemeral container: animal-belief, database: pgdocker://dbsnapper:dbsnapper@nutrition/troupe\n--&gt; Restoring snapshot to Sanitization DB\n--&gt; Sanitizing Snapshot\n--&gt; Building sanitized snapshot\n--&gt; Pushing original snapshot to cloud\n--&gt; Uploading snapshot to Cloud Storage Profile: R2-Original - r2://dbsnapper/original/0bd58a2a-4db1-49a8-8917-f513d829c31b.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: 0bd58a2a-4db1-49a8-8917-f513d829c31b\n--&gt; Pushing sanitized snapshot to cloud\n--&gt; Uploading snapshot to Cloud Storage Profile: R2-Sanitized - r2://dbsnapper/sanitized/cb062d1f-cf2d-4bae-a1b0-489853e5a500.san.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: cb062d1f-cf2d-4bae-a1b0-489853e5a500\nSanitize Complete\n</code></pre>"}, {"location": "release-notes/#environment-variables-for-non-docker-environments", "title": "Environment Variables for Non-Docker Environments", "text": "<p>Of course, you can always use envirnoment variables when running the DBSnapper agent in a non-Docker environment as well.</p> <pre><code>DBSNAPPER_SECRET_KEY=XXX DBSNAPPER_AUTHTOKEN=YYY dbsnapper targets\n</code></pre> <p>This will list all targets using the specified secret key and authtoken. If you don't have a configuration file, it will end up listing all targets created in the DBSnapper cloud.</p>"}, {"location": "release-notes/#destination-database-override-flag", "title": "Destination Database Override Flag", "text": "<p>The <code>load</code> command now supports a <code>--destdb</code> flag that can be used to override the snapshot destination database specified in the target configuration. The following example overrides the database set for the <code>target-name</code> target with the <code>postgres://postgres:postgres@localhost:5432/database_snapshot_override</code> database:</p> <pre><code>dbsnapper load target-name --destdb=\"postgres://postgres:postgres@localhost:5432/database_snapshot_override\"\n</code></pre>"}, {"location": "release-notes/#v240-ephemeral-sanitization-support", "title": "v2.4.0 - Ephemeral Sanitization Support", "text": "<p>This release is bringing back the ability to use ephemeral containers for sanitization. This streamlines the sanitization process, leveraging containers to spin up a temporary database that can be used to sanitize the unsanitized snapshot data.</p> <p>The <code>sanitize</code> command now behaves as follows:</p> <ol> <li>It will create a new unsanitized and sanitized snapshot set if no snapshots exist for a target or the <code>-n</code> flag is set.</li> <li>Will use an ephemeral container to sanitize the data if the <code>sanitize: dst_url</code> is not specified in the configuration file, or the <code>-e</code> flag is set. </li> </ol> <p>You can combine both the <code>-n</code> and <code>-e</code> flags to create a new snapshot set and use an ephemeral container for sanitization.</p>"}, {"location": "release-notes/#v230-new-user-interface-share-targets-storage-engines-improvements-and-more", "title": "v2.3.0 - New User Interface, Share Targets, Storage Engines Improvements, and More", "text": "<p>A Terminal User Interface (TUI) has been added to the DBSnapper Agent, making it even easier to use. See all your targets, drill down into their snapshots, and load them all from the new UI.</p> <p> DBSnapper Agent User Interface - All Targets </p> <p>Sharing Targets have been added to DBSnapper. Leveraging the ability to specify different storage profiles for original and sanitized snapshots, you can now create a share target in your configuration file, that will allow you to list and load sanitized snapshots from a shared storage location. This is useful for sharing sanitized snapshots with developers, testers, and other stakeholders.</p> <p>New Storage Engines have been added. In addition to our support for AWS S3 and CloudFlare R2, we have added support for Minio and Digital Ocean Spaces</p> <p>Storage Engines now support retrieving credentials from  the AWS CLI shared configuration. It is now possible to retrieve S3 compatible storage engine credentials from environment variables, or you can specify an <code>awscli_profile</code> in your storage profile configuration to use the credentials from the specified AWS CLI profile. More information on this can be found in the Storage Engine Configuration documentation.</p>"}, {"location": "release-notes/#v220-separate-storage-profiles-for-unsanitized-and-sanitized-snapshots", "title": "v2.2.0 - Separate Storage Profiles for Unsanitized and Sanitized Snapshots", "text": "<p>You can now specify different storage profiles for unsanitized (original) and sanitized snapshots, allowing you to store them in different buckets or cloud providers if desired.  This will allow sharing only the sanitized snapshot cloud storage buckets with developers, while keeping the unsanitized snapshots private.</p> <p>Up next is additional sharing functionality for accessing and loading the sanitized snapshots.</p> <p>Download the v2.2.0 release for your platform.</p>"}, {"location": "release-notes/#v210-connection-string-url-templates", "title": "v2.1.0 - Connection String URL Templates", "text": "<p>All connection string URLs now support templating. This allows you to access environment variables in the connection string URLs. For example, you can now use the following connection string URL for a Postgres database:</p> <pre><code>snapshot:\n  src_url: postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@localhost:5432/{{`DB_NAME` | env}}\n</code></pre> <p>In this example we are indicating we want the username, password, and database name to be read from the <code>DB_USER</code>, <code>DB_PASSWORD</code>, and <code>DB_NAME</code> environment variables, respectively.</p> <p>Templates conform to Go Templates syntax. Specify the <code>env</code> function to read the value from the environment.</p> <pre><code>{{`ENV_VAR` | env}} # substitute the value of the ENV_VAR environment variable\n{{`CONSTANT`}} # substitute the supplied `CONSTANT` value\n</code></pre>"}, {"location": "release-notes/#v200-subsetting", "title": "v2.0.0 - Subsetting!", "text": "<p>We're excited to announce the release of DBSnapper v2.0, which introduces a major new feature: Database Subsetting. This feature allows you to create a relationally consitent copy of your database that contains only a subset of the data. This is useful for creating smaller, more manageable datasets for development and testing.</p> <p>Backwards Compatibility</p> <p>This release introduces a new configuration file format and options. If you are upgrading from a previous version, you will need to update your configuration file to the new format. See the Configuration Settings documentation for more information.</p>"}, {"location": "release-notes/#additional-improvements", "title": "Additional Improvements", "text": "<ul> <li>Improved support for MySQL databases.</li> <li>Support for PostgreSQL COPY protocol for fast data copy operations.</li> <li>Simplified the sanitization command, eliminating the use of ephemeral database containers,</li> <li>Released Docker images for easier installation and use.</li> <li>An extensive refactoring and testing of the codebase to improve performance, quality, and maintainability.</li> <li>Improved documentation and examples.</li> </ul>"}, {"location": "requirements/", "title": "Requirements", "text": "<p>DBSnapper is designed to work across diverse environments, from local development machines to enterprise container orchestration platforms. Here's what you need to get started.</p>"}, {"location": "requirements/#system-requirements", "title": "System Requirements", "text": ""}, {"location": "requirements/#supported-platforms", "title": "Supported Platforms", "text": "<p>DBSnapper runs on all major platforms with pre-built binaries available from the GitHub Releases page:</p> <ul> <li>macOS - Universal binary supporting both Intel (x86_64) and Apple Silicon (ARM64) architectures</li> <li>Linux - Full compatibility across distributions:</li> <li>x86_64 - Standard 64-bit Intel/AMD processors</li> <li>ARM64 - ARM 64-bit processors (AWS Graviton, Raspberry Pi 4+, etc.)</li> <li>Distributions: Ubuntu, Debian, RHEL, CentOS, Fedora, Alpine Linux, and others</li> </ul>"}, {"location": "requirements/#hardware-requirements", "title": "Hardware Requirements", "text": "<p>Minimum System Requirements: - CPU: 1 core (multi-core recommended for large databases) - Memory: 512MB RAM minimum, 2GB+ recommended - Storage: 100MB for installation + space for snapshots - Network: Internet access for DBSnapper Cloud integration (optional)</p> <p>Recommended for Production: - CPU: 4+ cores for optimal snapshot performance - Memory: 4GB+ RAM for large database operations - Storage: SSD storage for working directory - Network: High-bandwidth connection for cloud storage operations</p>"}, {"location": "requirements/#performance-scaling", "title": "Performance Scaling", "text": "<p>DBSnapper v3.0 introduces configurable CPU usage for database operations:</p> <pre><code># Configure CPU usage globally\ndefaults:\n  cpus: 4  # Use 4 CPU cores for all operations\n\n# Override per target for large databases\ntargets:\n  production_db:\n    cpus: 8  # Use 8 cores for this target\n</code></pre> <p>Performance Benefits: - PostgreSQL: Multi-core <code>pg_dump</code> and <code>pg_restore</code> operations - Large datasets: Significantly faster snapshot creation and restoration - Concurrent operations: Better resource utilization in container environments</p>"}, {"location": "requirements/#database-support", "title": "Database Support", "text": ""}, {"location": "requirements/#supported-database-engines", "title": "Supported Database Engines", "text": "<p>DBSnapper leverages native database utilities for maximum compatibility and performance:</p>"}, {"location": "requirements/#postgresql", "title": "PostgreSQL", "text": "<ul> <li>Versions: PostgreSQL 10, 11, 12, 13, 14, 15, 16+</li> <li>Deployment types: Local, Docker, RDS, Aurora, Cloud SQL, self-hosted</li> <li>Advanced features:</li> <li>Schema filtering (include/exclude specific schemas)</li> <li>Multi-core parallel operations</li> <li>Connection pooling support</li> <li>SSL/TLS encryption</li> </ul>"}, {"location": "requirements/#mysql", "title": "MySQL", "text": "<ul> <li>Versions: MySQL 5.7, 8.0+, MariaDB 10.3+</li> <li>Deployment types: Local, Docker, RDS, Aurora, Cloud SQL, self-hosted</li> <li>Features:</li> <li>Full database snapshots</li> <li>Multi-core operations (when supported)</li> <li>SSL/TLS encryption</li> </ul>"}, {"location": "requirements/#database-access-requirements", "title": "Database Access Requirements", "text": "<p>Source Database Permissions: - <code>SELECT</code> privilege on all tables to be snapshotted - <code>SHOW DATABASES</code> privilege (MySQL) or connection access (PostgreSQL) - For schema operations: appropriate schema-level permissions</p> <p>Destination Database Permissions: - <code>CREATE DATABASE</code> privilege on the destination server - <code>DROP DATABASE</code> privilege (snapshots replace existing databases) - Full read/write access to the target database</p> <p>Security Best Practices: - Use dedicated DBSnapper database users with minimal required permissions - Enable SSL/TLS connections for production databases - Consider read replicas for snapshot sources to reduce production load</p>"}, {"location": "requirements/#database-client-tools", "title": "Database Client Tools", "text": "<p>DBSnapper requires database client tools for snapshot operations. You have two options:</p>"}, {"location": "requirements/#option-1-docker-based-tools-recommended", "title": "Option 1: Docker-Based Tools (Recommended)", "text": "<p>Use the DBSnapper Docker image with all tools pre-installed:</p> <pre><code>docker pull ghcr.io/dbsnapper/dbsnapper:latest\n</code></pre> <p>Benefits: - All tools included and version-tested - Consistent environment across different systems - No local tool installation required - Perfect for CI/CD pipelines and containers</p>"}, {"location": "requirements/#option-2-local-tools-installation", "title": "Option 2: Local Tools Installation", "text": "<p>Install database client tools on your local system:</p> macOSUbuntu/DebianRHEL/CentOS/FedoraAlpine Linux <pre><code># PostgreSQL tools\nbrew install postgresql\n\n# MySQL tools\nbrew install mysql-client\n</code></pre> <pre><code># PostgreSQL tools\nsudo apt-get install postgresql-client\n\n# MySQL tools\nsudo apt-get install mysql-client\n</code></pre> <pre><code># PostgreSQL tools\nsudo dnf install postgresql\n\n# MySQL tools\nsudo dnf install mysql\n</code></pre> <pre><code># PostgreSQL tools\nsudo apk add postgresql-client\n\n# MySQL tools\nsudo apk add mysql-client\n</code></pre> <p>Required Tools: - PostgreSQL: <code>psql</code>, <code>pg_dump</code>, <code>pg_restore</code> - MySQL: <code>mysql</code>, <code>mysqldump</code></p>"}, {"location": "requirements/#container-and-orchestration-support", "title": "Container and Orchestration Support", "text": ""}, {"location": "requirements/#docker-requirements", "title": "Docker Requirements", "text": "<ul> <li>Docker Engine: Version 18.09+ recommended</li> <li>Docker permissions: User must be able to run Docker containers</li> <li>Network access: Container networking for database connections</li> </ul>"}, {"location": "requirements/#kubernetescontainer-orchestration", "title": "Kubernetes/Container Orchestration", "text": "<p>DBSnapper works excellently in container orchestration platforms:</p> <p>Resource Requirements: <pre><code>resources:\n  requests:\n    memory: \"512Mi\"\n    cpu: \"250m\"\n  limits:\n    memory: \"2Gi\"\n    cpu: \"2000m\"  # Adjust based on CPU configuration\n</code></pre></p> <p>Volume Requirements: - Configuration volume: <code>~/.config/dbsnapper</code> - Working directory volume: <code>~/.dbsnapper</code> (for local snapshots) - Optional: Shared storage for team access</p>"}, {"location": "requirements/#cicd-integration", "title": "CI/CD Integration", "text": "<p>DBSnapper integrates seamlessly with CI/CD pipelines:</p> <p>GitHub Actions: <pre><code>- uses: dbsnapper/install-dbsnapper-agent-action@v1\n  with:\n    version: latest\n</code></pre></p> <p>GitLab CI, Jenkins, etc.: Use the Docker image for consistent environments</p>"}, {"location": "requirements/#cloud-storage-integration", "title": "Cloud Storage Integration", "text": ""}, {"location": "requirements/#supported-storage-providers", "title": "Supported Storage Providers", "text": "<p>DBSnapper follows a \"Bring Your Own Cloud Storage\" model for maximum security and flexibility:</p>"}, {"location": "requirements/#amazon-s3", "title": "Amazon S3", "text": "<ul> <li>AWS S3 (all regions)</li> <li>S3-compatible services (MinIO, DigitalOcean Spaces, etc.)</li> <li>Authentication: AWS CLI profiles, IAM roles, access keys</li> <li>Features: Server-side encryption, versioning, lifecycle policies</li> </ul>"}, {"location": "requirements/#cloudflare-r2", "title": "Cloudflare R2", "text": "<ul> <li>Cloudflare R2 storage</li> <li>Authentication: API tokens, S3-compatible credentials</li> <li>Benefits: Zero egress fees, global distribution</li> </ul>"}, {"location": "requirements/#additional-s3-compatible-providers", "title": "Additional S3-Compatible Providers", "text": "<ul> <li>MinIO: Self-hosted object storage</li> <li>DigitalOcean Spaces: Managed object storage</li> <li>Backblaze B2: Cost-effective cloud storage</li> <li>Wasabi: Hot cloud storage</li> </ul>"}, {"location": "requirements/#storage-requirements", "title": "Storage Requirements", "text": "<p>Capacity Planning: - Snapshot sizes typically 10-50% of original database size (due to compression) - Plan for multiple snapshots per target (original + sanitized versions) - Consider retention policies for automated cleanup</p> <p>Performance Considerations: - High-bandwidth network connections for large database uploads - Consider storage regions close to your database infrastructure - Use presigned URLs for secure, direct uploads/downloads</p>"}, {"location": "requirements/#network-and-security-requirements", "title": "Network and Security Requirements", "text": ""}, {"location": "requirements/#network-access", "title": "Network Access", "text": "<p>Outbound connections required for: - Database servers (source and destination) - DBSnapper Cloud API (if using cloud features): <code>app.dbsnapper.com</code> - Cloud storage providers (S3, R2, etc.) - Container registry: <code>ghcr.io</code> (for Docker images)</p> <p>Firewall Considerations: - Standard database ports: PostgreSQL (5432), MySQL (3306) - HTTPS (443) for cloud API and storage operations - Custom ports if using non-standard database configurations</p>"}, {"location": "requirements/#security-requirements", "title": "Security Requirements", "text": "<p>Encryption: - In transit: TLS 1.2+ for all network communications - At rest: AES-256 encryption for stored configurations and snapshots - Database connections: SSL/TLS supported and recommended</p> <p>Authentication: - Support for all standard database authentication methods - Integration with AWS IAM, OIDC, and other identity providers - Optional: SSO integration with Okta, Auth0, Azure AD</p>"}, {"location": "requirements/#development-environment-integration", "title": "Development Environment Integration", "text": ""}, {"location": "requirements/#ide-support", "title": "IDE Support", "text": "<p>VS Code Extension: DBSnapper for VS Code - In-editor snapshot management - Target configuration and testing - Direct integration with workspace settings</p>"}, {"location": "requirements/#mcp-server-integration", "title": "MCP Server Integration", "text": "<p>AI Assistant Integration: - Model Context Protocol (MCP) server for Claude Desktop and compatible AI assistants - Programmatic database operations through AI conversation - Requires DBSnapper installation and configuration</p>"}, {"location": "requirements/#terraform-provider", "title": "Terraform Provider", "text": "<p>Infrastructure as Code: DBSnapper Terraform Provider - Manage DBSnapper resources with Terraform - Integration with existing infrastructure automation - Supports all major DBSnapper configuration options</p>"}, {"location": "requirements/#compatibility-matrix", "title": "Compatibility Matrix", "text": "Feature Local Mode Cloud Mode Docker Kubernetes CI/CD PostgreSQL Support \u2705 \u2705 \u2705 \u2705 \u2705 MySQL Support \u2705 \u2705 \u2705 \u2705 \u2705 Schema Filtering \u2705 \u2705 \u2705 \u2705 \u2705 Multi-Core Operations \u2705 \u2705 \u2705 \u2705 \u2705 Cloud Storage \u274c \u2705 \u2705 \u2705 \u2705 Team Sharing \u274c \u2705 \u2705 \u2705 \u2705 SSO Integration \u274c \u2705 \u2705 \u2705 \u2705 MCP Server \u2705 \u2705 \u2705 \u2705 \u274c"}, {"location": "requirements/#getting-started", "title": "Getting Started", "text": "<p>With these requirements in mind, choose your deployment approach:</p> <ol> <li>Quick Trial: Use the Docker image - no local setup required</li> <li>Local Development: Install natively for your platform  </li> <li>Team Setup: Configure DBSnapper Cloud for collaboration</li> <li>Production Deployment: Use container orchestration for scale</li> </ol> <p>Next: Ready to install? Check out our Installation Guide for step-by-step instructions.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/", "title": "Backup Amazon RDS Databases with DBSnapper", "text": "<p>DBSnapper is a powerful tool for creating and managing backups of your databases. It is designed to be easy to use, and can be installed and configured in minutes. In this tutorial, we'll show you how to use DBSnapper to backup an Amazon RDS database deployed in a private VPC.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#overview", "title": "Overview", "text": "<p>In this tutorial we'll be using the following tools to backup the Amazon RDS database:</p> <ul> <li>DBSnapper Agent to facilitate the backup process</li> <li>DBSnapper Cloud to provide the target configuration and snapshot storage</li> <li>Docker to launch a DBSnapper container with the necessary database client tools.</li> <li>Amazon EC2 instance to connect to the Amazon RDS database</li> </ul> <p>Here's our target definition on the DBSnapper Cloud. We've simply provided the connection string of the Amazon RDS database as the source URL, and a Cloudflare R2 storage profile as the destination URL.</p> <p></p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#tldr-backup-in-four-commands", "title": "TL;DR Backup in Four Commands", "text": "<p>Update: You can now do this in a SINGLE command</p> <p>(copied from the DBSnapper Quick Start Guide)</p> <p>Just use the latest version of the DBSnapper container image, provide the minimum required <code>DBSNAPPER_SECRET_KEY</code> and <code>DBSNAPPER_AUTHTOKEN</code> environment variables necessary to run DBSnapper in Cloud mode, without a configuration file.</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -e DBSNAPPER_SECRET_KEY=XXX -e DBSNAPPER_AUTHTOKEN=YYY --rm --network dbsnapper --pull always ghcr.io/dbsnapper/dbsnapper:latest dbsnapper build dvdrental-cloud\n</code></pre> <p>This command does the following:</p> <ol> <li>It pulls the latest version of the DBSnapper Agent container image from the GitHub Container Registry,</li> <li>It mounts the host Docker socket. This is optional, but necessary for ephemeral sanitization operations requiring Docker-in-Docker (DinD) support.</li> <li>Passes the <code>DBSNAPPER_SECRET_KEY</code> and <code>DBSNAPPER_AUTHTOKEN</code> environment variables to the container</li> <li>Runs the <code>dbsnapper build dvdrental-cloud</code> command to build a snapshot of the <code>dvdrental-cloud</code> target defined in the DBSnapper Cloud.</li> <li>If a storage profile is defined in the DBSnapper Cloud, the snapshot will be uploaded to the configured storage provider.</li> </ol> Amazon RDS Backup in four commands<pre><code># Launch interactive DBSnapper Container from EC2 instance\ndocker run -it ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n\n# Create our configuration and connect to the DBSnapper Cloud\ndbsnapper config init\ndbsnapper auth token &lt;YOUR_DBSNAPPER_CLOUD_AUTH_TOKEN&gt;\n\n# Build the snapshot and upload to object storage\ndbsnapper build amazon-rds-postgres\n</code></pre>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#launch-an-ec2-instance", "title": "Launch an EC2 Instance", "text": "<p>To easily gain access to the Amazon RDS database, we'll launch an EC2 instance in the same VPC as the RDS database. This will allow us to run DBSnapper and connect to the RDS database without exposing the database to the public internet.</p> <p>For this instance I am using the <code>Canonical, Ubuntu, 22.04 LTS, arm64 (ami-05d47d29a4c2d19e1)</code> image. But you can use other Linux distributions as well.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#launch-an-interactive-dbsnapper-container", "title": "Launch an Interactive DBSnapper Container", "text": "<p>The DBSnapper Docker Images include the necessary database client tools for PostgreSQL and MySQL, so it is a quick way to get started with all the necessary dependencies.</p> <p>Docker Dependency</p> <p>This tutorial uses Docker to launch the container - if you need to install Docker, see the Optional: Install Docker Community Edition section below.</p> Launch DBSnapper Container &amp; Setup DBSnapper Agent<pre><code># Launch interactive DBSnapper Container\ndocker run -it ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n\n# Create our configuration and connect to the DBSnapper Cloud\ndbsnapper config init\ndbsnapper auth token &lt;YOUR_DBSNAPPER_CLOUD_AUTH_TOKEN&gt;\ndbsnapper targets\n</code></pre> <p>Using the commands above, we will launch an interactive DBSnapper container, initialize the DBSnapper configuration file, and authenticate with the DBSnapper Cloud using the authentication token provided on the Get Started page, and finally list the available targets.</p> Command output<pre><code>1c09173ca85b:~# dbsnapper config init\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Standalone Mode\n\nInitializing dbsnapper config at       : /root/.config/dbsnapper/dbsnapper.yml\nCreating dbsnapper working directory at: /root/.dbsnapper\n\n1c09173ca85b:~# dbsnapper auth token &lt;YOUR_DBSNAPPER_CLOUD_AUTH_TOKEN&gt;\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Standalone Mode\n\nCloud API token written to configuration file: /root/.config/dbsnapper/dbsnapper.yml\n\n1c09173ca85b:~# dbsnapper targets\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Enabled\n\nListing all targets\n+---------------------+----------+--------+------------------------------------------------------+------+-----+-----------------+-------+----------+\n|        NAME         | LOCATION | STATUS |                         SRC                          | SIZE | DST | STORAGE PROFILE | QUERY | MESSAGES |\n+---------------------+----------+--------+------------------------------------------------------+------+-----+-----------------+-------+----------+\n| amazon-rds-postgres | cloud    |        | pgl://dbsna...amazonaws.com:5432/rds_backup_tutorial | 0 B  |     | Cloudflare R2   | No    |          |\n+---------------------+----------+--------+------------------------------------------------------+------+-----+-----------------+-------+----------+\n</code></pre> <p>With three simple DBSnapper commands (highlighted) we've initialized the configuration file, authenticated with the DBSnapper Cloud, and listed the available targets. We see our Amazon RDS database listed as the <code>amazon-rds-postgres</code> target.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#backup-the-amazon-rds-database", "title": "Backup the Amazon RDS Database", "text": ""}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#build-the-snapshot", "title": "Build the Snapshot", "text": "<p>Now that we have the DBSnapper container running and have authenticated with the DBSnapper Cloud, we can simply use the <code>build</code> command to create a snapshot of the Amazon RDS database.</p> Backup Amazon RDS Database<pre><code>dbsnapper build amazon-rds-postgres\n</code></pre> Command Output<pre><code>1c09173ca85b:~# dbsnapper build amazon-rds-postgres\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Enabled\n\nSTART: Build Snapshot for target: amazon-rds-postgres with engine: postgres-local\n--&gt; Cloud target, Cloud storage, non-localhost DB.\n--&gt; Zipping snapshot 1708621936_amazon-rds-postgres to /root/.dbsnapper/1708621936_amazon-rds-postgres.zip\n--&gt; Uploading snapshot to Cloud Storage Profile: Cloudflare R2 - r2://dbsnapper-r2/dbsnapper-playground/cc6e21cb-e591-4a0e-8166-eba3c2a98dca.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: cc6e21cb-e591-4a0e-8166-eba3c2a98dca\nFINISH: Building DB Snapshot for target: amazon-rds-postgres\n</code></pre> <p>And there you have it. We've backed up the Amazon RDS database and it is now available in your cloud storage and can be retrieved and loaded by the DBSnapper Agent on any machine with access to the your DBSnapper Cloud account.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#list-the-snapshot", "title": "List the Snapshot", "text": "<p>We can verify that the snapshot was created by listing the snapshots for the target.</p> List Snapshots for the Target<pre><code>1c09173ca85b:~# dbsnapper target amazon-rds-postgres\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Enabled\n\nTables in target:  amazon-rds-postgres\n+---------------+------------+--------+\n|     NAME      | EST'D ROWS |  SIZE  |\n+---------------+------------+--------+\n| address       |        603 | 131 kB |\n| users         |          4 | 49 kB  |\n| store         |          2 | 41 kB  |\n| staff         |          2 | 33 kB  |\n| payment       |     14,596 | 1.9 MB |\n| film          |      1,000 | 934 kB |\n| inventory     |      4,581 | 451 kB |\n| film_category |      1,000 | 90 kB  |\n| language      |          6 | 25 kB  |\n| country       |        109 | 25 kB  |\n| film_actor    |      5,462 | 500 kB |\n| customer      |        599 | 188 kB |\n| city          |        600 | 90 kB  |\n| actor         |        200 | 49 kB  |\n| category      |         16 | 25 kB  |\n| rental        |     16,044 | 2.4 MB |\n+---------------+------------+--------+\nListing ALL snapshots for target: amazon-rds-postgres\n+-------+-------------------------+--------------------------------+------------------------------------------+--------+------------+-------+\n| INDEX |         CREATED         |              NAME              |                 FILENAME                 |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+--------------------------------+------------------------------------------+--------+------------+-------+\n|     0 | 2024-Feb-22 @ 17:12:17Z | 1708621936_amazon-rds-postgres | cc6e21cb-e591-4a0e-8166-eba3c2a98dca.zip | 613 kB | false      |       |\n+-------+-------------------------+--------------------------------+------------------------------------------+--------+------------+-------+\n</code></pre>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#optional-test-ec2-rds-connection", "title": "Optional: Test EC2 &lt;-&gt; RDS Connection", "text": "<p>To ensure we're able to connect to the Amazon RDS database from our EC2 Instance, we can use the <code>psql</code> command to connect to the database and list the available tables.</p> Connect to Amazon RDS Database<pre><code>psql -d postgres://postgres:&lt;PASSWORD&gt;dbsnapper-playground.&lt;RDS_HOSTNAME...&gt;:5432/rds_backup_tutorial\n\npsql (16.2, server 16.1)\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off)\nType \"help\" for help.\n\nrds_backup_tutorial=&gt; \\dt\n             List of relations\n Schema |     Name      | Type  |  Owner\n--------+---------------+-------+----------\n public | actor         | table | postgres\n public | address       | table | postgres\n public | category      | table | postgres\n public | city          | table | postgres\n public | country       | table | postgres\n public | customer      | table | postgres\n public | film          | table | postgres\n public | film_actor    | table | postgres\n public | film_category | table | postgres\n public | inventory     | table | postgres\n public | language      | table | postgres\n public | payment       | table | postgres\n public | rental        | table | postgres\n public | staff         | table | postgres\n public | store         | table | postgres\n public | users         | table | postgres\n(16 rows)\n\nrds_backup_tutorial=&gt;\n</code></pre>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#optional-install-docker-community-edition", "title": "Optional: Install Docker Community Edition", "text": "<p>We're using Docker for this tutorial so if your instance doesn't have it installed, the following commands will install and setup Docker community edition in rootless mode on your instance.</p> Install Docker Community Edition on Ubuntu 22.04 LTS<pre><code>curl -fsSL https://get.docker.com -o get-docker.sh &amp;&amp; \\\nsudo sh ./get-docker.sh\n\nsudo sh -eux &lt;&lt;EOF\n# Install newuidmap &amp; newgidmap binaries\napt-get install -y uidmap\nEOF\n\ndockerd-rootless-setuptool.sh install\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/", "title": "Creating Sanitized Database Snapshots with DBSnapper", "text": "<p>Updated Feb. 21 2024 for v2.0</p> <p>This tutorial has been updated to reflect the changes in DBSnapper v2.0.</p> <p>In this example, we're going to create a simple database with User account information. Our goal is to be able to create sanitized snapshot of this database that we can use in our development</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#create-a-database-and-add-some-data", "title": "Create a database and add some data", "text": "<p>Let's create a database and call it <code>example_app</code></p> <pre><code>psql -d 'postgres://postgres:postgres@localhost:15432?sslmode=disable' -c 'create database example_app;'\n</code></pre> <p>We'll create a simple users table with some basic user and authentication fields</p> <pre><code>DROP TABLE IF EXISTS users;\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    first_name text,\n    last_name text,\n    email character varying(110) unique not null,\n    password character varying(50) not null,\n    pin character varying(8)\n);\n</code></pre> <p>And we'll load some sample data</p> <pre><code>INSERT INTO users (first_name, last_name, email, password, pin) VALUES\n('John', 'Doe', 'johndoe@example.com', 'secretpassword', '2468'),\n('Jane', 'Doe', 'janedoe@example.com', 'ubersecretpassword', '1357'),\n('Fred', 'Smith', 'fsmith@dbsnapper.com', 'opensesame', '7890'),\n('Sam', 'Jackson', 'sj@example.com', 'iamsam', '1234');\n</code></pre> <p>Our database so far</p> <pre><code>example_app=# select * from users;\n id | first_name | last_name |        email         |      password      | pin\n----+------------+-----------+----------------------+--------------------+------\n  1 | John       | Doe       | johndoe@example.com  | secretpassword     | 2468\n  2 | Jane       | Doe       | janedoe@example.com  | ubersecretpassword | 1357\n  3 | Fred       | Smith     | fsmith@dbsnapper.com | opensesame         | 7890\n  4 | Sam        | Jackson   | sj@example.com       | iamsam             | 1234\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#create-a-target-configuration", "title": "Create a target configuration", "text": "<p>Our goal is to create a snapshot of the production <code>example_app</code> database that we can copy locally to our development database. Let's do that:</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code>docker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:latest\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: /Users/joescharf/.dbsnapper\ntargets:\n  example_app:\n    name: example_app\n    snapshot:\n      src_url: postgres://postgres:postgres@localhost:15432/example_app?sslmode=disable\n      dst_url: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n    sanitize:\n      dst_url:\n      query_file:\n</code></pre> <p>As you can see in the highlighted section, we're specifying <code>example_app</code> as our source datbase and <code>example_app_snap</code> as our destination</p> <p>Danger: Destination database <code>dst_url</code> will be DROPPED and RECREATED</p> <p>Remember, the any database specified on a <code>dst_url</code> attribute will be DROPPED and a new empty database with the same name will be CREATED prior to loading the data from the snapshot!</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-our-target", "title": "Checking our target", "text": "<p>Let's make sure our target is configured properly and we can access it:</p> <pre><code>dbsnapper targets\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha-dev Build Date: 2024-02-21T16:20:40-07:00\nDBSnapper Cloud: Standalone Mode\n\nListing all targets\n+-------------+----------+--------+-----------------------------------+--------+----------------------------------------+-----------------+-------+----------+\n|    NAME     | LOCATION | STATUS |                SRC                |  SIZE  |                  DST                   | STORAGE PROFILE | QUERY | MESSAGES |\n+-------------+----------+--------+-----------------------------------+--------+----------------------------------------+-----------------+-------+----------+\n| example_app | local    | OK     | pgl://localhost:15432/example_app | 7.8 MB | pgl://localhost:15432/example_app_snap |                 | No    |          |\n+-------------+----------+--------+-----------------------------------+--------+----------------------------------------+-----------------+-------+----------+```\n</code></pre> <p>STATUS shows OK and no errors, so we're good to go.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#build-a-snapshot", "title": "Build a snapshot", "text": "<p>The first step in our journey is creating the snapshot. This can be done easily with the <code>build</code> command:</p> <pre><code>$ dbsnapper build example_app\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Build Snapshot for target: example_app with engine: postgres-local\n--&gt; Local target, Local storage, non-localhost DB.\n--&gt; Zipping snapshot 1708557745_example_app to /Users/joescharf/.dbsnapper/1708557745_example_app.zip\nFINISH: Building DB Snapshot for target: example_app\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-our-snapshot", "title": "Checking our snapshot", "text": "<p>Now that we've successfully built a snapshot of our <code>example_app</code> database, let's check the snapshots for this target:</p> <pre><code>$ dbsnapper target example_app\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nListing ALL snapshots for target: example_app\n+-------+-------------------------+------------------------+----------------------------+--------+------------+-------+\n| INDEX |         CREATED         |          NAME          |          FILENAME          |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+-------+\n|     0 | 2024-Feb-21 @ 16:32:11Z | 1708558331_example_app | 1708558331_example_app.zip | 1.4 kB | false      |       |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+-------+\n</code></pre> <p>Great, we have a single snapshot available. Note that the <code>SANFN</code> (Sanitized FileName) cell is empty, indicating that we don't have a sanitized version of this snapshot. That's ok, we'll deal with that later.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#load-our-snapshot", "title": "Load our snapshot", "text": "<p>Now that we have our snapshot, let's load it into our development database specified on <code>dst_url</code> of our target configuration.</p> <pre><code>$ dbsnapper load example_app 0\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Loading Snapshot\n-&gt; LOADING original Snapshot #0: Name: 1708558331_example_app, Snapshot File: 1708558331_example_app.zip, Dest DB URL: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n--&gt; Using engine: postgres-local\n--&gt; Using Target: example_app\n--&gt; Pulling to local file: /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Local snapshot already exists at /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Pulled snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Unzipping snapshot /Users/joescharf/.dbsnapper/1708558331_example_app.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-1871306128\n--&gt; Dropping and recreating database pgl://localhost:15432/example_app_snap\n-&gt; LOADING Snapshot Completed for Target: example_app\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-our-new-database", "title": "Checking our new database", "text": "<p>Another success. Let's switch to the database and take a look at the data in the <code>users</code> table:</p> <pre><code>postgres=# \\l\n                                                          List of databases\n             Name              |  Owner   | Encoding |  Collate   |   Ctype    | ICU Locale | Locale Provider |   Access privileges\n-------------------------------+----------+----------+------------+------------+------------+-----------------+-----------------------\n example_app                   | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n example_app_snap              | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n\npostgres=# \\c example_app_snap\npsql (15.6 (Postgres.app), server 15.2 (Debian 15.2-1.pgdg110+1))\nYou are now connected to database \"example_app_snap\" as user \"postgres\".\nexample_app_snap=# select * from users;\n id | first_name | last_name |        email         |      password      | pin\n----+------------+-----------+----------------------+--------------------+------\n  1 | John       | Doe       | johndoe@example.com  | secretpassword     | 2468\n  2 | Jane       | Doe       | janedoe@example.com  | ubersecretpassword | 1357\n  3 | Fred       | Smith     | fsmith@dbsnapper.com | opensesame         | 7890\n  4 | Sam        | Jackson   | sj@example.com       | iamsam             | 1234\n(4 rows)\n</code></pre> <p>Great. So far we've made an exact copy of our <code>example_app</code> source database which is now available in the <code>example_app_snap</code> database.</p> <p>But we have a problem. Passing around a database snapshot with all this personal information (PII) and sensitive authentication data (passwords, pins) is a security issue! Eventually something will git misplaced or misused, so we'll need to deal with that.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#sanitizing-and-de-identifying-the-snapshot", "title": "Sanitizing and de-identifying the snapshot", "text": "<p>The sanitization process takes a query and executes it against a database snapshot. The resulting changes to the snapshot are exported and stored in a sanitized snapshot file.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#create-the-query", "title": "Create the query", "text": "<p>Let's create a sanitization query that will</p> <ol> <li>Obfuscate <code>first_name</code> and <code>last_name</code></li> <li>Change the <code>email</code> to match the <code>first_name</code> and <code>last_name</code></li> <li>Change the <code>password</code> and <code>pin</code> for all users to a common password used for development</li> </ol> example_app.san.sql<pre><code>-- sanitize the first_name, last_name\nupdate users u\nset first_name = 'User',\nlast_name = 'Id' || id;\n\n-- sanitize the email\nupdate users u\nset email = first_name || '_' || last_name || '@example.com';\n\n-- sanitize password and pin\nupdate users u\nset password = 'genericpassword',\npin = '0000';\n</code></pre> <p>Let's also create a table that will record the time the sanitization was performed and the name of the query file used:</p> example_app.san.sql<pre><code>-- Add a dbsnapper_info table to record timestamp of sanitizaiton\n\nDROP TABLE IF EXISTS dbsnapper_info;\n\nCREATE TABLE dbsnapper_info (\n  created_at timestamp,\n  tags text[]\n);\n\nINSERT INTO dbsnapper_info (created_at, tags)\n  VALUES (NOW(), '{example_app.san.sql}');\n</code></pre> <p>And let's save this file as <code>example_app.san.sql</code> in our <code>working_directory</code> (which defaults to <code>~/.dbsnapper</code>)</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#update-the-target-configuration", "title": "Update the target configuration", "text": "<p>Now we need to update our target configuration and specify the <code>query_file</code> that will be used for sanitization along with the <code>dst_url</code> for the sanitized snapshot:</p> <p>Destination URLs and DBSnapper</p> <p>DBSnapper uses separate <code>dst_url</code> attributes for the <code>snapshot</code> <code>subset</code>, and <code>sanitize</code> operations. This allows you to specify different destination databases for the different DBSnapper operations.</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code>docker:\n  images:\n    postgres: postgres:latest\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: /Users/joescharf/.dbsnapper\ntargets:\n  example_app:\n    name: example_app\n    snapshot:\n      src_url: postgres://postgres:postgres@localhost:15432/example_app?sslmode=disable\n      dst_url: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n    sanitize:\n      dst_url: postgres://postgres:postgres@localhost:15432/example_app_sanitized?sslmode=disable\n      query_file: \"example_app.san.sql\"\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#perform-the-sanitization", "title": "Perform the sanitization", "text": "<p>Now we're ready to run the <code>sanitize</code> command against the <code>example_app</code> target:</p> <pre><code>$ dbsnapper sanitize example_app 0\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Sanitize Snapshot #0, Name: 1708558331_example_app\n-&gt; LOADING original Snapshot #0: Name: 1708558331_example_app, Snapshot File: 1708558331_example_app.zip, Dest DB URL: postgres://postgres:postgres@localhost:15432/example_app_sanitized?sslmode=disable\n--&gt; Using engine: postgres-local\n--&gt; Using Target: example_app\n--&gt; Pulling to local file: /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Local snapshot already exists at /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Pulled snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Unzipping snapshot /Users/joescharf/.dbsnapper/1708558331_example_app.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-1231401554\n--&gt; Dropping and recreating database pgl://localhost:15432/example_app_sanitized\n-&gt; LOADING Snapshot Completed for Target: example_app\n--&gt; Executing sanitization query\n--&gt; Building sanitized snapshot\n--&gt; Zipping snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Sanitized snapshot '1708558331_example_app' created at /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n</code></pre> <p>This output shows the steps taken to sanitize the database which involve:</p> <ol> <li>Pulling the snapshot from the cloud or finding it locally on disk.</li> <li>Loading the snapshot into the database specified in the <code>dst_url</code> attribute.</li> <li>Running the sanitization query specified in <code>query_file</code> against the <code>dst_url</code> database.</li> <li>Dumping the sanitized database to a snapshot, compressing it, and uploading it to the cloud if configured.</li> </ol>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#check-the-sanitized-snapshot", "title": "Check the sanitized snapshot", "text": "<p>Now that we've sanitized the database, let's take another look at the snapshots available for the <code>example_app</code> target</p> <pre><code>$ dbsnapper target example_app\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nListing ALL snapshots for target: example_app\n+-------+-------------------------+------------------------+----------------------------+--------+------------+--------------------------------+\n| INDEX |         CREATED         |          NAME          |          FILENAME          |  SIZE  | SANITIZED? |             SANFN              |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+--------------------------------+\n|     0 | 2024-Feb-21 @ 16:32:11Z | 1708558331_example_app | 1708558331_example_app.zip | 1.4 kB | true       | 1708558331_example_app.san.zip |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+--------------------------------+\n</code></pre> <p>The big difference here is that <code>1708558331_example_app.san.zip</code> is listed in the <code>SANFN</code> column, which indicates that we have a sanitized database snapshot available for this snapshot index.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#load-the-sanitized-snapshot", "title": "Load the sanitized snapshot", "text": "<p>Like we did above, we'll once again, load the snapshot to our <code>example_app_snap</code> snapshot database:</p> <pre><code>$ dbsnapper load example_app 0\n</code></pre> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Loading Snapshot\n-&gt; LOADING sanitized Snapshot #0: Name: 1708558331_example_app, Snapshot File: 1708558331_example_app.san.zip, Dest DB URL: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n--&gt; Using engine: postgres-local\n--&gt; Using Target: example_app\n--&gt; Pulling to local file: /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Local snapshot already exists at /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Pulled snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Unzipping snapshot /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-3818520887\n--&gt; Dropping and recreating database pgl://localhost:15432/example_app_snap\n-&gt; LOADING Snapshot Completed for Target: example_app\nFINISH: Loading Snapshot\n</code></pre> <p>In line 5 above we're loading the SANITIZED snapshot at index 0. The <code>load</code> command automatically loads the sanitized snapshot for an index if one exists.</p> <p>Note</p> <p>If a sanitized snapshot exists, but you'd like to load the original snapshot, you can use the <code>--original</code> flag to force this behavior:</p> <p><code>dbsnapper load example_app 0 --original</code></p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-the-sanitized-database", "title": "Checking the sanitized database", "text": "<p>Our sanitized database is now loaded, let's check it to see if it worked:</p> <pre><code>postgres=# \\l\n                                                          List of databases\n             Name              |  Owner   | Encoding |  Collate   |   Ctype    | ICU Locale | Locale Provider |   Access privileges\n-------------------------------+----------+----------+------------+------------+------------+-----------------+-----------------------\n example_app                   | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n example_app_sanitized         | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n example_app_snap              | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n\n\npostgres=# \\c example_app_snap\npsql (15.6 (Postgres.app), server 15.2 (Debian 15.2-1.pgdg110+1))\nYou are now connected to database \"example_app_snap\" as user \"postgres\".\nexample_app_snap=#\nexample_app_snap=# \\dt\n             List of relations\n Schema |      Name      | Type  |  Owner\n--------+----------------+-------+----------\n public | dbsnapper_info | table | postgres\n public | users          | table | postgres\n(2 rows)\n\n example_app_snap=# select * from dbsnapper_info;\n         created_at         |         tags\n----------------------------+-----------------------\n 2024-02-21 23:47:17.454871 | {example_app.san.sql}\n(2 rows)\n</code></pre> <p>There are our tables, with the new <code>dbsnapper_info</code> table that holds the sanitization timestamp entry.</p> <p>And here is our user table with the sanitized <code>first_name</code>, <code>last_name</code>, <code>email</code>, <code>password</code> and <code>pin</code> fields.</p> <pre><code>example_app_snap=# select * from users;\n id | first_name | last_name |        email         |    password     | pin\n----+------------+-----------+----------------------+-----------------+------\n  1 | User       | Id1       | User_Id1@example.com | genericpassword | 0000\n  2 | User       | Id2       | User_Id2@example.com | genericpassword | 0000\n  3 | User       | Id3       | User_Id3@example.com | genericpassword | 0000\n  4 | User       | Id4       | User_Id4@example.com | genericpassword | 0000\n(4 rows)\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#conclusion", "title": "Conclusion", "text": "<p>In this example, we created a simple database and demonstrated how you can use DBSnapper to copy it with the <code>build</code> and <code>load</code> commands, sanitize it with the <code>sanitize</code> command, and then <code>load</code> the sanitized version to your development database.</p> <p>We hope you find this example useful. If you have any questions about dbsnapper, you can contact us via email at contact@dbsnapper.com.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/", "title": "Part 1: Using DBSnapper with GitHub Actions and Amazon ECS", "text": ""}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#overview", "title": "Overview", "text": "<p>Note: This is Part 1 of a multi-part series on using DBSnapper with GitHub Actions and Amazon ECS. Go here to read Part 2 - A Simplified Approach with Third-Party Runners which includes the results of the workflow execution.</p> <p>Update: New DBSnapper GitHub Action</p> <p>We've updated this article to use our new Install DBSnapper Agent GitHub Action that makes it simple to install the DBSnapper Agent onto a GitHub Actions runner. These changes can be found in Step 4.</p> <p>The motivation behind this article is to describe an automated way to snapshot and sanitize database workloads running on private infrastructure. In this article, we'll use GitHub Actions and self-hosted runners to trigger a snapshot and sanitization of an Amazon RDS database using DBSnapper and Amazon ECS Fargate.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#services-and-components-used", "title": "Services and components used", "text": "<ul> <li>DBSnapper Agent to take the snapshot, sanitize it, and store it.</li> <li>DBSnapper Cloud providing target configuration and snapshot storage.</li> <li>GitHub Actions (GHA) as CI/CD provider to automate the environment setup and trigger the snapshot.</li> <li>GitHub Actions self-hosted runners to run the DBSnapper Agent in our private infrastructure.</li> <li>Amazon ECS Fargate to run an ephemeral GHA self-hosted runner</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#other-requirements", "title": "Other requirements", "text": "<ul> <li>We use GitHub's OIDC Provider to provide AWS credentials needed for the actions. We used the instructions given in the aws-actions/configure-aws-credentials OIDC Section to setup the federation between GitHub and AWS, using the CloudFormation Template to speed things up.</li> <li>We use the GitHub official <code>actions-runner</code> container for our self-hosted runner.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#github-secrets", "title": "GitHub Secrets:", "text": "<p>We will need the following GitHub Secrets configured for the GitHub Action:</p> <ul> <li><code>FG_PAT</code> - Fine-Grained GitHub Personal Access Token (PAT) with <code>repo:actions:(read/write)</code> and <code>org:self-hosted-runners:(read/write)</code> scopes.</li> <li><code>DBSNAPPER_AUTHTOKEN</code> - Your DBSnapper Cloud Auth Token used to authenticate to the DBSnapper API.</li> <li><code>DBSNAPPER_SECRET_KEY</code> - Your DBSnapper config secret key used to encrypt certain configuration values.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#overview-of-the-steps", "title": "Overview of the steps", "text": "<ol> <li>Setup the GitHub Actions workflow basics.</li> <li>Get a Registration token to register a self-hosted runner.</li> <li>Launch an Amazon ECS Fargate Task to run the self-hosted runner.</li> <li>Execute the DBSnapper Agent snapshot and sanitization commands.</li> <li>Cleanup the Amazon ECS Fargate Tasks.</li> </ol>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-1-setup-the-github-actions-workflow", "title": "Step 1 - Setup the GitHub Actions Workflow", "text": "Step 1 - Setup the GitHub Actions Workflow<pre><code>name: \"DBSnapper Build Snapshot\"\non: [push]\n\n# Permissions are needed for github OIDC provider - AWS federation\n# https://github.com/aws-actions/configure-aws-credentials?tab=readme-ov-file#oidc\npermissions:\n  id-token: write\n  contents: read\n  actions: write\n\n# Some necessary environment variables\nenv:\n  ORGANIZATION: dbsnapper # Your GitHub Organization\n  ORGANIZATION_URL: https://github.com/dbsnapper # Your GitHub Organization URL\n  AWS_REGION: us-east-1\n  AWS_IAM_ROLE: arn:aws:iam::&lt;acct_id&gt;:role/&lt;name of role&gt; # From OIDC Federation\n  ECS_CLUSTER: dbsnapper\n  ECS_SERVICE: github_runner\n  ECS_TASK_DEFINITION: .aws/ecs_task_definition_github_runner.json # Initial task definition file in the repo for the ECS Task\n</code></pre> <p>Here we setup the basic elements of the GitHub Actions workflow. We define the name of the workflow, the event that triggers it, and some environment variables that we will use throughout the workflow.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#iam-policy-for-the-aws_iam_role", "title": "IAM Policy for the AWS_IAM_ROLE", "text": "<p>The <code>permissions</code> section is needed for the GitHub OIDC provider to provide AWS credentials to the actions and the <code>AWS_IAM_ROLE</code> is the ARN of the role that the OIDC provider will assume to provide the AWS credentials. This role should have the necessary permissions to interact with the services we are using, specifically ECS in our case:</p> Example IAM Policy for the AWS_IAM_ROLE<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"RegisterTaskDefinition\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecs:RegisterTaskDefinition\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"PassRolesInTaskDefinition\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"iam:PassRole\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"DeployService\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecs:UpdateService\", \"ecs:DescribeServices\"],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Above is an example of the IAM policy that the <code>AWS_IAM_ROLE</code> should have to interact with ECS. You should set specific Resource ARNs for the <code>ecs:UpdateService</code> and <code>ecs:DescribeServices</code> actions to limit the access to the ECS cluster and service you are using. I've wildcarded them here for simplicity.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-2-get-registration-token-to-register-a-runner", "title": "Step 2 - Get Registration Token to Register a Runner", "text": "Step 2 - Get Registration Token<pre><code>jobs:\n  register-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      registration_token: ${{ steps.create-token.outputs.registration_token }}\n    steps:\n      - name: Create Registration Token\n        id: create-token\n        # https://docs.github.com/en/rest/actions/self-hosted-runners?apiVersion=2022-11-28#about-self-hosted-runners-in-github-actions\n        # use jq -r to output the raw version of the .token field\n        run: |\n          token=$(curl -L \\\n          -X POST \\\n          -H \"Accept: application/vnd.github+json\" \\\n          -H \"Authorization: Bearer ${{ secrets.FG_PAT }}\" \\\n          -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n          \"https://api.github.com/orgs/${{ env.ORGANIZATION }}/actions/runners/registration-token\" | jq -r '.token')\n\n          echo \"registration_token=$token\" &gt;&gt; $GITHUB_OUTPUT\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs", "title": "Inputs", "text": "<ul> <li><code>secrets.FG_PAT</code>- The Fine-Grained GitHub Personal Access Token (PAT) used in the <code>Authentication</code> header for the API call</li> <li><code>env.ORGANIZATION</code> - The GitHub Organization that the runner will be registered with, used in the API URL.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#outputs", "title": "Outputs", "text": "<ul> <li><code>create_token.registration_token</code> - The registration token that will be used to register the self-hosted runner</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description", "title": "Description", "text": "<p>Step two gives us a Registration Token that we can pass to the self-hosted runner configuration script to register the runner with the GitHub Actions service. We use the GitHub REST API to get the registration token and output it to a file that we can access in the next step.</p> <p>We use the <code>jq</code> utility to parse the JSON response from the API and extract the <code>token</code> field from the API response. We use the <code>-r</code> flag to output the raw version of the <code>token</code> field, otherwise it would be quoted and cause issues later in the workflow.</p> <p>in line 19 we save the output of the <code>token</code> to the $GITHUB_OUTPUT variable which is the new way to set outputs in GitHub Actions. This is assigned to the <code>registration_token</code> output of the job in line 5 for use later in the workflow.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-3-launch-amazon-ecs-task", "title": "Step 3 - Launch Amazon ECS Task", "text": "Step 3 - Launch Amazon ECS Task<pre><code>runner-ecs:\n  runs-on: ubuntu-latest\n  needs: register-runner\n  steps:\n    - name: Checkout for access to task definition\n      uses: actions/checkout@v4\n\n    # Uses OIDC connector for auth\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-region: ${{ env.AWS_REGION }}\n        role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n    - name: Update Task Definition JSON\n      id: update-task-def\n      uses: restackio/update-json-file-action@main\n      with:\n        file: ${{ env.ECS_TASK_DEFINITION }}\n        fields: |\n          {\n            \"containerDefinitions[0].command\": [\n            \"sh\",\n            \"-c\",\n            \"./config.sh --unattended --ephemeral --labels x64,linux,ecs --name ECS_Ephemeral_Runner --url ${{ env.ORGANIZATION_URL }} --token ${{ needs.register-runner.outputs.registration_token }} &amp;&amp; ./run.sh\"\n            ]\n          }\n\n    - name: Deploy Amazon ECS task definition\n      uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n      with:\n        task-definition: ${{ env.ECS_TASK_DEFINITION }}\n        service: ${{ env.ECS_SERVICE }}\n        cluster: ${{ env.ECS_CLUSTER }}\n        wait-for-service-stability: false\n\n    - name: ECS Service Set desired tasks to 1\n      run: |\n        aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 1\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs_1", "title": "Inputs", "text": "<ul> <li><code>env.AWS_REGION</code> - The AWS region where the ECS cluster is located</li> <li><code>env.AWS_IAM_ROLE</code> - The ARN of the IAM role that the OIDC provider will assume to provide the AWS credentials.</li> <li><code>env.ECS_CLUSTER</code> - The name of the ECS cluster where the task will be run.</li> <li><code>env.ECS_SERVICE</code> - The name of the ECS service that will run the task.</li> <li><code>env.ECS_TASK_DEFINITION</code> - The path and filename of the task definition file in the repository.</li> <li><code>.aws/ecs_task_definition_github_runner.json</code> - The initial task definition file in the repository for the ECS Task.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#actions", "title": "Actions", "text": "<ul> <li><code>actions/checkout@v4</code> - This action checks out the repository so that we can access the ECS task definition file.</li> <li><code>aws-actions/configure-aws-credentials@v4</code> - This action configures the AWS credentials for the actions to use. It uses the OIDC connector to authenticate with AWS.</li> <li><code>restackio/update-json-file-action@main</code> - This action updates the ECS task definition JSON file with the container command that provides the <code>--url</code> of the GitHub organization and the <code>--token</code> registration token to register the self-hosted runner. The main branch includes an updated version of node used in the action. We use this instead of <code>amazon-ecs-render-task-definition</code> because it improperly parses the provided command string, causing issues with the runner registration per this issue comment.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description_1", "title": "Description", "text": ""}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#checkout-and-configure-aws-credentials", "title": "Checkout and Configure AWS Credentials", "text": "<p>In this step, we're defining the <code>runner-ecs</code> job (that depends on successful execution of the <code>register-runner</code> job) that will launch an Amazon ECS Fargate Task to run the self-hosted runner. The first two steps are straightforward. We use the <code>actions/checkout@v4</code> action on line 6 to checkout the repository so that we can access the ECS task definition file. We then use the <code>aws-actions/configure-aws-credentials@v4</code> action on line 10 to configure the AWS credentials for the actions to use. This action uses the OIDC connector to authenticate with AWS.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#update-ecs-task-definition-json", "title": "Update ECS Task Definition JSON", "text": "<p>The next step starting on line 15, we update the ECS task definition file in our repository to reflect the command we'll run to configure the self-hosted runner. In this command we use the <code>config.sh</code> script to configure the runner with the <code>--url</code> and <code>--token</code> arguments, which are needed to register the runner in the GitHub Actions service. We can find our GHA runners in the organization actions settings URL: <code>https://github.com/organizations/&lt;your_organization&gt;/settings/actions/runners</code></p> <p>GHA Runner Docs</p> <p>It wasn't immediately obvious how to run the GHA runner container, so there was a bit of trial and error locally to get the commands right. Documentation can be found in the repository, and the Automate Configuring Self-Hosted Runners is especially useful. Two  scripts in the runner container are used to configure and run the runner:</p> <ul> <li><code>config.sh</code> - This script configures the runner with the provided <code>--url</code> and <code>--token</code> arguments. The <code>--ephemeral</code> flag is used to run the runner in ephemeral mode, meaning it will be removed when the task is stopped, and the <code>--unattended</code> flag is used to run the runner in unattended mode, meaning it won't prompt for user input.</li> <li><code>run.sh</code> - This script starts the runner, registering it with GitHub Actions, starting the runner service and waiting for jobs to run. This script is run after the runner is configured with the <code>config.sh</code> script.</li> </ul> <p>Both scripts will display help and available command line options by passing <code>--help</code> as an argument. The <code>config.sh</code> script is the only one that requires arguments, and the <code>run.sh</code> script will run the runner with the configuration provided by the <code>config.sh</code> script.</p> <p>The <code>--ephemeral</code> flag is used to run the runner in ephemeral mode, meaning the runner will be removed from the organization after it has completed a single job. This ensures we don't have any lingering runners in the organization.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#deploy-amazon-ecs-task-definition", "title": "Deploy Amazon ECS Task Definition", "text": "<p>The next step on line 29 uses the <code>aws-actions/amazon-ecs-deploy-task-definition@v1</code> action to deploy the updated task definition to the ECS cluster. This action takes the path to the task definition file, the cluster, and service name as inputs. We set the <code>wait-for-service-stability</code> input to <code>false</code> to avoid waiting for the service to stabilize before continuing with the workflow, since Step 4 - Execute the DBSnapper Agent Commands won't proceed until the runner is registered, running, and able to execute the job.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#ecs-service-set-desired-tasks-to-1", "title": "ECS Service: Set Desired Tasks to 1", "text": "<p>Finally, on line 39, we use the <code>aws ecs update-service</code> command (from the aws-cli) to set the desired task count for the service to 1. This will start a task in the service to run the self-hosted runner. The updated task definition will not start without this command. We use a similar command in Step 5 - Cleanup the Amazon ECS Task to set the desired task count to 0 to stop the runner after the job is complete.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#task-definition-file", "title": "Task Definition File", "text": "<p>To launch an ECS task via github actions and the <code>aws-actions/amazon-ecs-deploy-task-definition@v1</code> action, we need to provide a task definition file. This file is a JSON file that defines the properties of the task that we want to run. We can create this file using the ECS task definition wizard in the AWS console and then copy the task definition JSON to a file in our repository, or we can just use the sample proided below modified for your environment. In our workflow example, we're using the <code>ECS_TASK_DEFINITION</code> environment variable to specify the path to the task definition file.</p> ecs_task_definition_github_runner.json<pre><code>{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"github_runner\",\n      \"image\": \"ghcr.io/actions/actions-runner:2.317.0\",\n      \"cpu\": 0,\n      \"portMappings\": [],\n      \"essential\": true,\n      \"command\": [],\n      \"mountPoints\": [],\n      \"volumesFrom\": [],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/github_runner\",\n          \"awslogs-create-group\": \"true\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      },\n      \"systemControls\": []\n    }\n  ],\n  \"family\": \"github_runner\",\n  \"taskRoleArn\": \"arn:aws:iam::&lt;account_id&gt;:role/&lt;task_execution_role&gt;\",\n  \"executionRoleArn\": \"arn:aws:iam::&lt;account_id&gt;:role/&lt;task_execution_role&gt;\",\n  \"networkMode\": \"awsvpc\",\n  \"volumes\": [],\n  \"placementConstraints\": [],\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"512\",\n  \"memory\": \"2048\",\n  \"runtimePlatform\": {\n    \"cpuArchitecture\": \"X86_64\",\n    \"operatingSystemFamily\": \"LINUX\"\n  },\n  \"tags\": []\n}\n</code></pre> <p>In this task definition, we set the default GHA runner image on line 5. If needed, we can update this image in the workflow using the <code>restackio/update-json-file-action@main</code> action. We also set the <code>cpu</code> and <code>memory</code> values for this task on lines 31 and 32 to the minimum values needed to run the runner and any additional services.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-4-execute-the-dbsnapper-agent-commands", "title": "Step 4 - Execute the DBSnapper Agent Commands", "text": "Step 4 - Execute the DBSnapper Agent Commands<pre><code>dbsnapper:\n  runs-on: self-hosted\n  needs: runner-ecs\n  env:\n    DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n    DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n  steps:\n    # Updated to use the new DBSnapper GitHub Action\n    - name: Install DBSnapper Agent\n      uses: dbsnapper/install-dbsnapper-agent-action@v1\n      with:\n        version: latest\n    - name: Install Database Utilities\n      # Get the latest version of the postgres client\n      # https://www.postgresql.org/download/linux/ubuntu/\n      run: |\n        sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-common &amp;&amp; sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-client-16\n\n    - name: Run DBSnapper build command\n      run: dbsnapper build dvdrental-prod\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs_2", "title": "Inputs", "text": "<ul> <li><code>secrets.DBSNAPPER_SECRET_KEY</code> - Your DBSnapper config secret key used to encrypt certain configuration values.</li> <li><code>secrets.DBSNAPPER_AUTHTOKEN</code> - Your DBSnapper Cloud Auth Token used to authenticate to the DBSnapper API.</li> <li><code>DBSnapper Debian Package</code> - The latest release of the DBSnapper Agent. Download the <code>.deb</code> package that matches your architecture. We use the <code>dbsnapper_linux_x86_64.deb</code> package in this example.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description_2", "title": "Description", "text": "<p>The steps prior to this one were necessary to setup a self-hosted runner in our private infrastructure with access to the workloads in our network. In this step, we're finally able to run the DBSnapper Agent commands to snapshot and sanitize the database.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#runs-on-self-hosted", "title": "Runs-on: Self-Hosted", "text": "<p>On line 2, we specify the <code>runs-on</code> property as <code>self-hosted</code> to run the job on the self-hosted runner we registered in the previous step. This property can take additional labels to be more specific about the runner that should run the job, in cases where you have many different types of runners registered in your organization. In line 3 we specify that this job depends on the <code>runner-ecs</code> job to ensure the ecs jobs are run before running the dbsnapper job.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#environment-variables", "title": "Environment Variables", "text": "<p>We set our environment variables starting on line 4. In this example, we set the minimum required <code>DBSNAPPER_SECRET_KEY</code> and <code>DBSNAPPER_AUTHTOKEN</code> environment variables necessary to run DBSnapper without a configuration file.</p> <p>DBSnapper Configuration via Environment Variables</p> <p>DBSnapper can be configured exclusively through environment variables if you don't want to rely on a configuration file. All the configuration options can be represented as environment variables through a specific naming convention involving prefixing the environment variable with <code>DBSNAPPER</code> and replacing periods with two underscores <code>__</code>. Some examples from the DBSnapper Configuration Documentation include:</p> <ul> <li><code>docker.images.postgres</code> -&gt; <code>DBSNAPPER_DOCKER__IMAGES__POSTGRES: postgres:latest</code> # Sets the docker image to use for the postgres containers</li> <li><code>defaults.shared_target_dst_db_url</code> -&gt; <code>DBSNAPPER_DEFAULTS__SHARED_TARGET_DST_DB_URL: &lt;connstring&gt;</code> # Sets the default destination database URL for shared targets</li> <li><code>override.san_query</code> -&gt; <code>DBSNAPPER_OVERRIDE__SAN_QUERY: &lt;base-64-encoded-value&gt;</code> # Sets a query to use for sanitization overriding any existing queries.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#install-and-run-dbsnapper", "title": "Install and Run DBSnapper", "text": "<p>Starting on line 8 we use the new DBSnapper GitHub Action to install the latest version of the DBSnapper Agent. This action takes into account the operating system and architecture of the runner and installs the appropriate version of the DBSnapper Agent.</p> <p>Starting on line 8, we run commands to update our apt repository, install <code>curl</code>, and use <code>curl</code> to download the latest release of the DBSnapper Agent. We then use <code>dpkg</code> to install the <code>.deb</code> package.</p> <p>Why Not use the DBSnapper Docker Image?</p> <p>At this point, it would have been convenient to use the DBSnapper Docker image to run the DBSnapper commands. Unfortunately GitHub Actions... runner does not have Docker installed by default, so we would need to install Docker in the runner before we could use the Docker image. To avoid the additional complexity we decided to download and install the <code>.deb</code> package instead.</p> <p>...doesn't support using docker from a self-hosted runner at this time. See the following issues for more information:</p> <ul> <li>https://github.com/actions/runner/issues/406\u2060</li> <li>https://github.com/actions/runner/issues/367\u2060    </li> </ul> <p>Database Utilities Needed</p> <p>When using the DBSnapper container image, the Agent and database utilities are already included in the image. Since GitHub Actions doesn't support Docker containers, we need to install the tools by hand. In this case, we install the PostgreSQL client on line 17 to support the snapshot of our Postgresql RDS database. If you are using a different database, you will need to install the appropriate client.</p> <p>On line 13, we run the <code>dbsnapper build dvdrental-prod</code> command which will use the <code>DBSNAPPER_AUTHTOKEN</code> to authenticate to the DBSnapper Cloud, create a snapshot of the database specified in the <code>dvdrental-prod</code> target, and store it in the cloud storage specified in the target configuration. Once this is complete and no additional steps are needed, the task will be cleaned up in the next step.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-5-cleanup-the-amazon-ecs-task", "title": "Step 5 - Cleanup the Amazon ECS Task", "text": "Step 5 - Cleanup the Amazon ECS Task<pre><code>deprovision:\n  runs-on: ubuntu-latest\n  if: ${{ always() }}\n  needs: dbsnapper\n  steps:\n    # Uses OIDC connector for auth\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-region: ${{ env.AWS_REGION }}\n        role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n    - name: ECS Add desired tasks to 0\n      run: |\n        aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 0\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs_3", "title": "Inputs", "text": "<ul> <li><code>env.AWS_REGION</code> - The AWS region where the ECS cluster is located</li> <li><code>env.AWS_IAM_ROLE</code> - The ARN of the IAM role that the OIDC provider will assume to provide the AWS credentials.</li> <li><code>env.ECS_CLUSTER</code> - The name of the ECS cluster where the task will be run.</li> <li><code>env.ECS_SERVICE</code> - The name of the ECS service that will run the task.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#actions_1", "title": "Actions", "text": "<ul> <li><code>aws-actions/configure-aws-credentials@v4</code> - This action configures the AWS credentials for the actions to use. It uses the OIDC connector to authenticate with AWS.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description_3", "title": "Description", "text": "<p>In this final step, we need to use the <code>aws-actions/configure-aws-credentials@v4</code> action to configure the AWS credentials so we can use the AWS CLI to run the <code>aws ecs update-service</code> command. We use this command to set the desired task count for the service to 0, which will stop all ECS runner tasks, ensuring we don't get billed for unnecessary resources. Because we configured the runners with the <code>--ephemeral</code> flag, they will be automatically removed from the GitHub organization when they are terminated.</p> <p>Note: By using the <code>if: ${{ always() }}</code> condition on line 3, we ensure this step runs even if the previous steps fail. This is important to ensure that the ECS service is cleaned up even other steps fail, which under normal circumstances would not be the case.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#conclusion", "title": "Conclusion", "text": "<p>In this article, we've shown how to use GitHub Actions to trigger a snapshot and sanitization of an Amazon RDS database using DBSnapper and Amazon ECS Fargate. We've covered the setup of the GitHub Actions workflow, the registration of a self-hosted runner, the launch of an Amazon ECS Fargate task to run the runner, the execution of the DBSnapper Agent commands, and the cleanup of the Amazon ECS task. This workflow can be used to automate the snapshot and sanitization of database workloads running on private infrastructure, providing an automated and reliable manage database backups.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#full-workflow", "title": "Full Workflow", "text": "<p>Here is the full GitHub Actions workflow that we've described in this article. Be sure to replace any placeholders with your own values.</p> DBSnapper Build Snapshot - Full Workflow<pre><code>name: \"DBSnapper Build Snapshot\"\non: [push]\n\n# Permissions are needed for github OIDC provider - AWS federation\n# https://github.com/aws-actions/configure-aws-credentials?tab=readme-ov-file#oidc\npermissions:\n  id-token: write\n  contents: read\n  actions: write\n\n# Some necessary environment variables\nenv:\n  ORGANIZATION: dbsnapper # Your GitHub Organization\n  ORGANIZATION_URL: https://github.com/dbsnapper # Your GitHub Organization URL\n  AWS_REGION: us-east-1\n  AWS_IAM_ROLE: arn:aws:iam::&lt;acct_id&gt;:role/&lt;name of role&gt; # From OIDC Federation\n  ECS_CLUSTER: dbsnapper\n  ECS_SERVICE: github_runner\n  ECS_TASK_DEFINITION: .aws/ecs_task_definition_github_runner.json # Initial task definition file in the repo for the ECS Task\n\njobs:\n  register-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      registration_token: ${{ steps.create-token.outputs.registration_token }}\n    steps:\n      - name: Create Registration Token\n        id: create-token\n        # https://docs.github.com/en/rest/actions/self-hosted-runners?apiVersion=2022-11-28#about-self-hosted-runners-in-github-actions\n        # use jq -r to output the raw version of the .token field\n        run: |\n          token=$(curl -L \\\n          -X POST \\\n          -H \"Accept: application/vnd.github+json\" \\\n          -H \"Authorization: Bearer ${{ secrets.FG_PAT }}\" \\\n          -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n          \"https://api.github.com/orgs/${{ env.ORGANIZATION }}/actions/runners/registration-token\" | jq -r '.token')\n\n          echo \"registration_token=$token\" &gt;&gt; $GITHUB_OUTPUT\n\n  runner-ecs:\n    runs-on: ubuntu-latest\n    needs: register-runner\n    steps:\n        - name: Checkout for access to task definition\n          uses: actions/checkout@v4\n\n        # Uses OIDC connector for auth\n        - name: Configure AWS credentials\n          uses: aws-actions/configure-aws-credentials@v4\n          with:\n            aws-region: ${{ env.AWS_REGION }}\n            role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n        - name: Update Task Definition JSON\n          id: update-task-def\n          uses: restackio/update-json-file-action@main\n          with:\n            file: ${{ env.ECS_TASK_DEFINITION }}\n            fields: |\n            {\n                \"containerDefinitions[0].command\": [\n                \"sh\",\n                \"-c\",\n                \"./config.sh --unattended --ephemeral --labels x64,linux,ecs --name ECS_Ephemeral_Runner --url ${{ env.ORGANIZATION_URL }} --token ${{ needs.register-runner.outputs.registration_token }} &amp;&amp; ./run.sh\"\n                ]\n            }\n\n        - name: Deploy Amazon ECS task definition\n          uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n          with:\n            task-definition: ${{ env.ECS_TASK_DEFINITION }}\n            service: ${{ env.ECS_SERVICE }}\n            cluster: ${{ env.ECS_CLUSTER }}\n            wait-for-service-stability: false\n\n        - name: ECS Service Set desired tasks to 1\n          run: |\n            aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 1\n\n  dbsnapper:\n    runs-on: self-hosted\n    needs: runner-ecs\n    env:\n      DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n      DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n    steps:\n      # Updated to use the new DBSnapper GitHub Action\n      - name: Install DBSnapper Agent\n        uses: dbsnapper/install-dbsnapper-agent-action@v1\n        with:\n          version: latest\n      - name: Install Database Utilities\n        # Get the latest version of the postgres client\n        # https://www.postgresql.org/download/linux/ubuntu/\n        run: |\n          sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-common &amp;&amp; sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-client-16\n\n      - name: Run DBSnapper build command\n        run: dbsnapper build dvdrental-prod\n\n  deprovision:\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    needs: dbsnapper\n    steps:\n      # Uses OIDC connector for auth\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n      - name: ECS Add desired tasks to 0\n        run: |\n          aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 0\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/", "title": "Part 2: Using DBSnapper, GitHub Actions, and ECS - A Simplified Approach", "text": ""}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#overview", "title": "Overview", "text": "<p>Note: This is Part 2 of a multi-part series on using DBSnapper with GitHub Actions and Amazon ECS. Go here to read Part 1.</p> <p>Third Party GitHub Runners</p> <p>This article provides an alternative approach to using DBSnapper with GitHub Actions and Amazon ECS. This article uses a GitHub runner built by a third party. There are actually several nice third-party runners that build upon the official GitHub Actions runner, and provide additional capabilities that the official runner doesn't yet support. You can find a list of these runners on the Awesome-runners list. For this article we are using the myoung34 GitHub Runner.</p> <p>The previous version of this article used the official GitHub Actions runner to run the DBSnapper Agent in an Amazon ECS Fargate Task. We've found that there are third-party runners that provide additional capabilities that the official runner doesn't yet support. We will present the simplified approach using the myoung34 GitHub Runner in this article. We will be brief in our explanation of the steps, as they are similar to the previous article.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#using-the-myoung34-github-runner", "title": "Using the myoung34 GitHub Runner", "text": "<p>The myoung34 GitHub Runner is a third-party runner that builds upon the official GitHub Actions runner. This runner makes use of environment variables to configure the runner and and most importantly supports an <code>ACCESS_TOKEN</code> environment variable that will take care of registering the runner with the GitHub Actions service. This eliminates the need for the first step in the previous article where we had to get a registration token to register the runner with GitHub.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#simplified-github-actions-workflow", "title": "Simplified GitHub Actions Workflow", "text": "<p>In this workflow we only have three jobs instead of the four in the previous article. We describe some of the important changes below:</p> Simplified GitHub Actions Workflow<pre><code>name: \"GHA-my-ECS-DBSnapper Build Snapshot\"\non: [push]\n\n# For github OIDC provider\n# https://github.com/aws-actions/configure-aws-credentials?tab=readme-ov-file#oidc\npermissions:\n  id-token: write\n  contents: read\nenv:\n  ORGANIZATION: &lt;Your_GitHub_Organization_Name&gt;\n  ORGANIZATION_URL: &lt;Your_GitHub Organization URL&gt;\n  GHR_IMAGE: myoung34/github-runner:latest\n  AWS_REGION: us-east-1\n  AWS_IAM_ROLE: arn:aws:iam::&lt;account_id&gt;:&lt;role_name&gt;\n  ECS_CLUSTER: &lt;ECS_Cluster_Here&gt;\n  ECS_SERVICE: &lt;ECS_Service_Here&gt;\n  ECS_TASK_DEFINITION: .aws/ecs_task_definition_my_github_runner.json\n  DBSNAPPER_TARGET: dbs-prod-rds\n\njobs:\n  runner-ecs:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout for access to task definition\n        uses: actions/checkout@v4\n\n      # Uses OIDC connector for auth\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n      - name: Update Task Definition JSON\n        id: update-task-def\n        uses: restackio/update-json-file-action@main\n        with:\n          file: ${{ env.ECS_TASK_DEFINITION }}\n          fields: |\n            {\n              \"containerDefinitions[0].image\": \"${{ env.GHR_IMAGE }}\",\n              \"containerDefinitions[0].environment\": [\n                {\n                  \"name\": \"ACCESS_TOKEN\",\n                  \"value\": \"${{ secrets.FG_PAT }}\"\n                },\n                {\n                  \"name\": \"RUNNER_SCOPE\",\n                  \"value\": \"org\"\n                },\n                {\n                  \"name\": \"ORG_NAME\",\n                  \"value\": \"${{ env.ORGANIZATION }}\"\n                },\n                {\n                  \"name\": \"EPHEMERAL\",\n                  \"value\": \"true\"\n                },\n                {\n                  \"name\": \"LABELS\",\n                  \"value\": \"ecs\"\n                },\n                {\n                  \"name\": \"RUNNER_NAME_PREFIX\",\n                  \"value\": \"ecs\"\n                }\n              ]\n            }\n\n      - name: Deploy Amazon ECS task definition\n        uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n        with:\n          task-definition: ${{ env.ECS_TASK_DEFINITION }}\n          service: ${{ env.ECS_SERVICE }}\n          cluster: ${{ env.ECS_CLUSTER }}\n          wait-for-service-stability: false\n\n      - name: ECS Add desired tasks to 1\n        run: |\n          aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 1\n\n  dbsnapper:\n    runs-on: self-hosted\n    needs: runner-ecs\n    env:\n      DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n      DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n      # Using target templates so we provide the required env vars:\n      DATABASE_USERNAME: ${{ secrets.DATABASE_USERNAME }}\n      DATABASE_PASSWORD: ${{ secrets.DATABASE_PASSWORD }}\n      DATABASE_HOST: ${{ secrets.DATABASE_HOST }}\n\n    steps:\n      - name: Install DBSnapper Agent\n        uses: dbsnapper/install-dbsnapper-agent-action@v1\n        with:\n          version: latest\n\n      - name: Install Database Utilities\n        # Get the latest version of the postgres client\n        # https://www.postgresql.org/download/linux/ubuntu/\n        run: |\n          sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-common &amp;&amp; sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-client-16\n\n      - name: Run DBSnapper List Targets\n        run: dbsnapper targets\n\n      - name: Run Build Snapshot\n        run: dbsnapper build ${{ env.DBSNAPPER_TARGET }}\n\n  deprovision:\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    needs: dbsnapper\n    steps:\n      # Uses OIDC connector for auth\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n      - name: ECS Add desired tasks to 0\n        run: |\n          aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 0\n</code></pre> <p>The first job in this workflow involves setting up and starting the ECS task definition that will launch the myoung34 GitHub Runner. The biggest difference here is in the Update Task Definition JSON step starting on line 33. Instead of creating a command to run the runner as we did in the previous article, we instead update the task definition JSON file to include the necessary environment variables for the runner on lines 42-67. The environment variables we set are:</p> <ul> <li>ACCESS_TOKEN: We provide our Fine-Grained Personal Access Token (FG_PAT) as a secret to the runner. This token is used to authenticate the runner with GitHub Actions, saving us the step of registering the runner with GitHub which was necessary in the previous article.</li> <li>RUNNER_SCOPE: We set the runner scope to <code>org</code> to allow the runner to access all repositories in the organization.</li> <li>ORG_NAME: We provide the organization name to the runner.</li> <li>EPHEMERAL: We set the runner to ephemeral mode, so it will be deregistered from the GitHub Actions service when the task is stopped.</li> <li>LABELS: We provide a label to the runner to identify it as an ECS runner - customize this as you see fit.</li> <li>RUNNER_NAME_PREFIX: We provide a prefix for the runner name to identify it in the GitHub Actions service.</li> </ul> <p>There are several other environment variables you can use to configure the runner. You can find more information on the myoung34 GitHub Runner page.</p> <p>Database Utilities Needed</p> <p>When using the DBSnapper container image, the Agent and database utilities are already included in the image. Since GitHub Actions doesn't support Docker containers, we need to install the tools by hand. In this case, we install the PostgreSQL client on line 103 to support the snapshot of our Postgresql RDS database. If you are using a different database, you will need to install the appropriate client.</p> <p>Since our target defintion is using connection string templates for added security, we've set the <code>DATABASE_USERNAME</code>, <code>DATABASE_PASSWORD</code>, and <code>DATABASE_HOST</code> environment variables to the appropriate secrets on lines 89-91.</p> <p>The rest of the workflow is similar to the previous article, with the <code>dbsnapper</code> job installing and running the DBSnapper Agent to build a snapshot and the <code>deprovision</code> job stopping the ECS task.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#task-definition", "title": "Task Definition", "text": "<p>The task definition JSON file is similar to the one in the previous article, we're including it here for completeness.</p> .aws/ecs_task_definition_my_github_runner.json<pre><code>{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"my_github_runner\",\n      \"image\": \"myoung34/github-runner:latest\",\n      \"cpu\": 0,\n      \"portMappings\": [],\n      \"essential\": true,\n      \"environment\": [],\n      \"mountPoints\": [],\n      \"volumesFrom\": [],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/github_runner\",\n          \"awslogs-create-group\": \"true\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs-my\"\n        }\n      },\n      \"systemControls\": []\n    }\n  ],\n  \"family\": \"my_github_runner\",\n  \"taskRoleArn\": \"&lt;task_role_arn&gt;\",\n  \"executionRoleArn\": \"&lt;execution_role_arn&gt;\",\n  \"networkMode\": \"awsvpc\",\n  \"volumes\": [],\n  \"placementConstraints\": [],\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"512\",\n  \"memory\": \"2048\",\n  \"runtimePlatform\": {\n    \"cpuArchitecture\": \"X86_64\",\n    \"operatingSystemFamily\": \"LINUX\"\n  },\n  \"tags\": []\n}\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#target-settings", "title": "Target Settings", "text": "<p>The target we've configured in the DBSnapper Cloud is called <code>dbs-prod-rds</code>. This target uses a Source URL connection string template allowing us to store sensitive connection information in GitHub Secrets.</p> <p> Target settings for the dbs-prod-rds target. </p> <p>This target definition also includes a storage profile that uploads our snapshot to an S3 bucket in our AWS account. Since we aren't doing anything with Sanitization yet, we just leve the Sanitization Detais empty.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#workflow-execution-output", "title": "Workflow Execution Output", "text": "<p>When the workflow runs successfully, you should see output similar to the following:</p> GitHub Actions Workflow Output<pre><code> Listing all targets, update = false\n +-----------------------------+-------+---------------------------------------------------------------+-------+-----------------------------------------------------------+-----------------+\n |            NAME             | TYPE  |                              SRC                              | SIZE  |                            DST                            | STORAGE PROFILE |\n +-----------------------------+-------+---------------------------------------------------------------+-------+-----------------------------------------------------------+-----------------+\n | dbs-prod-rds                | cloud | pgl://dbsna...amazonaws.com:5432/app_dbsnapper_com_production | 0 B   |                                                           |                 |\n | dvdrental-san               | cloud | pgl://host....ker.internal:15432/dvdrental                    | 0 B   |                                                           |                 |\n | myd-ghost                   | cloud | myd://10.4.20.22:3306/ghost                                   | 0 B   | myd://mysql8:3306/ghost_prod_snap                         |                 |\n | snappy-rag-vecdb            | cloud | pgl://raghost:5432/snappy_rag                                 | 0 B   | pgl://raghost:5432/snappy_rag_snapshot                    |                 |\n |                             | ----- |                                                               |       |                                                           |                 |\n |                             | ----- |                                                               |       |                                                           |                 |\n |                             | ----- |                                                               |       |                                                           |                 |\n +-----------------------------+-------+---------------------------------------------------------------+-------+-----------------------------------------------------------+-----------------+\n ##[group]Run dbsnapper build dbs-prod-rds\n\n-- snip --\n\n env:\n   ORGANIZATION: dbsnapper\n   ORGANIZATION_URL: https://github.com/dbsnapper\n   GHR_IMAGE: myoung34/github-runner:latest\n\n-- snip --\n\n   DATABASE_USERNAME: ***\n   DATABASE_PASSWORD: ***\n   DATABASE_HOST: ***\n ##[endgroup]\n DBSnapper Agent - Version: 2.7.1 (aab3b1292b3e) Build Date: 2024-06-08T18:52:54Z\n DBSnapper Cloud: Enabled\n\n START: Build Snapshot for target: dbs-prod-rds with engine: postgres-local\n --&gt; Zipping snapshot to /root/.dbsnapper/1717956554_dbs-prod-rds.zip\n --&gt; Uploading snapshot to Cloud Storage via Presigned URL\n --&gt; Upload complete.\n --&gt; Local snapshot /root/.dbsnapper/1717956554_dbs-prod-rds.zip stored in cloud with ID: af36c454-eab4-4623-8485-4a51732634e7\n FINISH: Building DB Snapshot for target: dbs-prod-rds\n</code></pre> <p>As we can see above, the workflow first listed our targets as we requested on line 105 of the workflow. Here' we see the <code>dbs-prod-rds</code> target of interest. On lines 28-36 we see the output of the <code>dbsnapper build</code> command indicating that the snapshot was successfully built and uploaded to the cloud storage location specified in the target definition (line 35).</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#cloud-storage-verification", "title": "Cloud Storage Verification", "text": "<p>We can verify that the snapshot was uploaded to our S3 bucket by checking the bucket in the AWS Console. Here we see the snapshot file <code>af36c454-eab4-4623-8485-4a51732634e7.zip</code> in the bucket, which corresponds to the cloud ID of the snapshot we saw in the workflow output on line 35.</p> <p> Listing of objects in our snapshot bucket on S3. </p> <p>Now that we have a working, automated way to snapshot our database, we can include this workflow in our CI/CD pipeline to ensure that we always have the latest snapshot available as a point-in-time backup. We can use DBSnapper to create sanitized versions of these snapshots that we can share with our DevOps and development team for development and testing purposes.</p>"}, {"location": "articles/useful-postgres-mysql-queries/", "title": "Useful Postgres and MySQL Queries", "text": "<p>Here are some of the queries we use in DBSnapper to get database schema and other information.</p>"}, {"location": "articles/useful-postgres-mysql-queries/#connect-and-list-databases-tables", "title": "Connect and List Databases &amp; Tables", "text": "<p>Connect and list databases and tables.</p> PostgresMySQL <pre><code>psql -d postgres://postgres:postgres@localhost:15432\n\\l # list databases\n\\c database_name # connect to database\n\\d # list tables\n\\d+ table_name # list columns\n</code></pre> <pre><code>mysql -u root -P13306 -hlocalhost --protocol tcp -pmysql\nshow databases;\nuse database_name;\nshow tables;\ndescribe table_name;\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#list-tables", "title": "List Tables", "text": "<p>Tables</p> PostgresMySQL Query<pre><code>SELECT concat(concat(nsp.nspname, '.'), cls.relname) as table_name\nFROM pg_class cls\n  JOIN pg_namespace nsp ON nsp.oid = cls.relnamespace\nWHERE nsp.nspname NOT IN ('information_schema', 'pg_catalog')\n  AND cls.relkind = 'r';\n</code></pre> Output<pre><code>      table_name\n-----------------------\npublic.store\npublic.payment\npublic.film_category\npublic.actor\npublic.staff\npublic.dbsnapper_info\npublic.category\npublic.country\npublic.language\npublic.customer\npublic.users\npublic.rental\npublic.inventory\npublic.film\npublic.address\npublic.film_actor\npublic.city\n(17 rows)\n</code></pre> Query<pre><code>SELECT t.table_schema as schema_name,\n  t.table_name as table_name,\n  t.table_rows AS row_count,\n  ROUND(data_length) AS data_size,\n  ROUND(index_length) AS index_size,\n  ROUND(data_length + index_length) AS total_size\nFROM information_schema.TABLES t\nWHERE t.table_schema = 'sakila' -- replace sakila with your database name\n  AND t.table_type = 'BASE TABLE';\n</code></pre> Output<pre><code>+-------------+-----------------+-----------+-----------+------------+------------+\n| schema_name | table_name      | row_count | data_size | index_size | total_size |\n+-------------+-----------------+-----------+-----------+------------+------------+\n| sakila      | actor           |       200 |     16384 |      16384 |      32768 |\n| sakila      | address         |       603 |     98304 |      16384 |     114688 |\n| sakila      | category        |        16 |     16384 |          0 |      16384 |\n| sakila      | city            |       600 |     49152 |      16384 |      65536 |\n| sakila      | country         |       109 |     16384 |          0 |      16384 |\n| sakila      | customer        |       599 |     81920 |      49152 |     131072 |\n| sakila      | dbs_rental_copy |     15831 |   1589248 |          0 |    1589248 |\n| sakila      | film            |      1000 |    196608 |      81920 |     278528 |\n| sakila      | film_actor      |      5462 |    196608 |      81920 |     278528 |\n| sakila      | film_category   |      1000 |     65536 |      16384 |      81920 |\n| sakila      | film_text       |      1000 |    180224 |      16384 |     196608 |\n| sakila      | inventory       |      4581 |    180224 |     196608 |     376832 |\n| sakila      | language        |         6 |     16384 |          0 |      16384 |\n| sakila      | payment         |     16086 |   1589248 |     638976 |    2228224 |\n| sakila      | rental          |     16419 |   1589248 |    1196032 |    2785280 |\n| sakila      | staff           |         2 |     65536 |      32768 |      98304 |\n| sakila      | store           |         2 |     16384 |      32768 |      49152 |\n+-------------+-----------------+-----------+-----------+------------+------------+\n17 rows in set (0.01 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#list-table-columns", "title": "List Table Columns", "text": "<p>Table Columns</p> PostgresMySQL Query<pre><code>SELECT c.table_schema,\n  c.table_name,\n  c.column_name,\n  typ.typname AS type_name,\n  CASE\n    WHEN c.data_type = 'USER-DEFINED' THEN c.udt_name\n    ELSE c.data_type\n  END AS data_type,\n  CASE\n    WHEN c.data_type IN ('character', 'character varying') THEN c.character_maximum_length\n    ELSE NULL\n  END AS data_type_char_max,\n  -- Added this line\n  tc.constraint_name AS constraint_name,\n  (\n    SELECT COUNT(*) &gt; 0\n    FROM information_schema.key_column_usage cu\n      LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\n    WHERE cu.column_name = c.column_name\n      AND cu.table_name = c.table_name\n      AND tc.constraint_type = 'PRIMARY KEY'\n  ) AS is_primary,\n  (\n    SELECT COUNT(*) &gt; 0\n    FROM information_schema.key_column_usage cu\n      LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\n    WHERE cu.column_name = c.column_name\n      AND cu.table_name = c.table_name\n      AND tc.constraint_type = 'FOREIGN KEY'\n  ) AS is_foreign,\n  (\n    SELECT COUNT(*) &gt; 0\n    FROM information_schema.key_column_usage cu\n      LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\n    WHERE cu.column_name = c.column_name\n      AND cu.table_name = c.table_name\n      AND tc.constraint_type = 'UNIQUE'\n  ) AS is_unique,\n  BOOL_OR(c.is_nullable = 'YES') AS is_nullable,\n  att.attgenerated AS att_generated,\n  att.attidentity AS att_identity,\n  COALESCE(\n    STRING_AGG(\n      enumlabel,\n      ','\n      ORDER BY enumsortorder\n    ),\n    ''\n  ) AS enum_values,\n  COALESCE(pd.description, '') AS comment\nFROM information_schema.columns c\n  LEFT JOIN pg_type typ ON c.udt_name = typ.typname\n  LEFT JOIN pg_enum enu ON typ.oid = enu.enumtypid\n  LEFT JOIN pg_class cls ON c.table_name = cls.relname\n  LEFT JOIN pg_namespace ns ON cls.relnamespace = ns.oid\n  LEFT JOIN pg_description pd ON cls.oid = pd.objoid\n  LEFT JOIN pg_attribute att ON cls.oid = att.attrelid\n  LEFT JOIN information_schema.key_column_usage cu ON cu.column_name = c.column_name\n  AND cu.table_name = c.table_name\n  LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\nWHERE c.table_name = 'customer' -- REPLACE WITH YOUR TABLE NAME\n  AND c.table_schema = 'public' -- REPLACE WITH YOUR SCHEMA\nGROUP BY c.column_name,\n  c.table_schema,\n  c.table_name,\n  typ.typname,\n  c.data_type,\n  c.udt_name,\n  c.ordinal_position,\n  pd.description,\n  att.attgenerated,\n  att.attidentity,\n  tc.constraint_name,\n  c.character_maximum_length\nORDER BY c.ordinal_position;       \n</code></pre> Output<pre><code>table_schema | table_name | column_name | type_name |          data_type          | data_type_char_max |     constraint_name      | is_primary | is_foreign | is_unique | is_nullable | att_generated | att_identity | enum_values | comment\n--------------+------------+-------------+-----------+-----------------------------+--------------------+--------------------------+------------+------------+-----------+-------------+---------------+--------------+-------------+---------\npublic       | customer   | customer_id | int4      | integer                     |                    | customer_pkey            | t          | f          | f         | f           |               |              |             |\npublic       | customer   | store_id    | int2      | smallint                    |                    |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | first_name  | varchar   | character varying           |                 45 |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | last_name   | varchar   | character varying           |                 45 |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | email       | varchar   | character varying           |                 50 |                          | f          | f          | f         | t           |               |              |             |\npublic       | customer   | address_id  | int2      | smallint                    |                    | customer_address_id_fkey | f          | t          | f         | f           |               |              |             |\npublic       | customer   | activebool  | bool      | boolean                     |                    |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | create_date | date      | date                        |                    |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | last_update | timestamp | timestamp without time zone |                    |                          | f          | f          | f         | t           |               |              |             |\npublic       | customer   | active      | int4      | integer                     |                    |                          | f          | f          | f         | t           |               |              |             |\n(10 rows)\n</code></pre> Query<pre><code>select c.table_schema as table_schema,\n  c.table_name as table_name,\n  c.column_name as column_name,\n  c.data_type as type_name,\n  c.data_type as data_type,\n    c.character_maximum_length AS data_type_char_max,\n  k.constraint_name AS constraint_name,\n  (\n    select count(*) &gt; 0\n    from information_schema.KEY_COLUMN_USAGE\n    where table_name = c.table_name\n      and column_name = c.column_name\n      and constraint_name = 'PRIMARY'\n  ) as is_primary,\n  (\n    select count(*) &gt; 0\n    from information_schema.key_column_usage cu\n      left join information_schema.table_constraints tc on tc.constraint_name = cu.constraint_name\n    where cu.column_name = c.column_name\n      and cu.table_name = c.table_name\n      and tc.constraint_type = 'FOREIGN KEY'\n  ) as is_foreign,\n  (\n    select count(*) &gt; 0\n    from information_schema.key_column_usage cu\n      left join information_schema.table_constraints tc on tc.constraint_name = cu.constraint_name\n    where cu.column_name = c.column_name\n      and cu.table_name = c.table_name\n      and tc.constraint_type = 'UNIQUE'\n  ) as is_unique,\n  IF(c.is_nullable = 'YES', 1, 0) as is_nullable,\n  case\n    when c.data_type = 'enum' then REPLACE(\n      REPLACE(\n        REPLACE(REPLACE(c.column_type, 'enum', ''), '\\'', ''),\n        '(',\n        ''\n      ),\n      ')',\n      ''\n    )\n    else ''\n  end as enum_values,\n  c.column_comment as comment\nfrom information_schema.columns c\nLEFT JOIN information_schema.key_column_usage k ON c.table_schema = k.table_schema\nAND c.table_name = k.table_name\n  AND c.column_name = k.column_name\nwhere c.table_name = 'customer' -- replace with your table name\n  and c.TABLE_SCHEMA = 'sakila' -- replace with your schema\norder by c.ordinal_position;\n</code></pre> Output<pre><code>+--------------+------------+-------------+-----------+-----------+--------------------+---------------------+------------+------------+-----------+-------------+-------------+---------+\n| table_schema | table_name | column_name | type_name | data_type | data_type_char_max | constraint_name     | is_primary | is_foreign | is_unique | is_nullable | enum_values | comment |\n+--------------+------------+-------------+-----------+-----------+--------------------+---------------------+------------+------------+-----------+-------------+-------------+---------+\n| sakila       | customer   | customer_id | smallint  | smallint  |               NULL | PRIMARY             |          1 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | store_id    | tinyint   | tinyint   |               NULL | fk_customer_store   |          0 |          1 |         0 |           0 |             |         |\n| sakila       | customer   | first_name  | varchar   | varchar   |                 45 | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | last_name   | varchar   | varchar   |                 45 | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | email       | varchar   | varchar   |                 50 | NULL                |          0 |          0 |         0 |           1 |             |         |\n| sakila       | customer   | address_id  | smallint  | smallint  |               NULL | fk_customer_address |          0 |          1 |         0 |           0 |             |         |\n| sakila       | customer   | active      | tinyint   | tinyint   |               NULL | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | create_date | datetime  | datetime  |               NULL | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | last_update | timestamp | timestamp |               NULL | NULL                |          0 |          0 |         0 |           1 |             |         |\n+--------------+------------+-------------+-----------+-----------+--------------------+---------------------+------------+------------+-----------+-------------+-------------+---------+\n9 rows in set (0.28 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#show-primary-keys", "title": "Show Primary Keys", "text": "<p>Primary Keys</p> PostgresMySQL Query<pre><code>SELECT cl.relname as table_name,\n  a.attname AS column_name\nFROM pg_constraint c\n  JOIN pg_namespace ns ON ns.oid = c.connamespace\n  JOIN pg_class cl ON cl.oid = c.conrelid\n  JOIN pg_attribute a ON a.attnum = ANY(c.conkey)\n  AND a.attrelid = c.conrelid\nWHERE (\n    ('public' = '') IS NOT FALSE -- replace with your schema\n    OR ns.nspname = 'public' -- replace with your schema\n  )\n  AND cl.relname = 'address' -- replace with your table name\n  AND c.contype = 'p'\nORDER BY a.attnum;\n</code></pre> Output<pre><code>table_name | column_name\n------------+-------------\naddress    | address_id\n(1 row)\n</code></pre> Query<pre><code>SELECT DISTINCT(column_name) as attname,\n  table_name as table_name,\n  table_schema as table_schema,\n  constraint_name as constraint_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'sakila' -- replace with your schema\n  AND table_name = 'customer' -- replace with your table name\n  AND constraint_name = 'PRIMARY';\n</code></pre> Output<pre><code>+-------------+------------+--------------+-----------------+\n| attname     | table_name | table_schema | constraint_name |\n+-------------+------------+--------------+-----------------+\n| customer_id | customer   | sakila       | PRIMARY         |\n+-------------+------------+--------------+-----------------+\n1 row in set (0.00 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#database-relationships", "title": "Database Relationships", "text": "<p>All Database Relationships</p> PostgresMySQL Query<pre><code>SELECT fk_nsp.nspname || '.' || fk_table AS fk_table,\n  array_agg(\n    fk_att.attname\n    ORDER BY fk_att.attnum\n  ) AS fk_columns,\n  tar_nsp.nspname || '.' || ref_table AS ref_table,\n  array_agg(\n    tar_att.attname\n    ORDER BY fk_att.attnum\n  ) AS ref_columns\nFROM (\n    SELECT fk.oid AS fk_table_id,\n      fk.relnamespace AS fk_schema_id,\n      fk.relname AS fk_table,\n      unnest(con.conkey) as fk_column_id,\n      tar.oid AS target_table_id,\n      tar.relnamespace AS target_schema_id,\n      tar.relname AS ref_table,\n      unnest(con.confkey) as target_column_id,\n      con.connamespace AS constraint_nsp,\n      con.conname AS constraint_name\n    FROM pg_constraint con\n      JOIN pg_class fk ON con.conrelid = fk.oid\n      JOIN pg_class tar ON con.confrelid = tar.oid\n    WHERE con.contype = 'f'\n  ) sub\n  JOIN pg_attribute fk_att ON fk_att.attrelid = fk_table_id\n  AND fk_att.attnum = fk_column_id\n  JOIN pg_attribute tar_att ON tar_att.attrelid = target_table_id\n  AND tar_att.attnum = target_column_id\n  JOIN pg_namespace fk_nsp ON fk_schema_id = fk_nsp.oid\n  JOIN pg_namespace tar_nsp ON target_schema_id = tar_nsp.oid\nGROUP BY 1,\n  3,\n  sub.constraint_nsp,\n  sub.constraint_name;\n</code></pre> Output<pre><code>      fk_table       |     fk_columns     |    ref_table     |  ref_columns\n----------------------+--------------------+------------------+----------------\npublic.address       | {city_id}          | public.city      | {city_id}\npublic.city          | {country_id}       | public.country   | {country_id}\npublic.customer      | {address_id}       | public.address   | {address_id}\npublic.film          | {language_id}      | public.language  | {language_id}\npublic.film_actor    | {actor_id}         | public.actor     | {actor_id}\npublic.film_actor    | {film_id}          | public.film      | {film_id}\npublic.film_category | {category_id}      | public.category  | {category_id}\npublic.film_category | {film_id}          | public.film      | {film_id}\npublic.inventory     | {film_id}          | public.film      | {film_id}\npublic.payment       | {customer_id}      | public.customer  | {customer_id}\npublic.payment       | {rental_id}        | public.rental    | {rental_id}\npublic.payment       | {staff_id}         | public.staff     | {staff_id}\npublic.rental        | {customer_id}      | public.customer  | {customer_id}\npublic.rental        | {inventory_id}     | public.inventory | {inventory_id}\npublic.rental        | {staff_id}         | public.staff     | {staff_id}\npublic.staff         | {address_id}       | public.address   | {address_id}\npublic.store         | {address_id}       | public.address   | {address_id}\npublic.store         | {manager_staff_id} | public.staff     | {staff_id}\n(18 rows)\n</code></pre> Query<pre><code>SELECT concat(table_schema, '.', table_name) AS fk_table,\n  group_concat(\n    column_name\n    ORDER BY ordinal_position\n  ) AS fk_columns,\n  concat(\n    referenced_table_schema,\n    '.',\n    referenced_table_name\n  ) AS ref_table,\n  group_concat(\n    referenced_column_name\n    ORDER BY ordinal_position\n  ) AS ref_columns\nFROM information_schema.key_column_usage\nWHERE referenced_table_schema NOT IN (\n    'information_schema',\n    'performance_schema',\n    'sys',\n    'mysql',\n    'innodb',\n    'tmp'\n  ) AND table_schema = 'sakila' -- replace with your schema\n\nGROUP BY 1,\n  3,\n  constraint_schema,\n  constraint_name;\n</code></pre> Output<pre><code>+----------------------+----------------------+------------------+--------------+\n| fk_table             | fk_columns           | ref_table        | ref_columns  |\n+----------------------+----------------------+------------------+--------------+\n| sakila.address       | city_id              | sakila.city      | city_id      |\n| sakila.city          | country_id           | sakila.country   | country_id   |\n| sakila.customer      | address_id           | sakila.address   | address_id   |\n| sakila.customer      | store_id             | sakila.store     | store_id     |\n| sakila.film          | language_id          | sakila.language  | language_id  |\n| sakila.film          | original_language_id | sakila.language  | language_id  |\n| sakila.film_actor    | actor_id             | sakila.actor     | actor_id     |\n| sakila.film_actor    | film_id              | sakila.film      | film_id      |\n| sakila.film_category | category_id          | sakila.category  | category_id  |\n| sakila.film_category | film_id              | sakila.film      | film_id      |\n| sakila.inventory     | film_id              | sakila.film      | film_id      |\n| sakila.inventory     | store_id             | sakila.store     | store_id     |\n| sakila.payment       | customer_id          | sakila.customer  | customer_id  |\n| sakila.payment       | rental_id            | sakila.rental    | rental_id    |\n| sakila.payment       | staff_id             | sakila.staff     | staff_id     |\n| sakila.rental        | customer_id          | sakila.customer  | customer_id  |\n| sakila.rental        | inventory_id         | sakila.inventory | inventory_id |\n| sakila.rental        | staff_id             | sakila.staff     | staff_id     |\n| sakila.staff         | address_id           | sakila.address   | address_id   |\n| sakila.staff         | store_id             | sakila.store     | store_id     |\n| sakila.store         | address_id           | sakila.address   | address_id   |\n| sakila.store         | manager_staff_id     | sakila.staff     | staff_id     |\n+----------------------+----------------------+------------------+--------------+\n22 rows in set (0.02 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#forward-relationships", "title": "Forward Relationships", "text": "<pre><code>erDiagram\n\npayment {\n        numeric amount\n        smallint customer_id FK\n        timestamp_without_time_zone payment_date\n        integer payment_id PK\n        integer rental_id FK\n        smallint staff_id FK\n    }\n\ncustomer {\n        integer active\n        boolean activebool\n        smallint address_id FK\n        date create_date\n        integer customer_id PK\n        character_varying email\n        character_varying first_name\n        character_varying last_name\n        timestamp_without_time_zone last_update\n        smallint store_id\n    }\n\nrental {\n    smallint customer_id FK\n    integer inventory_id FK\n    timestamp_without_time_zone last_update\n    timestamp_without_time_zone rental_date\n    integer rental_id PK\n    timestamp_without_time_zone return_date\n    smallint staff_id FK\n}\n\nstaff {\n    boolean active\n    smallint address_id FK\n    character_varying email\n    character_varying first_name\n    character_varying last_name\n    timestamp_without_time_zone last_update\n    character_varying password\n    bytea picture\n    integer staff_id PK\n    smallint store_id\n    character_varying username\n}\n\npayment }o--|| customer : \"customer_id\"\nrental }o--|| customer : \"customer_id\"\npayment }o--|| rental : \"rental_id\"\npayment }o--|| staff : \"staff_id\"\nrental }o--|| staff : \"staff_id\"\n</code></pre> <p>Forward Relationships</p> PostgresMySQL Query<pre><code>SELECT\n  COALESCE(conname, '') AS constraint_name,\n  conrelid::regclass AS table_name,\n  a.attname AS column_name,\n  confrelid::regclass AS referenced_table_name,\n  COALESCE(af.attname, '') AS referenced_column_name\nFROM\n  pg_constraint c\nJOIN\n  pg_namespace n ON n.oid = c.connamespace\nJOIN\n  pg_class cl ON cl.oid = c.conrelid\nJOIN\n  pg_attribute a ON a.attnum = ANY(c.conkey) AND a.attrelid = c.conrelid\nJOIN\n  pg_class conf ON conf.oid = c.confrelid\nJOIN\n  pg_attribute af ON af.attnum = ANY(c.confkey) AND af.attrelid = c.confrelid\nWHERE\n  n.nspname = 'public' -- replace with your schema\n  AND cl.relname = 'payment' -- replace with your table name\n  AND c.contype = 'f'\nORDER BY\n  constraint_name;\n</code></pre> Output<pre><code>    constraint_name      | table_name | column_name | referenced_table_name | referenced_column_name\n--------------------------+------------+-------------+-----------------------+------------------------\npayment_customer_id_fkey | payment    | customer_id | customer              | customer_id\npayment_rental_id_fkey   | payment    | rental_id   | rental                | rental_id\npayment_staff_id_fkey    | payment    | staff_id    | staff                 | staff_id\n(3 rows)\n</code></pre> Query<pre><code>SELECT coalesce(table_name, '') as table_name,\n  coalesce (column_name, '') as column_name,\n  coalesce(referenced_column_name, '') as referenced_column_name,\n  coalesce(referenced_table_name, '') as referenced_table_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'sakila' -- replace with your schema\n  AND table_name = 'payment' -- replace with your table name\n  AND referenced_table_name != 'NULL';\n</code></pre> Output<pre><code>+------------+-------------+------------------------+-----------------------+\n| table_name | column_name | referenced_column_name | referenced_table_name |\n+------------+-------------+------------------------+-----------------------+\n| payment    | customer_id | customer_id            | customer              |\n| payment    | rental_id   | rental_id              | rental                |\n| payment    | staff_id    | staff_id               | staff                 |\n+------------+-------------+------------------------+-----------------------+\n3 rows in set (0.01 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#reverse-relationships", "title": "Reverse Relationships", "text": "<p>ER Diagram</p> <pre><code>erDiagram\n\npayment {\n        numeric amount\n        smallint customer_id FK\n        timestamp_without_time_zone payment_date\n        integer payment_id PK\n        integer rental_id FK\n        smallint staff_id FK\n    }\n\ncustomer {\n        integer active\n        boolean activebool\n        smallint address_id FK\n        date create_date\n        integer customer_id PK\n        character_varying email\n        character_varying first_name\n        character_varying last_name\n        timestamp_without_time_zone last_update\n        smallint store_id\n    }\n\nrental {\n    smallint customer_id FK\n    integer inventory_id FK\n    timestamp_without_time_zone last_update\n    timestamp_without_time_zone rental_date\n    integer rental_id PK\n    timestamp_without_time_zone return_date\n    smallint staff_id FK\n}\n\n\npayment }o--|| customer : \"customer_id\"\nrental }o--|| customer : \"customer_id\"\npayment }o--|| rental : \"rental_id\"</code></pre> <p>Reverse Relationships</p> PostgresMySQL Query<pre><code>SELECT\n  COALESCE(conname, '') AS constraint_name,\n  conrelid::regclass AS table_name,\n  a.attname AS column_name,\n  confrelid::regclass AS referenced_table_name,\n  COALESCE(af.attname, '') AS referenced_column_name\nFROM\n  pg_constraint c\nJOIN\n  pg_namespace n ON n.oid = c.connamespace\nJOIN\n  pg_class cl ON cl.oid = c.conrelid\nJOIN\n  pg_attribute a ON a.attnum = ANY(c.conkey) AND a.attrelid = c.conrelid\nJOIN\n  pg_class conf ON conf.oid = c.confrelid\nJOIN\n  pg_attribute af ON af.attnum = ANY(c.confkey) AND af.attrelid = c.confrelid\nWHERE\n  n.nspname = 'public' -- replace with your schema\n  AND\n  confrelid::regclass::text = 'customer' -- replace with your table name\nORDER BY\n  constraint_name;\n</code></pre> Output<pre><code>    constraint_name      | table_name | column_name | referenced_table_name | referenced_column_name\n--------------------------+------------+-------------+-----------------------+------------------------\npayment_customer_id_fkey | payment    | customer_id | customer              | customer_id\nrental_customer_id_fkey  | rental     | customer_id | customer              | customer_id\n(2 rows)\n</code></pre> Query<pre><code>SELECT coalesce(table_name, '') as table_name,\n  coalesce (column_name, '') as column_name,\n  coalesce(referenced_column_name, '') as referenced_column_name,\n  coalesce(referenced_table_name, '') as referenced_table_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'sakila' -- replace with your schema\n  AND referenced_table_name = 'customer'; -- replace with your table name\n</code></pre> Output<pre><code>+------------+-------------+------------------------+-----------------------+\n| table_name | column_name | referenced_column_name | referenced_table_name |\n+------------+-------------+------------------------+-----------------------+\n| payment    | customer_id | customer_id            | customer              |\n| rental     | customer_id | customer_id            | customer              |\n+------------+-------------+------------------------+-----------------------+\n2 rows in set (0.00 sec)\n</code></pre>"}, {"location": "cloud-storage-engines/amazon-s3/", "title": "Amazon S3 Storage Engine", "text": ""}, {"location": "cloud-storage-engines/amazon-s3/#overview", "title": "Overview", "text": "<p>The Amazon S3 Storage Engine allows developers to store large amounts of unstructured data using the Amazon Web Services S3 service.</p>"}, {"location": "cloud-storage-engines/amazon-s3/#requirements", "title": "Requirements", "text": "<p>To use the Amazon S3 Storage engine, you must have a Amazon Web Services account. You will need to create a programmatic IAM user and provide the access key and secret key to the DBSnapper Cloud Storage Profile page. You will also need to provide the AWS region that you're using for your S3 storage.</p> <p>Create a bucket in your Amazon S3 account and note the bucket name. You can specify an optional bucket prefix that you can use to organize your snapshots.</p>"}, {"location": "cloud-storage-engines/cloudflare-r2/", "title": "Cloudflare R2 Storage Engine", "text": ""}, {"location": "cloud-storage-engines/cloudflare-r2/#overview", "title": "Overview", "text": "<p>The Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services. It uses an Amazon S3 compatible API to store data with high durability and availability.</p>"}, {"location": "cloud-storage-engines/cloudflare-r2/#requirements", "title": "Requirements", "text": "<p>To use the Cloudflare R2 Storage engine, you must have a Cloudflare R2 account. You can sign up for a free Cloudflare account at https://dash.cloudflare.com/sign-up. You will then need to generate an API key that consists of an access key and secret key. Using these credentials and your cloudflare account id.</p> <p>Create a bucket in your Cloudflare R2 account and note the bucket name. You can specify an optional bucket prefix that you can use to organize your snapshots.</p>"}, {"location": "cloud-storage-engines/configuration/", "title": "Storage Engine Configuration", "text": "<p>You specify configurations for storage engines in the DBSnapper configuration file under the <code>storage_profiles</code> section. These storage engines can be used with sharing targets to retrieve shared snapshots from cloud storage.</p> <p>Several variations of <code>storage_profile</code> configurations are shown in the following example for different cloud storage providers and methods of retrieving credentials.</p>"}, {"location": "cloud-storage-engines/configuration/#example", "title": "Example", "text": "<p><code>storage profiles</code> configuration</p> <pre><code># Storage profiles for sharing configurations\nstorage_profiles:\n  # All Profiles:\n  # Leave access_key and secret_key to use a profile\n  # Leave the awscli_profile empty to use the default profile\n  # If the endpoint is specified in the profile it will override the\n  # endpoint specified in this configuration.\n\n  # S3\n  s3-from-credentials:\n    provider: s3\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    region: &lt;region&gt;\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  s3-from-awscli-profile:\n    provider: s3\n    awscli_profile: dbsnapper_credentials\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  # R2\n  # Uses the account ID to create the endpoint url.\n  # Can be omitted if the r2 endpoint_url is specified in the awscli config.\n  # Or set the following Env Variables and leave the awscli_profile empty:\n  # AWS_ACCESS_KEY\n  # AWS_SECRET_KEY\"\n  # AWS_ENDPOINT_URL\"\n  r2-from-credentials:\n    provider: r2\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    account_id: &lt;account_id&gt;\n    bucket: dbsnapper-test-r2\n    prefix:\n\n  r2-from-awscli-profile:\n    provider: r2\n    awscli_profile: r2_production\n    account_id: # optional if enpoint_url set in profile\n    bucket: dbsnapper-test-r2\n    prefix: \n\n  r2-from-env-or-default-awscli_profile:\n    provider: r2\n    bucket: dbsnapper-test-r2\n    prefix: sanitized\n\n  # Minio\n  minio-from-credentials:\n    provider: minio\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    endpoint: http://localhost:9000\n    bucket: dbsnapper-test-minio\n    prefix:\n\n  # Digital Ocean Spaces\n  # Endpoint shold be set to the endpoint of the spaces region i.e. nyc3 below\n  dospaces-from-credentials:\n    provider: dospaces\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    endpoint: https://nyc3.digitaloceanspaces.com\n    bucket: dbsnapper-test-do\n    prefix:\n</code></pre>"}, {"location": "cloud-storage-engines/configuration/#configuration-options", "title": "Configuration Options", "text": "<p>Storage profile configuration options essentially consist of a section to configure credentials and service endpoints, and another section for specifying the <code>bucket</code> and optional <code>prefix</code> for snapshots.</p> <p>Storage Profile configuration options (* denotes required)</p> <code>provider</code> * <p>The name of the cloud storage provider. Supported providers are <code>s3</code>, <code>r2</code>, <code>minio</code>, and <code>dospaces</code></p> <code>awscli_profile</code> <p>If you have configured the AWS CLI with named profiles, you can specify the profile name here. If this is not specified and the <code>access_key</code> and <code>secret_key</code> are omitted, environment variables or the default profile will be used.</p> <code>access_key</code> <p>The access key for the cloud storage provider. This can be omitted if the <code>awscli_profile</code> is specified or you have the appropriate environment variables set</p> <code>secret_key</code> <p>The secret key for the cloud storage provider. This can be omitted if the <code>awscli_profile</code> is specified or you have the appropriate environment variables set</p> <code>region</code> <p>The region for the cloud storage provider. This is required for AWS S3 storage profiles.</p> <code>account_id</code> <p>The account ID for the Cloudflare R2 storage provider. This is required for R2 storage profiles and is used in creating the endpoint URL.</p> <code>endpoint</code> <p>The endpoint URL for the cloud storage provider. This is required for Minio and Digital Ocean Spaces storage profiles, and is automatically generated for R2 storage profiles using the account ID.</p> <code>bucket</code> * <p>The name of the bucket in the cloud storage provider where snapshots will be stored.</p> <code>prefix</code> <p>An optional prefix to use for the snapshots in the cloud storage provider. This can be used to organize snapshots in the bucket.</p>"}, {"location": "cloud-storage-engines/configuration/#aws-shared-configuration", "title": "AWS Shared Configuration", "text": "<p>DBSnapper supports retrieving credentials from the AWS CLI shared configuration.</p>"}, {"location": "cloud-storage-engines/configuration/#credentials-retrieval", "title": "Credentials Retrieval", "text": "<p>Credentials are retrieved from the shared configuration in the following order:</p> <ol> <li>If the <code>access_key</code> and <code>secret_key</code> are specified in the storage profile configuration, those credentials will be used and the shared configuration will be ignored.</li> <li>The envirnoment variables <code>AWS_ACCESS_KEY</code> and <code>AWS_SECRET_KEY</code> will be used if they are set. Along with <code>AWS_REGION</code> and <code>AWS_ENDPOINT_URL</code> if they are needed for the storage provider.</li> <li>If the <code>awscli_profile</code> is specified in the storage profile configuration, the credentials for the specified profile will be used.</li> <li>If the <code>awscli_profile</code> is not specified and the above values are not present, the default profile (if it exists) will be used.</li> </ol>"}, {"location": "cloud-storage-engines/configuration/#environment-variables", "title": "Environment Variables", "text": "<p>If the <code>awscli_profile</code> is not specified and the <code>access_key</code> and <code>secret_key</code> are not set, the following environment variables can be used to set the credentials:</p> <ul> <li><code>AWS_ACCESS_KEY</code></li> <li><code>AWS_SECRET</code></li> <li><code>AWS_REGION</code></li> <li><code>AWS_ENDPOINT_URL</code></li> </ul> <p>See the AWS CLI shared configuration documentation for more information on supported environment variables.</p>"}, {"location": "cloud-storage-engines/configuration/#endpoint-url", "title": "Endpoint URL", "text": "<p>Non AWS S3 storage providers require the <code>endpoint</code> to be specified in the storage profile configuration. This is the URL for the storage provider's API. For R2 storage profiles, the endpoint URL is automatically generated using the account ID.</p> <p>You may also provide the endpoint URL in the AWS CLI shared configuration. If the endpoint is specified in the profile, it will override the endpoint specified in the configuration.</p>"}, {"location": "cloud-storage-engines/introduction/", "title": "Introduction", "text": "<p>DBSnapper provides support for several Cloud Storage Engines that allow you to use your own private cloud storage provider to store your database snapshots keeping them in your control and secure.</p> <p>Cloud storage engine configurations can be specified on the DBSnapper Cloud and in the configuration file under the <code>storage_profiles</code> section. These storage engines can be used with sharing targets to retrieve shared snapshots from cloud storage.</p> <p> </p>"}, {"location": "cloud-storage-engines/introduction/#supported-cloud-storage-engines", "title": "Supported Cloud Storage Engines", "text": "<p>Currently supported Cloud Storage engines include:</p> <ul> <li>Amazon S3</li> <li>Cloudflare R2</li> <li>Minio</li> <li>Digital Ocean Spaces</li> </ul>"}, {"location": "cloud-storage-engines/introduction/#getting-started", "title": "Getting Started", "text": "<p>To get started with using a cloud storage engine, you will need to configure a <code>storage_profile</code> in your DBSnapper configuration file. The <code>storage_profile</code> configuration specifies the cloud storage provider, credentials, and bucket information for the storage engine.</p> <p>AWS S3 configuration examples</p> <pre><code>storage_profiles:\n  s3-with-provided-credentials:\n    provider: s3\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    region: &lt;region&gt;\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  s3-from-awscli-shared-profile:\n    provider: s3\n    awscli_profile: dbsnapper_credentials\n    bucket: dbsnapper-test-s3\n    prefix:\n\ntargets:\n  # Share target\n  shared-s3:\n    name: shared-s3\n    share:\n      storage_profile: s3-from-awscli-shared-profile\n      dst_url: postgres://localhost:5432/dbsnapper_test\n</code></pre> <p>In the example above we have two <code>storage_profiles</code> configurations for AWS S3. The first configuration <code>s3-with-provided-credentials</code> explicitly specifies the access key, secret key, region, and bucket for an S3 storage engine. <code>s3-from-awscli-shared-profile</code>, on the other hand, indicates that we want to retrieve the credentials from the <code>dbsnapper_credentials</code> AWS shared configuration profile as specified in the <code>awscli_profile</code> field.</p> <p>We have also defined a <code>share target</code> configuration on line 19 that uses the <code>s3-from-awscli-shared-profile</code> storage profile to access shared snapshots from the <code>dbsnapper-test-s3</code> bucket. These shared snapshots will be loaded into the <code>dbsnapper_test</code> database as specified in the <code>dst_url</code> field on line 23.</p>"}, {"location": "cmd/dbsnapper/", "title": "dbsnapper", "text": ""}, {"location": "cmd/dbsnapper/#dbsnapper_1", "title": "dbsnapper", "text": "<p>Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</p>"}, {"location": "cmd/dbsnapper/#synopsis", "title": "Synopsis", "text": "<p>DBSnapper is a database snapshot creation and management tool for developers.      DBSnapper makes it easy to snapshot, subset, sanitize and share your database to speed up application development and testing with real-world data.</p> <pre><code>You can specify your database targets and other configuration properties in the `~/.config/dbsnapper/dbsnapper.yml` configuration file.\n</code></pre>"}, {"location": "cmd/dbsnapper/#options", "title": "Options", "text": "<pre><code>      --config string   config file (default is ~/.config/dbsnapper/dbsnapper.yml)\n  -h, --help            help for dbsnapper\n      --nocloud         Disable cloud mode to speed up operations by skipping cloud API calls\n</code></pre>"}, {"location": "cmd/dbsnapper/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper auth    - Login to DBSnapper Cloud</li> <li>dbsnapper build  - Build a database snapshot</li> <li>dbsnapper config    - Configuration commands</li> <li>dbsnapper dev  - Development utilities (requires feature flag)</li> <li>dbsnapper load    - Load a target snapshot or SQL file to a database</li> <li>dbsnapper mcp  - Start the MCP (Model Context Protocol) server</li> <li>dbsnapper pull    - Retrieve a snapshot and store it locally</li> <li>dbsnapper sanitize    - Used to sanitize a snapshot</li> <li>dbsnapper subset    - Used to subset a target based on the configuration in targets..subset <li>dbsnapper target    - Used to list snapshots for a target</li> <li>dbsnapper targets  - Use to list all targets</li>"}, {"location": "commands/auth-token/", "title": "Auth Token Command", "text": "<p>The <code>auth token</code> command configures your DBSnapper Cloud API token, enabling authentication with DBSnapper Cloud services. This essential command unlocks cloud features including shared targets, cloud storage, and team collaboration capabilities.</p>"}, {"location": "commands/auth-token/#overview", "title": "Overview", "text": "<p>The auth token command provides secure API token management: - Cloud authentication: Establishes secure connection to DBSnapper Cloud - Token storage: Encrypts and stores API tokens in configuration file - Access enablement: Unlocks cloud targets, shared targets, and cloud storage - Team integration: Enables participation in team collaboration features - Multi-environment support: Manages different tokens for different environments</p>"}, {"location": "commands/auth-token/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper auth token &lt;api_token&gt; [flags]\n</code></pre>"}, {"location": "commands/auth-token/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;api_token&gt;</code>: The API token obtained from DBSnapper Cloud dashboard</li> </ul>"}, {"location": "commands/auth-token/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for token\n</code></pre>"}, {"location": "commands/auth-token/#api-token-acquisition", "title": "API Token Acquisition", "text": ""}, {"location": "commands/auth-token/#getting-your-api-token", "title": "Getting Your API Token", "text": "<ol> <li>Visit DBSnapper Cloud: Navigate to https://app.dbsnapper.com</li> <li>Create Account: Register for a new account or sign in to existing account</li> <li>Access Dashboard: Navigate to the main dashboard after login</li> <li>Get Started Page: Go to the \"Get Started\" section</li> <li>Copy Token: Copy your personal API token from the Get Started page</li> </ol>"}, {"location": "commands/auth-token/#token-characteristics", "title": "Token Characteristics", "text": "<ul> <li>Unique identifier: Each user has a unique API token</li> <li>Secure format: Long, cryptographically secure string</li> <li>Limited scope: Token permissions are limited to your account</li> <li>Revocable: Tokens can be revoked from the cloud dashboard</li> </ul>"}, {"location": "commands/auth-token/#how-it-works", "title": "How It Works", "text": ""}, {"location": "commands/auth-token/#token-storage-process", "title": "Token Storage Process", "text": "<ol> <li>Validation: Verifies token format and basic validity</li> <li>Encryption: Encrypts token using your configuration's secret key</li> <li>Configuration update: Adds encrypted token to configuration file</li> <li>Cloud connection: Tests connection to verify token functionality</li> <li>Feature activation: Enables cloud features for subsequent commands</li> </ol>"}, {"location": "commands/auth-token/#configuration-integration", "title": "Configuration Integration", "text": "<p>The token is stored in your configuration file's <code>authtoken</code> field:</p> <pre><code># Before token configuration\nauthtoken: \"\"\n\n# After token configuration\nauthtoken: \"encrypted_token_value_here\"\n</code></pre>"}, {"location": "commands/auth-token/#token-security", "title": "Token Security", "text": ""}, {"location": "commands/auth-token/#encryption-at-rest", "title": "Encryption at Rest", "text": "<ul> <li>Secret key encryption: Tokens encrypted using your configuration's secret key</li> <li>No plain text: Tokens never stored in plain text</li> <li>File protection: Configuration file should have restricted permissions</li> </ul>"}, {"location": "commands/auth-token/#secure-handling", "title": "Secure Handling", "text": "<pre><code># Set appropriate file permissions\nchmod 600 ~/.config/dbsnapper/dbsnapper.yml\n\n# Verify permissions\nls -la ~/.config/dbsnapper/dbsnapper.yml\n# Should show: -rw------- (owner read/write only)\n</code></pre>"}, {"location": "commands/auth-token/#environment-variable-alternative", "title": "Environment Variable Alternative", "text": "<p>For automation and CI/CD, use environment variables:</p> <pre><code># Set token via environment variable\nexport DBSNAPPER_AUTHTOKEN=\"your_token_here\"\n\n# DBSnapper will use environment variable over config file\ndbsnapper targets  # Uses token from environment\n</code></pre>"}, {"location": "commands/auth-token/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/auth-token/#basic-token-configuration", "title": "Basic Token Configuration", "text": "<pre><code># Configure API token (replace with your actual token)\ndbsnapper auth token hNsdelkjcxgoiuwalkmcgoi1234567890abcdef\n</code></pre> <p>Output: <pre><code>\u2705 DBSnapper Cloud API token configured successfully\n\u2705 Cloud connectivity verified\n\ud83d\udd17 You now have access to cloud features:\n  - Cloud targets\n  - Shared targets  \n  - Cloud storage profiles\n  - Team collaboration\n</code></pre></p>"}, {"location": "commands/auth-token/#environment-specific-configuration", "title": "Environment-Specific Configuration", "text": "<pre><code># Configure token for development environment\ndbsnapper auth token --config ~/.config/dbsnapper/dev.yml &lt;dev_token&gt;\n\n# Configure token for production environment\ndbsnapper auth token --config ~/.config/dbsnapper/prod.yml &lt;prod_token&gt;\n\n# Use specific environment\ndbsnapper --config ~/.config/dbsnapper/dev.yml targets\n</code></pre>"}, {"location": "commands/auth-token/#verification-after-configuration", "title": "Verification After Configuration", "text": "<pre><code># Verify cloud connectivity\ndbsnapper config check | grep \"DBSnapper Cloud\"\n\n# List targets to see cloud and shared targets\ndbsnapper targets\n\n# Check for cloud-specific features\ndbsnapper targets | grep -E \"(cloud|share)\"\n</code></pre>"}, {"location": "commands/auth-token/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/auth-token/#individual-developer-setup", "title": "Individual Developer Setup", "text": "<pre><code># Personal development setup\ndbsnapper auth token &lt;personal_token&gt;\n\n# Verify access to personal cloud targets\ndbsnapper targets | grep cloud\n\n# Access shared team data\ndbsnapper targets | grep share\n</code></pre>"}, {"location": "commands/auth-token/#team-member-onboarding", "title": "Team Member Onboarding", "text": "<pre><code># New team member setup\n# 1. Team lead provides onboarding instructions\n# 2. New member creates DBSnapper Cloud account\n# 3. Configures authentication\ndbsnapper auth token &lt;new_member_token&gt;\n\n# 4. Verifies access to shared team resources\ndbsnapper targets | grep share\ndbsnapper pull shared-development-data\n</code></pre>"}, {"location": "commands/auth-token/#cicd-pipeline-configuration", "title": "CI/CD Pipeline Configuration", "text": "<pre><code># Automated pipeline setup\n# Store token in CI/CD secrets management\nexport DBSNAPPER_AUTHTOKEN=\"$CI_DBSNAPPER_TOKEN\"\n\n# Verify authentication in pipeline\nif ! dbsnapper config check | grep -q \"DBSnapper Cloud connected\"; then\n    echo \"Cloud authentication failed\"\n    exit 1\nfi\n\n# Use cloud features in automation\ndbsnapper pull production-snapshots\n</code></pre>"}, {"location": "commands/auth-token/#multi-environment-management", "title": "Multi-Environment Management", "text": "<pre><code># Development environment\ndbsnapper auth token --config config/dev.yml &lt;dev_token&gt;\n\n# Staging environment\ndbsnapper auth token --config config/staging.yml &lt;staging_token&gt;\n\n# Production environment\ndbsnapper auth token --config config/prod.yml &lt;prod_token&gt;\n\n# Verify each environment\nfor env in dev staging prod; do\n    echo \"Testing $env environment...\"\n    dbsnapper --config config/$env.yml config check\ndone\n</code></pre>"}, {"location": "commands/auth-token/#token-rotation", "title": "Token Rotation", "text": "<pre><code># Regular security maintenance\n# 1. Generate new token in cloud dashboard\n# 2. Update configuration\ndbsnapper auth token &lt;new_token&gt;\n\n# 3. Verify new token works\ndbsnapper config check\n\n# 4. Revoke old token in cloud dashboard\n</code></pre>"}, {"location": "commands/auth-token/#cloud-features-enabled", "title": "Cloud Features Enabled", "text": ""}, {"location": "commands/auth-token/#after-token-configuration", "title": "After Token Configuration", "text": "<p>Once authenticated, you gain access to:</p>"}, {"location": "commands/auth-token/#cloud-targets", "title": "Cloud Targets", "text": "<pre><code># List cloud-managed targets\ndbsnapper targets | grep cloud\n\n# Build snapshots with cloud storage\ndbsnapper build cloud-production-db\n</code></pre>"}, {"location": "commands/auth-token/#shared-targets", "title": "Shared Targets", "text": "<pre><code># Access team-shared targets (via Okta SSO)\ndbsnapper targets | grep share\n\n# Pull shared sanitized snapshots\ndbsnapper pull team-development-data\n</code></pre>"}, {"location": "commands/auth-token/#cloud-storage-profiles", "title": "Cloud Storage Profiles", "text": "<pre><code># Configure cloud storage in your configuration\nstorage_profiles:\n  aws-s3:\n    type: \"s3\"\n    region: \"us-west-2\"\n    bucket: \"my-snapshots\"\n    access_key_id: \"${AWS_ACCESS_KEY_ID}\"\n    secret_access_key: \"${AWS_SECRET_ACCESS_KEY}\"\n</code></pre>"}, {"location": "commands/auth-token/#web-interface-integration", "title": "Web Interface Integration", "text": "<ul> <li>Target management: Manage targets via web interface</li> <li>Snapshot browsing: Browse snapshots in cloud dashboard</li> <li>Team management: Manage team members and permissions</li> <li>Usage monitoring: Monitor snapshot usage and costs</li> </ul>"}, {"location": "commands/auth-token/#token-management", "title": "Token Management", "text": ""}, {"location": "commands/auth-token/#token-lifecycle", "title": "Token Lifecycle", "text": "<ol> <li>Generation: Create token in DBSnapper Cloud dashboard</li> <li>Configuration: Add token to DBSnapper configuration</li> <li>Usage: Token enables cloud features</li> <li>Rotation: Regularly rotate tokens for security</li> <li>Revocation: Revoke tokens when no longer needed</li> </ol>"}, {"location": "commands/auth-token/#best-practices", "title": "Best Practices", "text": "<pre><code># Regular token rotation (monthly/quarterly)\n# 1. Generate new token\n# 2. Test new token in non-production first\ndbsnapper auth token --config test.yml &lt;new_token&gt;\ndbsnapper --config test.yml config check\n\n# 3. Update production configuration\ndbsnapper auth token &lt;new_token&gt;\n\n# 4. Revoke old token\n</code></pre>"}, {"location": "commands/auth-token/#multiple-token-management", "title": "Multiple Token Management", "text": "<pre><code># Different tokens for different purposes\ndbsnapper auth token --config personal.yml &lt;personal_token&gt;\ndbsnapper auth token --config work.yml &lt;work_token&gt;\ndbsnapper auth token --config project-a.yml &lt;project_a_token&gt;\n\n# Use appropriate configuration for each context\nalias dbsnapper-personal='dbsnapper --config personal.yml'\nalias dbsnapper-work='dbsnapper --config work.yml'\n</code></pre>"}, {"location": "commands/auth-token/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/auth-token/#common-issues", "title": "Common Issues", "text": ""}, {"location": "commands/auth-token/#invalid-token-format", "title": "Invalid Token Format", "text": "<pre><code>Error: invalid token format\n</code></pre> <p>Solutions: 1. Verify token was copied completely from cloud dashboard 2. Check for extra spaces or characters 3. Regenerate token from cloud dashboard if needed</p>"}, {"location": "commands/auth-token/#token-authentication-failed", "title": "Token Authentication Failed", "text": "<pre><code>Error: authentication failed - invalid token\n</code></pre> <p>Solutions: 1. Verify token is current and not revoked 2. Check network connectivity to DBSnapper Cloud 3. Generate new token from cloud dashboard 4. Verify cloud service availability</p>"}, {"location": "commands/auth-token/#network-connection-issues", "title": "Network Connection Issues", "text": "<pre><code>Error: cannot connect to DBSnapper Cloud\n</code></pre> <p>Solutions: 1. Check internet connectivity 2. Verify DNS resolution 3. Check corporate firewall settings 4. Try different network connection 5. Contact IT about cloud service access</p>"}, {"location": "commands/auth-token/#configuration-file-issues", "title": "Configuration File Issues", "text": "<pre><code>Error: cannot write to configuration file\n</code></pre> <p>Solutions: <pre><code># Check file permissions\nls -la ~/.config/dbsnapper/dbsnapper.yml\n\n# Fix permissions if needed\nchmod 600 ~/.config/dbsnapper/dbsnapper.yml\n\n# Check directory permissions\nls -la ~/.config/dbsnapper/\n\n# Create directory if missing\nmkdir -p ~/.config/dbsnapper\n</code></pre></p>"}, {"location": "commands/auth-token/#diagnostic-commands", "title": "Diagnostic Commands", "text": "<pre><code># Test cloud connectivity\ndbsnapper config check\n\n# Verify token is configured\ngrep authtoken ~/.config/dbsnapper/dbsnapper.yml\n\n# Test cloud features\ndbsnapper targets | grep -E \"(cloud|share)\"\n\n# Check environment variable\necho $DBSNAPPER_AUTHTOKEN\n</code></pre>"}, {"location": "commands/auth-token/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "commands/auth-token/#token-protection", "title": "Token Protection", "text": "<ol> <li>File permissions: Restrict configuration file access</li> <li>Version control: Never commit tokens to version control</li> <li>Environment variables: Use environment variables in automation</li> <li>Regular rotation: Rotate tokens regularly</li> <li>Revocation: Revoke compromised tokens immediately</li> </ol>"}, {"location": "commands/auth-token/#access-control", "title": "Access Control", "text": "<ol> <li>Principle of least privilege: Use tokens with minimal required permissions</li> <li>Separate tokens: Use different tokens for different environments</li> <li>Team management: Regularly review team member access</li> <li>Audit logging: Monitor token usage and access patterns</li> </ol>"}, {"location": "commands/auth-token/#incident-response", "title": "Incident Response", "text": "<pre><code># If token is compromised:\n# 1. Revoke token immediately in cloud dashboard\n# 2. Generate new token\n# 3. Update all configurations\ndbsnapper auth token &lt;new_secure_token&gt;\n\n# 4. Verify security\ndbsnapper config check\n\n# 5. Review access logs in cloud dashboard\n</code></pre>"}, {"location": "commands/auth-token/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/auth-token/#cloud-enabled-commands", "title": "Cloud-Enabled Commands", "text": "<p>After token configuration, these commands gain cloud functionality:</p> <pre><code># Enhanced target listing (includes cloud and shared targets)\ndbsnapper targets\n\n# Cloud snapshot operations\ndbsnapper pull cloud-target-name\ndbsnapper build cloud-target-name  # May upload to cloud storage\n\n# Configuration checking includes cloud connectivity\ndbsnapper config check\n</code></pre>"}, {"location": "commands/auth-token/#workflow-integration", "title": "Workflow Integration", "text": "<pre><code># Complete cloud setup workflow\ndbsnapper config init              # Initialize configuration\ndbsnapper auth token &lt;your_token&gt;  # Configure authentication\ndbsnapper config check             # Verify cloud connectivity\ndbsnapper targets                  # List available targets\n</code></pre>"}, {"location": "commands/auth-token/#environment-variables", "title": "Environment Variables", "text": ""}, {"location": "commands/auth-token/#alternative-token-configuration", "title": "Alternative Token Configuration", "text": "<pre><code># Set token via environment variable\nexport DBSNAPPER_AUTHTOKEN=\"your_token_here\"\n\n# Verify environment variable is used\ndbsnapper config check | grep \"DBSnapper Cloud\"\n</code></pre>"}, {"location": "commands/auth-token/#cicd-integration", "title": "CI/CD Integration", "text": "<pre><code># GitHub Actions example\nenv:\n  DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_TOKEN }}\n\nsteps:\n  - name: Test DBSnapper Cloud connectivity\n    run: |\n      dbsnapper config check\n      dbsnapper targets | grep cloud\n</code></pre>"}, {"location": "commands/auth-token/#best-practices-summary", "title": "Best Practices Summary", "text": ""}, {"location": "commands/auth-token/#security-best-practices", "title": "Security Best Practices", "text": "<ol> <li>Secure storage: Use file permissions and encryption</li> <li>Environment separation: Different tokens per environment</li> <li>Regular rotation: Rotate tokens periodically</li> <li>Access monitoring: Monitor token usage and access</li> <li>Incident response: Have procedures for compromised tokens</li> </ol>"}, {"location": "commands/auth-token/#operational-best-practices", "title": "Operational Best Practices", "text": "<ol> <li>Team onboarding: Include token setup in onboarding process</li> <li>Documentation: Document token management procedures</li> <li>Testing: Test tokens in non-production environments first</li> <li>Backup access: Maintain alternative access methods</li> <li>Compliance: Ensure token usage meets compliance requirements</li> </ol> <p>Cloud Dashboard</p> <p>Generate and manage your API tokens through the DBSnapper Cloud dashboard at https://app.dbsnapper.com. The Get Started page provides your current API token.</p> <p>Environment Variables</p> <p>For automation and CI/CD, use the <code>DBSNAPPER_AUTHTOKEN</code> environment variable instead of storing tokens in configuration files.</p> <p>Token Security</p> <p>Protect your API tokens like passwords. Use appropriate file permissions, rotate regularly, and never commit tokens to version control.</p>"}, {"location": "commands/auth-token/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>auth</code> - Overview of authentication commands and cloud integration</li> <li><code>targets</code> - List targets (shows cloud and shared targets after authentication)</li> <li><code>pull</code> - Pull cloud snapshots (requires authentication)</li> <li><code>config check</code> - Verify cloud connectivity and authentication status</li> </ul>"}, {"location": "commands/auth-token/#see-also", "title": "See Also", "text": "<ul> <li>DBSnapper Cloud - Complete overview of cloud features</li> <li>Shared Targets - Team collaboration with shared targets</li> <li>Storage Profiles - Cloud storage configuration</li> <li>SSO Integration - Enterprise SSO setup and configuration</li> </ul>"}, {"location": "commands/auth/", "title": "Auth Command", "text": "<p>The <code>auth</code> command manages authentication with DBSnapper Cloud, enabling access to cloud features including shared targets, cloud storage, and team collaboration capabilities. This command serves as the gateway to DBSnapper's cloud ecosystem.</p>"}, {"location": "commands/auth/#overview", "title": "Overview", "text": "<p>The auth command provides cloud authentication management: - Cloud connectivity: Enables access to DBSnapper Cloud services - API token management: Secure storage and configuration of authentication tokens - Team collaboration: Unlocks shared targets and team snapshot sharing - Cloud storage: Enables cloud storage profiles for snapshot backup and sharing - SSO integration: Facilitates Okta SSO-based target sharing</p>"}, {"location": "commands/auth/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper auth [command]\n</code></pre>"}, {"location": "commands/auth/#available-subcommands", "title": "Available Subcommands", "text": ""}, {"location": "commands/auth/#authentication-commands", "title": "Authentication Commands", "text": "Command Purpose Use When <code>token</code> Add DBSnapper Cloud API token Setting up cloud integration"}, {"location": "commands/auth/#cloud-integration-benefits", "title": "Cloud Integration Benefits", "text": ""}, {"location": "commands/auth/#core-cloud-features", "title": "Core Cloud Features", "text": "<p>Once authenticated, DBSnapper Cloud provides:</p>"}, {"location": "commands/auth/#cloud-targets", "title": "Cloud Targets", "text": "<ul> <li>Centralized management: Manage targets via web interface</li> <li>Team synchronization: Targets automatically sync across team members</li> <li>Configuration backup: Target configurations stored securely in cloud</li> <li>Cross-device access: Access same targets from multiple machines</li> </ul>"}, {"location": "commands/auth/#cloud-storage-profiles", "title": "Cloud Storage Profiles", "text": "<ul> <li>S3-compatible storage: Support for AWS S3, Cloudflare R2, and other providers</li> <li>Snapshot backup: Automatic snapshot backup to cloud storage</li> <li>Cross-environment sharing: Share snapshots between development, staging, and production</li> <li>Disaster recovery: Cloud-based snapshot recovery capabilities</li> </ul>"}, {"location": "commands/auth/#shared-targets", "title": "Shared Targets", "text": "<ul> <li>Okta SSO integration: Share targets based on Okta group membership</li> <li>Secure sharing: Only sanitized snapshots are shared for security</li> <li>Access control: Fine-grained control over who can access shared data</li> <li>Audit trail: Track access to shared snapshots</li> </ul>"}, {"location": "commands/auth/#team-collaboration", "title": "Team Collaboration", "text": "<ul> <li>Centralized configuration: Share target configurations across team</li> <li>Collaborative development: Team members can access shared development data</li> <li>Environment consistency: Ensure consistent data across team environments</li> <li>Knowledge sharing: Share database schemas and structures</li> </ul>"}, {"location": "commands/auth/#authentication-workflow", "title": "Authentication Workflow", "text": ""}, {"location": "commands/auth/#initial-cloud-setup", "title": "Initial Cloud Setup", "text": "<pre><code># 1. Register account at DBSnapper Cloud\n# Visit https://app.dbsnapper.com and create account\n\n# 2. Get API token from cloud dashboard\n# Navigate to Get Started page in DBSnapper Cloud app\n\n# 3. Configure authentication\ndbsnapper auth token &lt;your_api_token&gt;\n\n# 4. Verify cloud connectivity\ndbsnapper targets  # Should now show cloud targets\n</code></pre>"}, {"location": "commands/auth/#cloud-features-verification", "title": "Cloud Features Verification", "text": "<pre><code># Test cloud connectivity\ndbsnapper targets | grep cloud\n\n# Check for shared targets\ndbsnapper targets | grep share\n\n# Verify cloud storage capabilities\ndbsnapper config check  # Should show \"DBSnapper Cloud connected\"\n</code></pre>"}, {"location": "commands/auth/#cloud-vs-local-mode", "title": "Cloud vs Local Mode", "text": ""}, {"location": "commands/auth/#local-mode-no-authentication", "title": "Local Mode (No Authentication)", "text": "<p>Available Features: - Local target management - Local snapshot storage - Local configuration only - Standalone operation</p> <p>Limitations: - No team collaboration - No cloud storage backup - No shared targets - No cross-device synchronization</p>"}, {"location": "commands/auth/#cloud-mode-authenticated", "title": "Cloud Mode (Authenticated)", "text": "<p>Additional Features: - All local mode features, plus: - Cloud target management - Cloud storage profiles - Shared targets via Okta SSO - Team collaboration - Cross-device synchronization - Web-based configuration management</p>"}, {"location": "commands/auth/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "commands/auth/#api-token-security", "title": "API Token Security", "text": "<ul> <li>Secure storage: Tokens stored encrypted in configuration file</li> <li>Limited scope: Tokens have limited permissions and scope</li> <li>Rotation: Tokens can be rotated regularly for security</li> <li>Revocation: Tokens can be revoked from cloud dashboard</li> </ul>"}, {"location": "commands/auth/#data-protection", "title": "Data Protection", "text": "<ul> <li>Encrypted transit: All cloud communications use TLS encryption</li> <li>Data sovereignty: Choose cloud storage regions for compliance</li> <li>Access controls: Fine-grained permissions and access controls</li> <li>Audit logging: Complete audit trail of cloud operations</li> </ul>"}, {"location": "commands/auth/#best-practices", "title": "Best Practices", "text": "<pre><code># Use environment variables for tokens in automation\nexport DBSNAPPER_AUTHTOKEN=\"your_token_here\"\n\n# Rotate tokens regularly\n# 1. Generate new token in cloud dashboard\n# 2. Update configuration: dbsnapper auth token &lt;new_token&gt;\n# 3. Revoke old token in cloud dashboard\n\n# Separate tokens for different environments\ndbsnapper auth token --config dev.yml &lt;dev_token&gt;\ndbsnapper auth token --config prod.yml &lt;prod_token&gt;\n</code></pre>"}, {"location": "commands/auth/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/auth/#basic-authentication-setup", "title": "Basic Authentication Setup", "text": "<pre><code># Set up cloud authentication\ndbsnapper auth token hNsdelkjcxgoiuwalkmcgoi...\n\n# Verify authentication worked\ndbsnapper targets  # Should include cloud targets\n</code></pre>"}, {"location": "commands/auth/#environment-specific-authentication", "title": "Environment-Specific Authentication", "text": "<pre><code># Set up different tokens for different environments\ndbsnapper auth token --config ~/.config/dbsnapper/dev.yml &lt;dev_token&gt;\ndbsnapper auth token --config ~/.config/dbsnapper/prod.yml &lt;prod_token&gt;\n\n# Use specific environment\ndbsnapper --config ~/.config/dbsnapper/dev.yml targets\n</code></pre>"}, {"location": "commands/auth/#team-setup-workflow", "title": "Team Setup Workflow", "text": "<pre><code># Team lead sets up cloud integration\ndbsnapper auth token &lt;team_lead_token&gt;\n\n# Create and share targets via cloud interface\n# (Done through DBSnapper Cloud web app)\n\n# Team members authenticate with their tokens\ndbsnapper auth token &lt;team_member_token&gt;\n\n# Verify access to shared targets\ndbsnapper targets | grep share\n</code></pre>"}, {"location": "commands/auth/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/auth/#individual-developer-setup", "title": "Individual Developer Setup", "text": "<pre><code># Personal cloud backup and multi-device access\ndbsnapper auth token &lt;personal_token&gt;\n\n# Configure cloud storage for snapshot backup\n# (Edit configuration to add storage profiles)\n\n# Access targets from multiple development machines\n</code></pre>"}, {"location": "commands/auth/#team-collaboration_1", "title": "Team Collaboration", "text": "<pre><code># Development team sharing sanitized production data\n# 1. Team lead configures production targets in cloud\n# 2. Team members authenticate with their cloud accounts\ndbsnapper auth token &lt;team_member_token&gt;\n\n# 3. Access shared sanitized snapshots\ndbsnapper targets | grep share\ndbsnapper pull shared-production-data\n</code></pre>"}, {"location": "commands/auth/#multi-environment-management", "title": "Multi-Environment Management", "text": "<pre><code># Manage development, staging, and production environments\n# Each environment uses separate cloud accounts/tokens\n\n# Development environment\ndbsnapper auth token --config dev.yml &lt;dev_token&gt;\n\n# Staging environment  \ndbsnapper auth token --config staging.yml &lt;staging_token&gt;\n\n# Production environment\ndbsnapper auth token --config prod.yml &lt;prod_token&gt;\n</code></pre>"}, {"location": "commands/auth/#cicd-pipeline-integration", "title": "CI/CD Pipeline Integration", "text": "<pre><code># Automated pipeline with cloud integration\nexport DBSNAPPER_AUTHTOKEN=\"$CI_DBSNAPPER_TOKEN\"\n\n# Use cloud targets in automated testing\ndbsnapper pull shared-test-data\ndbsnapper load ci-test-environment\n</code></pre>"}, {"location": "commands/auth/#disaster-recovery-setup", "title": "Disaster Recovery Setup", "text": "<pre><code># Set up cloud storage for disaster recovery\ndbsnapper auth token &lt;recovery_token&gt;\n\n# Configure cloud storage profiles for backup\n# (Edit configuration to add S3-compatible storage)\n\n# Regular backup to cloud\ndbsnapper build critical-database  # Automatically backs up to cloud\n</code></pre>"}, {"location": "commands/auth/#cloud-service-integration", "title": "Cloud Service Integration", "text": ""}, {"location": "commands/auth/#dbsnapper-cloud-web-interface", "title": "DBSnapper Cloud Web Interface", "text": "<ul> <li>Target management: Create and configure targets via web interface</li> <li>Storage profiles: Configure cloud storage providers</li> <li>Team management: Invite team members and manage permissions</li> <li>Snapshot browser: Browse and manage cloud snapshots</li> <li>Usage monitoring: Monitor snapshot usage and costs</li> </ul>"}, {"location": "commands/auth/#okta-sso-integration", "title": "Okta SSO Integration", "text": "<ul> <li>Enterprise authentication: Use existing Okta accounts</li> <li>Group-based sharing: Share targets based on Okta group membership</li> <li>Automatic provisioning: Automatic access provisioning/deprovisioning</li> <li>Compliance: Maintain compliance with enterprise security policies</li> </ul>"}, {"location": "commands/auth/#cloud-storage-providers", "title": "Cloud Storage Providers", "text": "<ul> <li>AWS S3: Native integration with Amazon S3</li> <li>Cloudflare R2: Cost-effective S3-compatible storage</li> <li>Custom S3: Any S3-compatible storage provider</li> <li>Multi-region: Support for multiple storage regions</li> </ul>"}, {"location": "commands/auth/#troubleshooting-authentication", "title": "Troubleshooting Authentication", "text": ""}, {"location": "commands/auth/#common-issues", "title": "Common Issues", "text": ""}, {"location": "commands/auth/#invalid-token", "title": "Invalid Token", "text": "<pre><code>Error: authentication failed - invalid token\n</code></pre> <p>Solutions: 1. Verify token is correct (check for typos) 2. Generate new token from cloud dashboard 3. Check token hasn't expired or been revoked</p>"}, {"location": "commands/auth/#network-connectivity", "title": "Network Connectivity", "text": "<pre><code>Error: cannot connect to DBSnapper Cloud\n</code></pre> <p>Solutions: 1. Check internet connectivity 2. Verify DNS resolution 3. Check corporate firewall settings 4. Try different network connection</p>"}, {"location": "commands/auth/#configuration-issues", "title": "Configuration Issues", "text": "<pre><code>Error: configuration file not found\n</code></pre> <p>Solutions: 1. Initialize configuration: <code>dbsnapper config init</code> 2. Specify config file path: <code>--config /path/to/config.yml</code> 3. Verify file permissions</p>"}, {"location": "commands/auth/#diagnostic-commands", "title": "Diagnostic Commands", "text": "<pre><code># Test cloud connectivity\ndbsnapper config check | grep \"DBSnapper Cloud\"\n\n# Verify authentication\ndbsnapper targets | grep -E \"(cloud|share)\"\n\n# Check configuration\ncat ~/.config/dbsnapper/dbsnapper.yml | grep authtoken\n</code></pre>"}, {"location": "commands/auth/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/auth/#cloud-dependent-operations", "title": "Cloud-Dependent Operations", "text": "<p>These commands require authentication for full functionality:</p> <pre><code># Access cloud and shared targets\ndbsnapper targets\n\n# Pull cloud snapshots\ndbsnapper pull cloud-target-name\n\n# Build with cloud storage backup\ndbsnapper build production-db  # Uploads to cloud if storage profile configured\n</code></pre>"}, {"location": "commands/auth/#configuration-integration", "title": "Configuration Integration", "text": "<pre><code># Authentication works with all config commands\ndbsnapper config check  # Shows cloud connectivity status\ndbsnapper config init   # Can be run before or after authentication\n</code></pre>"}, {"location": "commands/auth/#migration-from-local-to-cloud", "title": "Migration from Local to Cloud", "text": ""}, {"location": "commands/auth/#gradual-migration-strategy", "title": "Gradual Migration Strategy", "text": "<pre><code># 1. Start with local configuration\ndbsnapper config init\n\n# 2. Add cloud authentication\ndbsnapper auth token &lt;your_token&gt;\n\n# 3. Verify cloud connectivity\ndbsnapper config check\n\n# 4. Gradually migrate targets to cloud management\n# (Use web interface to recreate targets in cloud)\n\n# 5. Configure cloud storage for existing targets\n# (Edit configuration to add storage profiles)\n</code></pre>"}, {"location": "commands/auth/#hybrid-mode", "title": "Hybrid Mode", "text": "<p>You can operate in hybrid mode with both local and cloud targets:</p> <pre><code># List all targets (local, cloud, and shared)\ndbsnapper targets\n\n# Use local targets normally\ndbsnapper build local-dev-db\n\n# Use cloud targets with full cloud features\ndbsnapper build cloud-prod-db\ndbsnapper pull shared-team-data\n</code></pre>"}, {"location": "commands/auth/#best-practices_1", "title": "Best Practices", "text": ""}, {"location": "commands/auth/#authentication-management", "title": "Authentication Management", "text": "<ol> <li>Secure storage: Protect configuration files containing tokens</li> <li>Token rotation: Regularly rotate authentication tokens</li> <li>Environment separation: Use separate tokens for different environments</li> <li>Access monitoring: Monitor token usage and access patterns</li> </ol>"}, {"location": "commands/auth/#team-coordination", "title": "Team Coordination", "text": "<ol> <li>Onboarding: Include cloud setup in team onboarding process</li> <li>Documentation: Document cloud setup procedures for team</li> <li>Access review: Regularly review team member access</li> <li>Training: Train team on cloud features and security practices</li> </ol>"}, {"location": "commands/auth/#security-operations", "title": "Security Operations", "text": "<ol> <li>Principle of least privilege: Grant minimum required permissions</li> <li>Regular audits: Audit cloud access and usage regularly</li> <li>Incident response: Have procedures for token compromise</li> <li>Compliance: Ensure cloud usage meets compliance requirements</li> </ol> <p>Cloud Benefits</p> <p>Authenticating with DBSnapper Cloud unlocks powerful team collaboration features including shared targets, cloud storage backup, and cross-environment snapshot sharing.</p> <p>Token Security</p> <p>API tokens are stored encrypted in your configuration file. Protect your configuration file and rotate tokens regularly for security.</p> <p>Network Requirements</p> <p>Cloud features require internet connectivity. Some corporate networks may require firewall configuration to allow access to DBSnapper Cloud services.</p>"}, {"location": "commands/auth/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>auth token</code> - Configure API token for cloud authentication</li> <li><code>targets</code> - List targets (shows cloud and shared targets when authenticated)</li> <li><code>pull</code> - Pull cloud snapshots (requires authentication)</li> <li><code>config check</code> - Verify cloud connectivity and authentication</li> </ul>"}, {"location": "commands/auth/#see-also", "title": "See Also", "text": "<ul> <li>DBSnapper Cloud - Complete cloud features overview</li> <li>Shared Targets - Team collaboration with shared targets</li> <li>Storage Profiles - Cloud storage configuration</li> <li>SSO Integration - Okta SSO setup and configuration</li> </ul>"}, {"location": "commands/build/", "title": "Build Command", "text": "<p>The <code>build</code> command creates database snapshots from your configured targets. This is the primary command for capturing database state for later restoration, sharing, or sanitization.</p>"}, {"location": "commands/build/#overview", "title": "Overview", "text": "<p>The build command connects to a source database, creates a comprehensive snapshot of its structure and data, and stores it locally. If cloud storage is configured, the snapshot is also uploaded to your cloud storage provider for sharing and backup.</p>"}, {"location": "commands/build/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper build &lt;target_name&gt; [flags]\n</code></pre>"}, {"location": "commands/build/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> </ul>"}, {"location": "commands/build/#options", "title": "Options", "text": ""}, {"location": "commands/build/#command-specific-flags", "title": "Command-Specific Flags", "text": "<pre><code>  -h, --help   Show help for the build command\n</code></pre>"}, {"location": "commands/build/#global-flags", "title": "Global Flags", "text": "<pre><code>      --config string   Config file (default is ~/.config/dbsnapper/dbsnapper.yml)\n      --nocloud         Disable cloud mode to speed up operations by skipping cloud API calls\n</code></pre>"}, {"location": "commands/build/#target-configuration", "title": "Target Configuration", "text": "<p>Before using the build command, you need a target properly configured in your <code>dbsnapper.yml</code> file. A target defines the database connection and snapshot settings.</p>"}, {"location": "commands/build/#required-configuration", "title": "Required Configuration", "text": "<p>Minimal Target Setup: A target entry is broken into sections that reflect the operation being performed. A minimal build target configuration would specify source and destination URLs under the <code>snapshot</code> section.</p> <pre><code>targets:\n  my-target:\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/dbname\"\n      dst_url: \"postgresql://user:pass@dev:5432/dbname_copy\"\n</code></pre>"}, {"location": "commands/build/#complete-target-configuration", "title": "Complete Target Configuration", "text": "<p>Complete Target with All Options:</p> <pre><code>targets:\n  my-target:\n    name: \"My Database Target\" # Optional: Display name\n    cpus: 4 # Optional: CPU cores for PostgreSQL operations\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/dbname\"\n      dst_url: \"postgresql://user:pass@dev:5432/dbname_copy\"\n      schema_config: # Optional: PostgreSQL schema filtering\n        include_schemas: [\"public\", \"app_data\"]\n    storage_profile: \"s3-storage\" # Optional: Cloud storage\n    sanitize: # Optional: Sanitization configuration\n      dst_url: \"postgresql://user:pass@san:5432/dbname_sanitized\"\n      query_file: \"./sanitize.sql\"\n</code></pre>"}, {"location": "commands/build/#configuration-options", "title": "Configuration Options", "text": "Option Description Required Default <code>src_url</code> Source database connection string Yes - <code>dst_url</code> Destination database connection string Yes - <code>name</code> Human-readable target name No Target key <code>cpus</code> CPU cores for PostgreSQL operations No 1 <code>schema_config</code> PostgreSQL schema filtering options No Public schema only <code>storage_profile</code> Cloud storage profile name No None"}, {"location": "commands/build/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/build/#basic-build", "title": "Basic Build", "text": "<pre><code>dbsnapper build myapp-prod\n</code></pre>"}, {"location": "commands/build/#build-with-schema-filtering", "title": "Build with Schema Filtering", "text": "<pre><code># Builds only the schemas configured in schema_config\ndbsnapper build postgres-target\n</code></pre>"}, {"location": "commands/build/#how-it-works", "title": "How It Works", "text": "<ol> <li>Connection: DBSnapper connects to the source database using the <code>src_url</code> from your target configuration</li> <li>Schema Analysis: For PostgreSQL databases, analyzes available schemas and applies filtering based on <code>schema_config</code></li> <li>Snapshot Creation: Creates a complete backup including schema, data, indexes, and constraints</li> <li>Local Storage: Saves the snapshot as a compressed ZIP file in your working directory</li> <li>Cloud Upload: If a storage profile is configured, uploads the snapshot to your cloud storage</li> <li>Metadata: Records snapshot information for future reference</li> </ol>"}, {"location": "commands/build/#output-files", "title": "Output Files", "text": ""}, {"location": "commands/build/#local-snapshots", "title": "Local Snapshots", "text": "<ul> <li>Format: <code>&lt;timestamp&gt;_&lt;target_name&gt;.zip</code></li> <li>Location: Working directory specified in configuration file</li> <li>Example: <code>1752444532_dvdrental.zip</code></li> </ul>"}, {"location": "commands/build/#sanitized-local-snapshots", "title": "Sanitized Local Snapshots", "text": "<ul> <li>Format: <code>&lt;timestamp&gt;_&lt;target_name&gt;.san.zip</code></li> <li>Location: Working directory specified in configuration file</li> <li>Example: <code>1752444532_dvdrental.san.zip</code></li> </ul>"}, {"location": "commands/build/#cloud-snapshots", "title": "Cloud Snapshots", "text": "<ul> <li>Name: <code>{timestamp}_{target_name}</code> (user-visible identifier)</li> <li>Storage: Unique UUID filename in your cloud bucket</li> <li>Access: Available through DBSnapper Cloud interface</li> </ul>"}, {"location": "commands/build/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>The build command supports advanced schema filtering for PostgreSQL databases, allowing you to control exactly which schemas are included in your snapshots.</p>"}, {"location": "commands/build/#schema-configuration-options", "title": "Schema Configuration Options", "text": "Configuration Behavior Use Case No <code>schema_config</code> Builds <code>public</code> schema only Default behavior for simple applications <code>use_default_schema: true</code> Builds default schema (<code>public</code>) Explicit default schema only <code>use_default_schema: false</code> Builds all available schemas Multi-schema applications <code>include_schemas: [\"public\", \"app\"]</code> Builds only specified schemas Selective schema inclusion <code>exclude_schemas: [\"temp\", \"logs\"]</code> Builds all schemas except excluded Exclude temporary/log schemas"}, {"location": "commands/build/#schema-configuration-examples", "title": "Schema Configuration Examples", "text": "<p>Include Specific Schemas:</p> <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        include_schemas: [\"public\", \"app_data\", \"reports\"]\n</code></pre> <p>Exclude Temporary Schemas:</p> <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        exclude_schemas: [\"temp_data\", \"debug_logs\", \"analytics\"]\n</code></pre> <p>Build All Schemas:</p> <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        use_default_schema: false # Include all available schemas\n</code></pre> <p>Advanced Schema-Table Filtering:</p> <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n        schema_table_filters:\n          public: [\"users\", \"orders\", \"products\"]\n          app_data: [\"analytics\", \"reports\"]\n</code></pre>"}, {"location": "commands/build/#important-notes", "title": "Important Notes", "text": "<ul> <li>PostgreSQL Only: Schema filtering only works with PostgreSQL databases. MySQL targets ignore <code>schema_config</code></li> <li>Dynamic Analysis: The build process analyzes available schemas in the source database at runtime</li> <li>Existing Schemas Only: Only schemas that actually exist in the source database will be included</li> <li>Load Consistency: The load command will respect the schemas that were captured during build</li> <li>Validation: Schemas cannot appear in both <code>include_schemas</code> and <code>exclude_schemas</code> lists</li> </ul> <p>MySQL Limitation</p> <p>Schema filtering is not supported for MySQL databases. MySQL targets will use standard mysqldump without schema filtering, regardless of <code>schema_config</code> settings.</p>"}, {"location": "commands/build/#cpu-configuration-postgresql-only", "title": "CPU Configuration (PostgreSQL Only)", "text": "<p>The build command supports configuring the number of CPU cores used for PostgreSQL dump operations via the <code>-j</code> flag. This feature allows you to optimize performance based on your system resources.</p>"}, {"location": "commands/build/#cpu-configuration-priority", "title": "CPU Configuration Priority", "text": "<p>The CPU configuration follows a three-level priority system:</p> <ol> <li>Target-level CPU configuration (highest priority)</li> <li>Global default CPU configuration (medium priority)</li> <li>Default value (lowest priority): Defaults to 1 CPU</li> </ol>"}, {"location": "commands/build/#cpu-configuration-examples", "title": "CPU Configuration Examples", "text": "<p>Target-Level CPU Configuration:</p> <pre><code>targets:\n  myapp-prod:\n    cpus: 4 # Use 4 CPUs for this target\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n</code></pre> <p>Global Default CPU Configuration:</p> <pre><code>defaults:\n  cpus: 2 # Default to 2 CPUs for all targets\n\ntargets:\n  myapp-prod:\n    # Will use 2 CPUs from global default\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n</code></pre>"}, {"location": "commands/build/#cpu-validation", "title": "CPU Validation", "text": "<ul> <li>The system automatically detects the maximum available CPUs on your machine</li> <li>If configured CPU count exceeds available CPUs, a warning is displayed and the count is capped</li> <li>Operations continue with the adjusted CPU count</li> </ul> <p>Example Warning:</p> <pre><code>CPU configuration warning: requested CPU count (16) exceeds maximum available CPUs (8), using 8 CPUs\n</code></pre> <p>Database Engine Support</p> <p>CPU configuration is currently supported for PostgreSQL only. MySQL operations ignore CPU configuration settings.</p>"}, {"location": "commands/build/#storage-profiles", "title": "Storage Profiles", "text": "<p>For cloud storage, configure a storage profile:</p> <pre><code>storage_profiles:\n  aws-s3:\n    type: \"s3\"\n    region: \"us-west-2\"\n    bucket: \"my-snapshots\"\n    # Additional S3 configuration...\n</code></pre>"}, {"location": "commands/build/#best-practices", "title": "Best Practices", "text": ""}, {"location": "commands/build/#timing", "title": "Timing", "text": "<ul> <li>Production Databases: Schedule builds during low-traffic periods</li> <li>Development: Build regularly to maintain fresh snapshots</li> <li>Testing: Build before major changes for quick rollback</li> </ul>"}, {"location": "commands/build/#security", "title": "Security", "text": "<ul> <li>Credentials: Use environment variables or secure credential management</li> <li>Network: Ensure secure network connections to production databases</li> <li>Access: Limit build permissions to authorized personnel</li> </ul>"}, {"location": "commands/build/#performance", "title": "Performance", "text": "<ul> <li>Large Databases: Monitor build times and storage requirements</li> <li>Network: Consider bandwidth impact for cloud uploads</li> <li>Resources: Ensure adequate disk space for local snapshots</li> </ul>"}, {"location": "commands/build/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/build/#common-issues", "title": "Common Issues", "text": "<p>Connection Failed</p> <pre><code>Error: failed to connect to database\n</code></pre> <ul> <li>Verify <code>src_url</code> connection string</li> <li>Check network connectivity</li> <li>Confirm database credentials</li> </ul> <p>Schema Configuration Errors</p> <pre><code>Error: schema 'analytics' cannot be in both include and exclude lists\n</code></pre> <ul> <li>Remove the schema from one of the lists</li> <li>Use either <code>include_schemas</code> OR <code>exclude_schemas</code> for any given schema</li> </ul> <p>Schema Not Found Warnings</p> <pre><code>Warning: schema 'non_existent' specified in include_schemas but not found in database\n</code></pre> <ul> <li>Verify schema names are correct</li> <li>Check that schemas exist in the source database</li> <li>Review available schemas in your database</li> </ul> <p>Insufficient Storage</p> <pre><code>Error: no space left on device\n</code></pre> <ul> <li>Free up disk space in working directory</li> <li>Consider cleanup of old snapshots</li> </ul> <p>Cloud Upload Failed</p> <pre><code>Error: failed to upload to cloud storage\n</code></pre> <ul> <li>Verify storage profile configuration</li> <li>Check cloud credentials and permissions</li> <li>Confirm bucket exists and is accessible</li> </ul>"}, {"location": "commands/build/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>load</code> - Load snapshots into databases</li> <li><code>sanitize</code> - Create sanitized versions of snapshots</li> <li><code>targets</code> - List available targets</li> <li><code>pull</code> - Download cloud snapshots</li> </ul>"}, {"location": "commands/build/#see-also", "title": "See Also", "text": "<ul> <li>Snapshot Configuration</li> <li>Cloud Storage Setup</li> <li>Target Configuration</li> </ul>"}, {"location": "commands/config-check/", "title": "Config Check Command", "text": "<p>The <code>config check</code> command validates your DBSnapper configuration and system dependencies, ensuring all required components are properly installed and configured. This comprehensive health check tool serves as both a diagnostic utility and an automated setup assistant.</p>"}, {"location": "commands/config-check/#overview", "title": "Overview", "text": "<p>The config check command performs a thorough validation of your DBSnapper environment: - Configuration file validation: Verifies syntax and structure - Database tool detection: Confirms required client tools are available - Docker integration: Validates Docker setup and required images - Cloud connectivity: Tests DBSnapper Cloud authentication and connection - Auto-discovery: Optionally finds and configures database tools automatically</p>"}, {"location": "commands/config-check/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper config check [flags]\n</code></pre>"}, {"location": "commands/config-check/#arguments", "title": "Arguments", "text": "<p>No arguments required - performs comprehensive system check by default.</p>"}, {"location": "commands/config-check/#options", "title": "Options", "text": "<pre><code>      --auto-discover   Automatically discover database client tools\n  -h, --help            help for check\n      --save            Save discovered tool paths to configuration file (use with --auto-discover)\n</code></pre>"}, {"location": "commands/config-check/#validation-components", "title": "Validation Components", "text": ""}, {"location": "commands/config-check/#configuration-file-validation", "title": "Configuration File Validation", "text": "<ul> <li>File existence: Confirms configuration file is present</li> <li>YAML syntax: Validates proper YAML structure</li> <li>Required sections: Checks for essential configuration sections</li> <li>Loading verification: Ensures configuration loads without errors</li> </ul>"}, {"location": "commands/config-check/#database-engine-validation", "title": "Database Engine Validation", "text": ""}, {"location": "commands/config-check/#postgresql-local-engine-pglocal", "title": "PostgreSQL Local Engine (pglocal)", "text": "<ul> <li><code>psql</code> client: Command-line PostgreSQL client for testing connections</li> <li><code>pg_dump</code> tool: Required for creating database snapshots</li> <li><code>pg_restore</code> tool: Required for loading database snapshots</li> <li>Version compatibility: Ensures tool versions are supported</li> </ul>"}, {"location": "commands/config-check/#mysql-local-engine-mylocal", "title": "MySQL Local Engine (mylocal)", "text": "<ul> <li><code>mysql</code> client: Command-line MySQL client for testing connections</li> <li><code>mysqldump</code> tool: Required for creating database snapshots</li> <li>Version compatibility: Ensures tool versions are supported</li> </ul>"}, {"location": "commands/config-check/#postgresql-docker-engine-pgdocker", "title": "PostgreSQL Docker Engine (pgdocker)", "text": "<ul> <li>Docker connectivity: Verifies Docker daemon is running and accessible</li> <li>Image configuration: Checks <code>docker.images.postgres</code> setting</li> <li>Image availability: Confirms PostgreSQL Docker image is present</li> <li>Container capability: Validates Docker container creation permissions</li> </ul>"}, {"location": "commands/config-check/#mysql-docker-engine-mydocker", "title": "MySQL Docker Engine (mydocker)", "text": "<ul> <li>Docker connectivity: Verifies Docker daemon is running and accessible</li> <li>Image configuration: Checks <code>docker.images.mysql</code> setting</li> <li>Image availability: Confirms MySQL Docker image is present</li> <li>Container capability: Validates Docker container creation permissions</li> </ul>"}, {"location": "commands/config-check/#cloud-integration-validation", "title": "Cloud Integration Validation", "text": "<ul> <li>DBSnapper Cloud connectivity: Tests connection to cloud services</li> <li>Authentication: Validates API token if configured</li> <li>Service availability: Confirms cloud services are accessible</li> </ul>"}, {"location": "commands/config-check/#auto-discovery-feature", "title": "Auto-Discovery Feature", "text": ""}, {"location": "commands/config-check/#tool-discovery-process", "title": "Tool Discovery Process", "text": "<p>The <code>--auto-discover</code> flag enables automatic detection of database tools:</p> <ol> <li>Search locations: Scans common installation paths</li> <li>Tool validation: Tests each found tool for compatibility</li> <li>Version checking: Ensures tool versions meet requirements</li> <li>Path resolution: Determines optimal tool paths</li> </ol>"}, {"location": "commands/config-check/#discovery-search-paths", "title": "Discovery Search Paths", "text": ""}, {"location": "commands/config-check/#postgresql-tools", "title": "PostgreSQL Tools", "text": "<ul> <li>Postgres.app: <code>/Applications/Postgres.app/Contents/Versions/*/bin/</code></li> <li>Homebrew: <code>/opt/homebrew/bin/</code>, <code>/usr/local/bin/</code></li> <li>System paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code></li> <li>Custom paths: Checks <code>PATH</code> environment variable</li> </ul>"}, {"location": "commands/config-check/#mysql-tools", "title": "MySQL Tools", "text": "<ul> <li>Homebrew: <code>/opt/homebrew/opt/mysql-client/bin/</code>, <code>/opt/homebrew/bin/</code></li> <li>System paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code>, <code>/usr/local/mysql/bin/</code></li> <li>Custom paths: Checks <code>PATH</code> environment variable</li> </ul>"}, {"location": "commands/config-check/#saving-discovered-tools", "title": "Saving Discovered Tools", "text": "<p>The <code>--save</code> flag (used with <code>--auto-discover</code>) updates your configuration:</p> <pre><code># Discover tools and save to configuration\ndbsnapper config check --auto-discover --save\n</code></pre> <p>This updates the <code>database_tools</code> section in your configuration file:</p> <pre><code>database_tools:\n  auto_detect: true\n  postgresql:\n    pg_dump: \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\"\n    pg_restore: \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\"\n    psql: \"/Applications/Postgres.app/Contents/Versions/17/bin/psql\"\n  mysql:\n    mysql: \"/opt/homebrew/opt/mysql-client/bin/mysql\"\n    mysqldump: \"/opt/homebrew/opt/mysql-client/bin/mysqldump\"\n</code></pre>"}, {"location": "commands/config-check/#check-output-and-status-indicators", "title": "Check Output and Status Indicators", "text": ""}, {"location": "commands/config-check/#status-icons", "title": "Status Icons", "text": "<ul> <li>\u2705 Success: Component is properly configured and available</li> <li>\u274c Error: Component has issues that need attention</li> <li>\u26a0\ufe0f Warning: Component works but has non-critical issues</li> <li>\ud83d\udd35 Info: Informational status about component checking</li> <li>\ud83d\udd0d Discovery: Auto-discovery process indicators</li> </ul>"}, {"location": "commands/config-check/#output-sections", "title": "Output Sections", "text": ""}, {"location": "commands/config-check/#configuration-file-status", "title": "Configuration File Status", "text": "<pre><code>\u2705 Config file ( /path/to/dbsnapper.yml ) found and loaded\n</code></pre>"}, {"location": "commands/config-check/#database-engine-status", "title": "Database Engine Status", "text": "<pre><code>\ud83d\udd35 Postgres Local Engine (pglocal)\n  \u2705 psql found at /Applications/Postgres.app/Contents/Versions/17/bin/psql\n  \u2705 pg_dump found at /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\n  \u2705 pg_restore found at /Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\n</code></pre>"}, {"location": "commands/config-check/#auto-discovery-results", "title": "Auto-Discovery Results", "text": "<pre><code>\ud83d\udd0d Auto-discovering database client tools...\n  \u2705 Database tools discovery completed\n  \ud83d\udccb Discovered tools:\n    pg_dump: \u2705 /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\n    mysql: \u2705 /opt/homebrew/opt/mysql-client/bin/mysql\n  \ud83d\udcbe Saving discovered tools to configuration file...\n  \u2705 Configuration saved to /path/to/dbsnapper.yml\n</code></pre>"}, {"location": "commands/config-check/#final-status", "title": "Final Status", "text": "<pre><code>\u2705 All supported database engines configured\n\u2705 DBSnapper Cloud connected\n\u2705 Configuration OK\n</code></pre>"}, {"location": "commands/config-check/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/config-check/#basic-health-check", "title": "Basic Health Check", "text": "<pre><code># Comprehensive system validation\ndbsnapper config check\n</code></pre> <p>Output: <pre><code>Checking DBSnapper Configuration\n  \u2705 Config file ( ~/.config/dbsnapper/dbsnapper.yml ) found and loaded\n  \ud83d\udd35 Postgres Local Engine (pglocal)\n    \u2705 psql found at /usr/local/bin/psql\n    \u2705 pg_dump found at /usr/local/bin/pg_dump\n    \u2705 pg_restore found at /usr/local/bin/pg_restore\n  \ud83d\udd35 MySQL Local Engine (mylocal)\n    \u2705 mysqldump found at /usr/local/bin/mysqldump\n    \u2705 mysql found at /usr/local/bin/mysql\n  \ud83d\udd35 Postgres Docker Engine (pgdocker)\n    \u2705 Docker client connected\n    \u2705 docker.images.postgres set in config file\n      \u2705 Found Docker image: postgres:17\n  \ud83d\udd35 Mysql Docker Engine (mydocker)\n    \u2705 Docker client connected\n    \u2705 docker.images.mysql set in config file\n      \u2705 Found Docker image: mysql:9\n  \u2705 All supported database engines configured\n  \u2705 DBSnapper Cloud connected\n\n  \u2705 Configuration OK\n</code></pre></p>"}, {"location": "commands/config-check/#auto-discovery-and-configuration-update", "title": "Auto-Discovery and Configuration Update", "text": "<pre><code># Discover tools and save to configuration\ndbsnapper config check --auto-discover --save\n</code></pre>"}, {"location": "commands/config-check/#discovery-without-saving", "title": "Discovery Without Saving", "text": "<pre><code># Discover tools but don't update configuration\ndbsnapper config check --auto-discover\n</code></pre>"}, {"location": "commands/config-check/#custom-configuration-file", "title": "Custom Configuration File", "text": "<pre><code># Check specific configuration file\ndbsnapper config check --config /path/to/custom/config.yml\n</code></pre>"}, {"location": "commands/config-check/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/config-check/#initial-setup-verification", "title": "Initial Setup Verification", "text": "<pre><code># After running `dbsnapper config init`\ndbsnapper config init\ndbsnapper config check\n\n# Verify everything is working before first use\n</code></pre>"}, {"location": "commands/config-check/#regular-health-monitoring", "title": "Regular Health Monitoring", "text": "<pre><code># Monthly system health check\ndbsnapper config check\n\n# Include in system monitoring scripts\nif dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n    echo \"DBSnapper configuration healthy\"\nelse\n    echo \"DBSnapper configuration issues detected\"\n    dbsnapper config check  # Show details\nfi\n</code></pre>"}, {"location": "commands/config-check/#after-software-updates", "title": "After Software Updates", "text": "<pre><code># After updating PostgreSQL or MySQL\ndbsnapper config check --auto-discover --save\n\n# Verify tools still work after OS updates\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-check/#troubleshooting-workflow", "title": "Troubleshooting Workflow", "text": "<pre><code># Diagnose configuration issues\ndbsnapper config check\n\n# Try to fix automatically\ndbsnapper config check --auto-discover --save\n\n# Verify fixes worked\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-check/#cicd-pipeline-validation", "title": "CI/CD Pipeline Validation", "text": "<pre><code># Validate build environment\ndbsnapper config check\n\n# Fail CI if configuration is invalid\ndbsnapper config check || exit 1\n\n# Auto-configure in containerized builds\ndbsnapper config check --auto-discover --save\n</code></pre>"}, {"location": "commands/config-check/#team-environment-standardization", "title": "Team Environment Standardization", "text": "<pre><code># Verify team member setup\ndbsnapper config check\n\n# Standardize tool paths across team\ndbsnapper config check --auto-discover --save\n</code></pre>"}, {"location": "commands/config-check/#error-detection-and-resolution", "title": "Error Detection and Resolution", "text": ""}, {"location": "commands/config-check/#common-issues-and-solutions", "title": "Common Issues and Solutions", "text": ""}, {"location": "commands/config-check/#configuration-file-issues", "title": "Configuration File Issues", "text": "<pre><code># Error: Config file not found\n\u274c Config file not found\n\n# Solution: Initialize configuration\ndbsnapper config init\n</code></pre> <pre><code># Error: Invalid YAML syntax\n\u274c Config file syntax error\n\n# Solution: Validate and fix syntax\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-check/#database-tool-issues", "title": "Database Tool Issues", "text": "<pre><code># Error: PostgreSQL tools not found\n\u274c psql not found\n\u274c pg_dump not found\n\n# Solution: Install PostgreSQL client tools and auto-discover\nbrew install postgresql  # or appropriate package manager\ndbsnapper config check --auto-discover --save\n</code></pre> <pre><code># Error: MySQL tools not found\n\u274c mysql not found\n\u274c mysqldump not found\n\n# Solution: Install MySQL client tools and auto-discover\nbrew install mysql-client  # or appropriate package manager\ndbsnapper config check --auto-discover --save\n</code></pre>"}, {"location": "commands/config-check/#docker-issues", "title": "Docker Issues", "text": "<pre><code># Error: Docker not available\n\u274c Docker client not connected\n\n# Solution: Start Docker service\n# macOS: Start Docker Desktop\n# Linux: sudo systemctl start docker\n</code></pre> <pre><code># Error: Docker images missing\n\u274c Docker image postgres:17 not found\n\n# Solution: Pull required images\ndocker pull postgres:17\ndocker pull mysql:9\n</code></pre>"}, {"location": "commands/config-check/#cloud-connectivity-issues", "title": "Cloud Connectivity Issues", "text": "<pre><code># Error: Cloud connection failed\n\u274c DBSnapper Cloud connection failed\n\n# Solution: Check authentication and network\ndbsnapper auth token &lt;your_token&gt;\n# Check internet connectivity and firewall settings\n</code></pre>"}, {"location": "commands/config-check/#automated-issue-resolution", "title": "Automated Issue Resolution", "text": "<p>The <code>--auto-discover --save</code> combination can automatically resolve many tool-related issues:</p> <pre><code># Comprehensive auto-fix attempt\ndbsnapper config check --auto-discover --save\n\n# Manual verification after auto-fix\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-check/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/config-check/#pre-operation-validation", "title": "Pre-Operation Validation", "text": "<pre><code># Verify configuration before critical operations\ndbsnapper config check &amp;&amp; dbsnapper build production-db\ndbsnapper config check &amp;&amp; dbsnapper sanitize sensitive-data\n</code></pre>"}, {"location": "commands/config-check/#automated-workflows", "title": "Automated Workflows", "text": "<pre><code>#!/bin/bash\n# Automated backup script with health check\nif ! dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Configuration issues detected, attempting auto-fix...\"\n    dbsnapper config check --auto-discover --save\n    if ! dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n        echo \"Configuration issues persist, manual intervention required\"\n        exit 1\n    fi\nfi\n\n# Proceed with backup operations\ndbsnapper build production-db\n</code></pre>"}, {"location": "commands/config-check/#system-monitoring-integration", "title": "System Monitoring Integration", "text": "<pre><code># Nagios/monitoring check script\n#!/bin/bash\nif dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n    echo \"OK - DBSnapper configuration healthy\"\n    exit 0\nelse\n    echo \"CRITICAL - DBSnapper configuration issues\"\n    exit 2\nfi\n</code></pre>"}, {"location": "commands/config-check/#performance-and-optimization", "title": "Performance and Optimization", "text": ""}, {"location": "commands/config-check/#check-performance", "title": "Check Performance", "text": "<ul> <li>Fast validation: Basic checks complete in seconds</li> <li>Docker checks: May be slower if Docker images need pulling</li> <li>Cloud checks: Depend on network connectivity</li> <li>Auto-discovery: May take longer on first run</li> </ul>"}, {"location": "commands/config-check/#optimization-strategies", "title": "Optimization Strategies", "text": "<pre><code># Skip cloud checks for faster validation\ndbsnapper config check --nocloud\n\n# Cache Docker images for faster checks\ndocker pull postgres:17 mysql:9\n\n# Use in monitoring with appropriate timeouts\ntimeout 30 dbsnapper config check\n</code></pre>"}, {"location": "commands/config-check/#best-practices", "title": "Best Practices", "text": ""}, {"location": "commands/config-check/#regular-health-checks", "title": "Regular Health Checks", "text": "<ol> <li>Monthly validation: Include in regular maintenance routine</li> <li>After updates: Run after software or OS updates</li> <li>Before critical operations: Validate before important snapshots</li> <li>Team onboarding: Include in new team member setup</li> </ol>"}, {"location": "commands/config-check/#automation-integration", "title": "Automation Integration", "text": "<ol> <li>CI/CD validation: Include in build pipelines</li> <li>Monitoring: Integrate with system monitoring</li> <li>Auto-healing: Use auto-discover for self-healing systems</li> <li>Documentation: Document configuration standards</li> </ol>"}, {"location": "commands/config-check/#error-handling", "title": "Error Handling", "text": "<ol> <li>Graceful failures: Handle check failures appropriately</li> <li>Notification: Alert on persistent configuration issues</li> <li>Recovery procedures: Document resolution steps</li> <li>Escalation: Define when manual intervention is needed</li> </ol> <p>Regular Validation</p> <p>Include <code>dbsnapper config check</code> in your regular maintenance routine to catch configuration issues before they impact operations.</p> <p>Auto-Discovery</p> <p>Use <code>--auto-discover --save</code> after installing or updating database client tools to automatically update your configuration.</p> <p>Docker Dependencies</p> <p>Docker-based engines require Docker to be running and have the appropriate images pulled. The check will help identify missing Docker images.</p>"}, {"location": "commands/config-check/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>config init</code> - Initialize DBSnapper configuration  </li> <li><code>config discover</code> - Dedicated tool discovery command</li> <li><code>config validate</code> - Validate configuration file syntax</li> <li><code>config</code> - Overview of all configuration commands</li> </ul>"}, {"location": "commands/config-check/#see-also", "title": "See Also", "text": "<ul> <li>Configuration Reference - Complete configuration options</li> <li>Database Engines - Supported databases and setup</li> <li>Installation Guide - DBSnapper installation instructions</li> </ul>"}, {"location": "commands/config-discover/", "title": "Config Discover Command", "text": "<p>The <code>config discover</code> command automatically finds and validates database client tools on your system. This intelligent discovery tool scans common installation paths, validates tool compatibility, and optionally updates your DBSnapper configuration with the discovered tools.</p>"}, {"location": "commands/config-discover/#overview", "title": "Overview", "text": "<p>The config discover command provides automated database tool management: - Automatic detection: Scans system for PostgreSQL and MySQL client tools - Path validation: Verifies tools are executable and compatible - Version checking: Ensures discovered tools meet DBSnapper requirements - Configuration integration: Optionally updates configuration with discovered paths - Multi-format output: Supports both human-readable and JSON output formats</p>"}, {"location": "commands/config-discover/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper config discover [flags]\n</code></pre>"}, {"location": "commands/config-discover/#arguments", "title": "Arguments", "text": "<p>No arguments required - performs comprehensive tool discovery by default.</p>"}, {"location": "commands/config-discover/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for discover\n      --json   Output results as JSON\n      --save   Save discovered tool paths to configuration file\n</code></pre>"}, {"location": "commands/config-discover/#discovery-process", "title": "Discovery Process", "text": ""}, {"location": "commands/config-discover/#search-strategy", "title": "Search Strategy", "text": "<p>The discover command uses a comprehensive search strategy to find database tools:</p> <ol> <li>Priority-based search: Checks high-priority locations first</li> <li>Path validation: Tests each discovered tool for executability</li> <li>Version compatibility: Verifies tool versions are supported</li> <li>Conflict resolution: Chooses optimal tools when multiple versions found</li> </ol>"}, {"location": "commands/config-discover/#search-locations", "title": "Search Locations", "text": ""}, {"location": "commands/config-discover/#postgresql-tools", "title": "PostgreSQL Tools", "text": "<p>The discovery process searches for <code>psql</code>, <code>pg_dump</code>, and <code>pg_restore</code> in:</p> <p>macOS: - Postgres.app: <code>/Applications/Postgres.app/Contents/Versions/*/bin/</code> - Homebrew: <code>/opt/homebrew/bin/</code>, <code>/usr/local/bin/</code> - MacPorts: <code>/opt/local/bin/</code> - System paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code></p> <p>Linux: - Package manager paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code> - PostgreSQL official: <code>/usr/pgsql-*/bin/</code> - Custom installations: User-defined paths in <code>$PATH</code></p> <p>Windows: - Program Files: <code>C:\\Program Files\\PostgreSQL\\*\\bin\\</code> - System paths: Paths defined in <code>%PATH%</code></p>"}, {"location": "commands/config-discover/#mysql-tools", "title": "MySQL Tools", "text": "<p>The discovery process searches for <code>mysql</code> and <code>mysqldump</code> in:</p> <p>macOS: - Homebrew: <code>/opt/homebrew/opt/mysql-client/bin/</code>, <code>/opt/homebrew/bin/</code> - MySQL official: <code>/usr/local/mysql/bin/</code> - System paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code></p> <p>Linux: - Package manager paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code> - MySQL official: <code>/usr/local/mysql/bin/</code> - MariaDB: <code>/usr/local/mariadb/bin/</code></p> <p>Windows: - Program Files: <code>C:\\Program Files\\MySQL\\MySQL Server *\\bin\\</code> - System paths: Paths defined in <code>%PATH%</code></p>"}, {"location": "commands/config-discover/#tool-validation", "title": "Tool Validation", "text": "<p>Each discovered tool undergoes validation:</p> <ol> <li>Executable test: Verifies the tool can be executed</li> <li>Version check: Ensures version compatibility with DBSnapper</li> <li>Functionality test: Basic functionality verification</li> <li>Permission check: Confirms appropriate execution permissions</li> </ol>"}, {"location": "commands/config-discover/#output-formats", "title": "Output Formats", "text": ""}, {"location": "commands/config-discover/#human-readable-output-default", "title": "Human-Readable Output (Default)", "text": "<pre><code>dbsnapper config discover\n</code></pre> <p>Output: <pre><code>DBSnapper v2.7.0-dev - Database Tools Discovery\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\u2192 Database Tools Discovery\n  \u2705 pg_dump: /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\n  \u2705 pg_restore: /Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\n  \u2705 psql: /Applications/Postgres.app/Contents/Versions/17/bin/psql\n  \u2705 mysql: /opt/homebrew/opt/mysql-client/bin/mysql\n  \u2705 mysqldump: /opt/homebrew/opt/mysql-client/bin/mysqldump\n\n\ud83d\udca1 To save this configuration, run:\n   dbsnapper config discover --save\n\n  \u2713 Database Tools Discovery completed\n</code></pre></p>"}, {"location": "commands/config-discover/#json-output", "title": "JSON Output", "text": "<pre><code>dbsnapper config discover --json\n</code></pre> <p>Output: <pre><code>{\n  \"success\": true,\n  \"discovered_tools\": {\n    \"auto_detect\": true,\n    \"postgresql\": {\n      \"pg_dump\": \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\",\n      \"pg_restore\": \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\", \n      \"psql\": \"/Applications/Postgres.app/Contents/Versions/17/bin/psql\"\n    },\n    \"mysql\": {\n      \"mysql\": \"/opt/homebrew/opt/mysql-client/bin/mysql\",\n      \"mysqldump\": \"/opt/homebrew/opt/mysql-client/bin/mysqldump\"\n    }\n  },\n  \"validation_results\": {\n    \"mysql\": \"\u2705 /opt/homebrew/opt/mysql-client/bin/mysql\",\n    \"mysqldump\": \"\u2705 /opt/homebrew/opt/mysql-client/bin/mysqldump\",\n    \"pg_dump\": \"\u2705 /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\", \n    \"pg_restore\": \"\u2705 /Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\",\n    \"psql\": \"\u2705 /Applications/Postgres.app/Contents/Versions/17/bin/psql\"\n  },\n  \"message\": \"Database tools discovery completed successfully\"\n}\n</code></pre></p>"}, {"location": "commands/config-discover/#configuration-integration", "title": "Configuration Integration", "text": ""}, {"location": "commands/config-discover/#save-discovered-tools", "title": "Save Discovered Tools", "text": "<p>The <code>--save</code> flag updates your configuration file with discovered tools:</p> <pre><code>dbsnapper config discover --save\n</code></pre> <p>This updates the <code>database_tools</code> section:</p> <pre><code>database_tools:\n  auto_detect: true\n  postgresql:\n    pg_dump: \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\"\n    pg_restore: \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\"\n    psql: \"/Applications/Postgres.app/Contents/Versions/17/bin/psql\"\n  mysql:\n    mysql: \"/opt/homebrew/opt/mysql-client/bin/mysql\"\n    mysqldump: \"/opt/homebrew/opt/mysql-client/bin/mysqldump\"\n</code></pre>"}, {"location": "commands/config-discover/#configuration-merge-behavior", "title": "Configuration Merge Behavior", "text": "<ul> <li>Preserves existing settings: Manual tool paths are not overwritten</li> <li>Updates auto-detected paths: Only updates paths that were auto-detected</li> <li>Maintains structure: Preserves other configuration sections</li> <li>Backup safety: Creates backup before making changes</li> </ul>"}, {"location": "commands/config-discover/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/config-discover/#basic-tool-discovery", "title": "Basic Tool Discovery", "text": "<pre><code># Discover tools without saving\ndbsnapper config discover\n</code></pre>"}, {"location": "commands/config-discover/#discovery-and-configuration-update", "title": "Discovery and Configuration Update", "text": "<pre><code># Discover tools and save to configuration\ndbsnapper config discover --save\n</code></pre>"}, {"location": "commands/config-discover/#json-output-for-automation", "title": "JSON Output for Automation", "text": "<pre><code># Get discovery results in JSON format\ndbsnapper config discover --json\n\n# Save JSON results and update configuration\ndbsnapper config discover --json --save\n</code></pre>"}, {"location": "commands/config-discover/#custom-configuration-file", "title": "Custom Configuration File", "text": "<pre><code># Discover tools for specific configuration file\ndbsnapper config discover --config /path/to/custom.yml --save\n</code></pre>"}, {"location": "commands/config-discover/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/config-discover/#initial-system-setup", "title": "Initial System Setup", "text": "<pre><code># After installing DBSnapper\ndbsnapper config init\ndbsnapper config discover --save\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-discover/#after-database-software-updates", "title": "After Database Software Updates", "text": "<pre><code># After upgrading PostgreSQL or MySQL\ndbsnapper config discover --save\n\n# Verify new tools work correctly\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-discover/#system-migration", "title": "System Migration", "text": "<pre><code># On new system or container\ndbsnapper config discover --json &gt; discovered-tools.json\ndbsnapper config discover --save\n\n# Verify migration success\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-discover/#cicd-environment-setup", "title": "CI/CD Environment Setup", "text": "<pre><code># Automated pipeline setup\ndbsnapper config discover --save\nif ! dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Database tools not properly configured\"\n    exit 1\nfi\n</code></pre>"}, {"location": "commands/config-discover/#team-environment-standardization", "title": "Team Environment Standardization", "text": "<pre><code># Generate standardized tool configuration\ndbsnapper config discover --json --save\n\n# Share configuration template with team\ncp ~/.config/dbsnapper/dbsnapper.yml team-template.yml\n</code></pre>"}, {"location": "commands/config-discover/#troubleshooting-tool-issues", "title": "Troubleshooting Tool Issues", "text": "<pre><code># Diagnose tool problems\ndbsnapper config discover\n\n# Compare with current configuration\ndbsnapper config check\n\n# Update if tools have moved\ndbsnapper config discover --save\n</code></pre>"}, {"location": "commands/config-discover/#advanced-usage", "title": "Advanced Usage", "text": ""}, {"location": "commands/config-discover/#programmatic-integration", "title": "Programmatic Integration", "text": "<pre><code>#!/bin/bash\n# Script to ensure tools are available\n\n# Discover tools and get JSON output\nDISCOVERY_RESULT=$(dbsnapper config discover --json)\n\n# Check if discovery was successful\nif echo \"$DISCOVERY_RESULT\" | jq -e '.success' &gt;/dev/null; then\n    echo \"Tools discovered successfully\"\n    dbsnapper config discover --save\nelse\n    echo \"Tool discovery failed\"\n    exit 1\nfi\n</code></pre>"}, {"location": "commands/config-discover/#automated-configuration-updates", "title": "Automated Configuration Updates", "text": "<pre><code>#!/bin/bash\n# Automated maintenance script\n\n# Check current configuration\nif ! dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Configuration issues detected\"\n\n    # Try to fix with discovery\n    dbsnapper config discover --save\n\n    # Verify fix worked\n    if dbsnapper config check &gt;/dev/null 2&gt;&amp;1; then\n        echo \"Configuration fixed automatically\"\n    else\n        echo \"Manual intervention required\"\n        exit 1\n    fi\nfi\n</code></pre>"}, {"location": "commands/config-discover/#multi-environment-discovery", "title": "Multi-Environment Discovery", "text": "<pre><code># Discover tools for different environments\nfor env in dev staging prod; do\n    echo \"Discovering tools for $env environment\"\n    dbsnapper config discover --config \"configs/$env.yml\" --save\ndone\n</code></pre>"}, {"location": "commands/config-discover/#tool-priority-and-selection", "title": "Tool Priority and Selection", "text": ""}, {"location": "commands/config-discover/#version-preference", "title": "Version Preference", "text": "<p>When multiple versions are found, the discovery process prioritizes:</p> <ol> <li>Newer versions: More recent tool versions are preferred</li> <li>Standard locations: Tools in standard locations over custom paths</li> <li>Package manager installations: Tools installed via package managers</li> <li>Consistency: Tools from the same installation/version when possible</li> </ol>"}, {"location": "commands/config-discover/#path-selection-logic", "title": "Path Selection Logic", "text": "<pre><code># Example priority order for PostgreSQL on macOS:\n# 1. /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\n# 2. /opt/homebrew/bin/pg_dump  \n# 3. /usr/local/bin/pg_dump\n# 4. /usr/bin/pg_dump\n</code></pre>"}, {"location": "commands/config-discover/#error-handling-and-troubleshooting", "title": "Error Handling and Troubleshooting", "text": ""}, {"location": "commands/config-discover/#common-discovery-issues", "title": "Common Discovery Issues", "text": ""}, {"location": "commands/config-discover/#no-tools-found", "title": "No Tools Found", "text": "<pre><code># Output when no tools are found\n\u274c No PostgreSQL tools found\n\u274c No MySQL tools found\n</code></pre> <p>Solutions: 1. Install database client tools using package manager 2. Add tool locations to system PATH 3. Install from official database websites</p>"}, {"location": "commands/config-discover/#permission-issues", "title": "Permission Issues", "text": "<pre><code># Output for permission problems\n\u26a0\ufe0f pg_dump found but not executable\n</code></pre> <p>Solutions: <pre><code># Fix permissions\nchmod +x /path/to/pg_dump\n\n# Check ownership\nls -la /path/to/pg_dump\n</code></pre></p>"}, {"location": "commands/config-discover/#version-compatibility-issues", "title": "Version Compatibility Issues", "text": "<pre><code># Output for incompatible versions\n\u26a0\ufe0f pg_dump version too old (requires 12+)\n</code></pre> <p>Solutions: 1. Upgrade to supported version 2. Install newer version alongside existing 3. Use package manager to update</p>"}, {"location": "commands/config-discover/#discovery-failures", "title": "Discovery Failures", "text": "<pre><code>{\n  \"success\": false,\n  \"error\": \"No database tools found\",\n  \"discovered_tools\": {},\n  \"validation_results\": {},\n  \"message\": \"Database tools discovery failed\"\n}\n</code></pre>"}, {"location": "commands/config-discover/#performance-considerations", "title": "Performance Considerations", "text": ""}, {"location": "commands/config-discover/#discovery-speed", "title": "Discovery Speed", "text": "<ul> <li>Fast search: Optimized path scanning</li> <li>Cached results: Results can be cached between runs</li> <li>Parallel validation: Tools validated concurrently when possible</li> <li>Early termination: Stops searching when optimal tools found</li> </ul>"}, {"location": "commands/config-discover/#system-impact", "title": "System Impact", "text": "<ul> <li>Low overhead: Minimal system resource usage</li> <li>No installation: Only discovers existing tools</li> <li>Safe operation: Read-only system scanning</li> <li>Respect limits: Honours system resource constraints</li> </ul>"}, {"location": "commands/config-discover/#best-practices", "title": "Best Practices", "text": ""}, {"location": "commands/config-discover/#discovery-strategy", "title": "Discovery Strategy", "text": "<ol> <li>Regular discovery: Run after software updates</li> <li>Validation: Always validate discovered tools with <code>config check</code></li> <li>Version tracking: Monitor tool versions for compatibility</li> <li>Documentation: Document custom tool installations</li> </ol>"}, {"location": "commands/config-discover/#configuration-management", "title": "Configuration Management", "text": "<ol> <li>Backup first: Backup configuration before auto-saving</li> <li>Version control: Track configuration changes</li> <li>Testing: Test configuration after discovery updates</li> <li>Rollback plan: Have rollback procedures for failed updates</li> </ol>"}, {"location": "commands/config-discover/#automation-integration", "title": "Automation Integration", "text": "<ol> <li>CI/CD integration: Include in pipeline setup</li> <li>Health monitoring: Regular discovery in monitoring scripts</li> <li>Error handling: Robust error handling in automated scripts</li> <li>Logging: Log discovery results for troubleshooting</li> </ol> <p>After Software Updates</p> <p>Run <code>dbsnapper config discover --save</code> after updating PostgreSQL or MySQL to ensure DBSnapper uses the latest tools.</p> <p>JSON Output</p> <p>Use <code>--json</code> flag for programmatic integration and automation scripts. The JSON output provides structured data for parsing and validation.</p> <p>Tool Compatibility</p> <p>The discover command validates tool compatibility with DBSnapper. Very old or very new tool versions may not be supported.</p>"}, {"location": "commands/config-discover/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>config init</code> - Initialize configuration (includes discovery)</li> <li><code>config check</code> - Validate discovered tools and configuration</li> <li><code>config validate</code> - Validate configuration file syntax</li> <li><code>config</code> - Overview of configuration management commands</li> </ul>"}, {"location": "commands/config-discover/#see-also", "title": "See Also", "text": "<ul> <li>Database Engines - Supported databases and client requirements</li> <li>Installation Guide - Installing database client tools</li> <li>Configuration Reference - Database tools configuration options</li> </ul>"}, {"location": "commands/config-init/", "title": "Config Init Command", "text": "<p>The <code>config init</code> command initializes DBSnapper by creating a configuration file and working directory structure. This is typically the first command new users run to set up their DBSnapper environment with sensible defaults and automatically discovered database tools.</p>"}, {"location": "commands/config-init/#overview", "title": "Overview", "text": "<p>The config init command bootstraps a complete DBSnapper installation by: - Creating a YAML configuration file with default settings - Generating a secure encryption key for sensitive data - Setting up the working directory structure - Auto-discovering database client tools on your system - Providing a foundation for adding targets and storage profiles</p>"}, {"location": "commands/config-init/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper config init [flags]\n</code></pre>"}, {"location": "commands/config-init/#arguments", "title": "Arguments", "text": "<p>No arguments required - all configuration is automatic with sensible defaults.</p>"}, {"location": "commands/config-init/#options", "title": "Options", "text": "<pre><code>  -h, --help   Show help for the config init command\n</code></pre>"}, {"location": "commands/config-init/#how-it-works", "title": "How It Works", "text": "<ol> <li>Configuration File Creation: Creates a YAML configuration file at the specified or default location</li> <li>Working Directory Setup: Creates the working directory for storing snapshots and temporary files</li> <li>Secret Key Generation: Generates a secure 32-character secret key for encrypting sensitive data</li> <li>Database Tool Discovery: Automatically discovers and configures PostgreSQL and MySQL client tools</li> <li>Default Settings: Applies sensible defaults for Docker images, CPU usage, and feature flags</li> <li>Validation: Validates discovered tools and reports their status</li> </ol>"}, {"location": "commands/config-init/#configuration-file-location", "title": "Configuration File Location", "text": "<p>The configuration file location follows this priority order:</p> <ol> <li><code>--config</code> flag: Explicitly specified file path</li> <li><code>DBSNAPPER_CONFIG</code> environment variable: Environment-specified path</li> <li>Development directory: If running from the DBSnapper repository, uses <code>dbsnapper.yml</code> in repo root</li> <li>Default location: <code>~/.config/dbsnapper/dbsnapper.yml</code></li> </ol>"}, {"location": "commands/config-init/#examples", "title": "Examples", "text": "<pre><code># Use default location (~/.config/dbsnapper/dbsnapper.yml)\ndbsnapper config init\n\n# Specify custom location with flag\ndbsnapper config init --config /path/to/custom/config.yml\n\n# Specify custom location with environment variable\nDBSNAPPER_CONFIG=/path/to/custom/config.yml dbsnapper config init\n</code></pre>"}, {"location": "commands/config-init/#generated-configuration-structure", "title": "Generated Configuration Structure", "text": "<p>The config init command creates a complete configuration file with these sections:</p>"}, {"location": "commands/config-init/#authentication-and-security", "title": "Authentication and Security", "text": "<pre><code>authtoken: \"\"                                    # DBSnapper Cloud API token\nsecret_key: 37493065d18a05a4df222decea24e956     # Generated encryption key\n</code></pre>"}, {"location": "commands/config-init/#database-tools-auto-discovered", "title": "Database Tools (Auto-Discovered)", "text": "<pre><code>database_tools:\n  auto_detect: true                              # Enable automatic tool discovery\n  postgresql:\n    pg_dump: /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\n    pg_restore: /Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\n    psql: /Applications/Postgres.app/Contents/Versions/17/bin/psql\n  mysql:\n    mysql: /opt/homebrew/opt/mysql-client/bin/mysql\n    mysqldump: /opt/homebrew/opt/mysql-client/bin/mysqldump\n</code></pre>"}, {"location": "commands/config-init/#default-settings", "title": "Default Settings", "text": "<pre><code>defaults:\n  working_directory: /Users/username/.dbsnapper  # Snapshot storage location\n  cpus: 2                                        # CPU cores for operations\n  shared_target_dst_db_url: \"\"                   # Default destination for shared targets\n</code></pre>"}, {"location": "commands/config-init/#docker-configuration", "title": "Docker Configuration", "text": "<pre><code>docker:\n  images:\n    mysql: mysql:9                               # MySQL Docker image\n    postgres: postgres:17                        # PostgreSQL Docker image\n</code></pre>"}, {"location": "commands/config-init/#feature-flags-and-overrides", "title": "Feature Flags and Overrides", "text": "<pre><code>debug: false                                     # Debug logging disabled\nfeature_flags:\n  development_utilities: false                   # Development tools disabled by default\noverride:\n  dst_db_url: \"\"                                # Global destination database override\n  nocloud: false                                # Cloud mode enabled by default\n  san_query: \"\"                                 # Global sanitization query override\n</code></pre>"}, {"location": "commands/config-init/#working-directory-structure", "title": "Working Directory Structure", "text": "<p>The initialized working directory (<code>~/.dbsnapper</code> by default) contains:</p> <pre><code>~/.dbsnapper/\n\u251c\u2500\u2500 snapshots/          # Created automatically when first snapshot is built\n\u2514\u2500\u2500 temp/              # Created automatically for temporary operations\n</code></pre> <p>Snapshot Files: When you create snapshots, they're stored as: - <code>{timestamp}_{target_name}.zip</code> - Original snapshots - <code>{timestamp}_{target_name}.san.zip</code> - Sanitized snapshots</p>"}, {"location": "commands/config-init/#database-tool-discovery", "title": "Database Tool Discovery", "text": "<p>The init command automatically discovers database client tools in these locations:</p>"}, {"location": "commands/config-init/#postgresql-tools", "title": "PostgreSQL Tools", "text": "<ul> <li>Postgres.app: <code>/Applications/Postgres.app/Contents/Versions/*/bin/</code></li> <li>Homebrew: <code>/opt/homebrew/bin/</code>, <code>/usr/local/bin/</code></li> <li>System paths: <code>/usr/bin/</code>, <code>/usr/local/bin/</code></li> <li>Custom installations: Checks PATH environment variable</li> </ul>"}, {"location": "commands/config-init/#mysql-tools", "title": "MySQL Tools", "text": "<ul> <li>Homebrew: <code>/opt/homebrew/opt/mysql-client/bin/</code>, <code>/opt/homebrew/bin/</code></li> <li>System installations: <code>/usr/bin/</code>, <code>/usr/local/bin/</code>, <code>/usr/local/mysql/bin/</code></li> <li>Custom installations: Checks PATH environment variable</li> </ul>"}, {"location": "commands/config-init/#tool-validation", "title": "Tool Validation", "text": "<p>Each discovered tool is validated by: - Checking executable permissions - Verifying the tool responds to version queries - Ensuring compatibility with DBSnapper requirements</p>"}, {"location": "commands/config-init/#configuration-file-format-evolution", "title": "Configuration File Format Evolution", "text": "<p>DBSnapper v3.0 introduces an improved configuration format while maintaining backwards compatibility:</p>"}, {"location": "commands/config-init/#new-format-recommended", "title": "New Format (Recommended)", "text": "<pre><code>defaults:\n  working_directory: ~/.dbsnapper    # New location under defaults\n  cpus: 2\n</code></pre>"}, {"location": "commands/config-init/#legacy-format-still-supported", "title": "Legacy Format (Still Supported)", "text": "<pre><code>working_directory: ~/.dbsnapper      # Legacy root-level location\n</code></pre>"}, {"location": "commands/config-init/#priority-rules", "title": "Priority Rules", "text": "<ul> <li>If both formats exist, <code>defaults.working_directory</code> takes precedence</li> <li>Legacy format is automatically upgraded on next configuration update</li> <li>No manual migration required</li> </ul>"}, {"location": "commands/config-init/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/config-init/#basic-initialization", "title": "Basic Initialization", "text": "<pre><code># Initialize with default settings\ndbsnapper config init\n</code></pre> <p>Output: <pre><code>DBSnapper v2.7.0-dev - Configuration Initialization\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\u2192 Config file location: /Users/username/.config/dbsnapper/dbsnapper.yml\n\u2192 Working directory: /Users/username/.dbsnapper\n\n\u2192 Configuration Initialization\n\n  \u2713 Configuration file initialized: /Users/username/.config/dbsnapper/dbsnapper.yml\n  \u2713 Working directory created: /Users/username/.dbsnapper\n</code></pre></p>"}, {"location": "commands/config-init/#custom-location-initialization", "title": "Custom Location Initialization", "text": "<pre><code># Initialize with custom configuration location\ndbsnapper config init --config /project/dbsnapper.yml\n</code></pre>"}, {"location": "commands/config-init/#development-setup", "title": "Development Setup", "text": "<pre><code># Initialize for development (from DBSnapper repo directory)\ncd /path/to/dbsnapper-repo\ndbsnapper config init  # Creates dbsnapper.yml in repo root\n</code></pre>"}, {"location": "commands/config-init/#post-initialization-steps", "title": "Post-Initialization Steps", "text": "<p>After running <code>config init</code>, you can:</p>"}, {"location": "commands/config-init/#1-verify-installation", "title": "1. Verify Installation", "text": "<pre><code># Check configuration and discovered tools\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config-init/#2-add-database-targets", "title": "2. Add Database Targets", "text": "<p>Edit your configuration file to add targets:</p> <pre><code>targets:\n  my-database:\n    name: \"My Database\"\n    snapshot:\n      src_url: \"postgresql://user:pass@localhost:5432/mydb\"\n      dst_url: \"postgresql://user:pass@localhost:5432/mydb_snap\"\n</code></pre>"}, {"location": "commands/config-init/#3-configure-cloud-integration-optional", "title": "3. Configure Cloud Integration (Optional)", "text": "<pre><code># Add DBSnapper Cloud authentication\ndbsnapper auth token &lt;your_token&gt;\n</code></pre>"}, {"location": "commands/config-init/#4-add-storage-profiles-optional", "title": "4. Add Storage Profiles (Optional)", "text": "<pre><code>storage_profiles:\n  aws-s3:\n    type: s3\n    region: us-west-2\n    bucket: my-snapshots\n</code></pre>"}, {"location": "commands/config-init/#configuration-file-management", "title": "Configuration File Management", "text": ""}, {"location": "commands/config-init/#manual-editing", "title": "Manual Editing", "text": "<p>The generated configuration file is standard YAML and can be edited manually:</p> <pre><code># Edit with your preferred editor\nnano ~/.config/dbsnapper/dbsnapper.yml\nvim ~/.config/dbsnapper/dbsnapper.yml\n</code></pre>"}, {"location": "commands/config-init/#validation", "title": "Validation", "text": "<pre><code># Validate configuration after manual changes\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-init/#tool-re-discovery", "title": "Tool Re-discovery", "text": "<pre><code># Re-discover database tools after installing new versions\ndbsnapper config discover --save\n</code></pre>"}, {"location": "commands/config-init/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/config-init/#new-user-setup", "title": "New User Setup", "text": "<pre><code># Complete new installation\ndbsnapper config init\ndbsnapper config check\n# Add targets via manual configuration or DBSnapper Cloud\n</code></pre>"}, {"location": "commands/config-init/#project-specific-configuration", "title": "Project-Specific Configuration", "text": "<pre><code># Setup per-project configuration\ncd /path/to/project\ndbsnapper config init --config ./dbsnapper.yml\n# Configure project-specific targets and settings\n</code></pre>"}, {"location": "commands/config-init/#development-environment", "title": "Development Environment", "text": "<pre><code># Setup development environment\ncd /path/to/dbsnapper-repo\ndbsnapper config init\n# Enables development features and uses repo-local config\n</code></pre>"}, {"location": "commands/config-init/#cicd-pipeline-setup", "title": "CI/CD Pipeline Setup", "text": "<pre><code># Automated pipeline initialization\nDBSNAPPER_CONFIG=/ci/config/dbsnapper.yml dbsnapper config init\n# Configure targets via environment variables or mounted config\n</code></pre>"}, {"location": "commands/config-init/#multiple-environment-management", "title": "Multiple Environment Management", "text": "<pre><code># Separate configurations for different environments\ndbsnapper config init --config ~/.config/dbsnapper/dev.yml\ndbsnapper config init --config ~/.config/dbsnapper/staging.yml\ndbsnapper config init --config ~/.config/dbsnapper/prod.yml\n</code></pre>"}, {"location": "commands/config-init/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "commands/config-init/#secret-key-protection", "title": "Secret Key Protection", "text": "<ul> <li>The generated <code>secret_key</code> encrypts sensitive data in the configuration</li> <li>Keep secure: Don't commit to version control</li> <li>Backup safely: Store in secure password manager</li> <li>Rotate regularly: Generate new keys periodically</li> </ul>"}, {"location": "commands/config-init/#configuration-file-security", "title": "Configuration File Security", "text": "<pre><code># Set appropriate file permissions\nchmod 600 ~/.config/dbsnapper/dbsnapper.yml\n\n# Verify permissions\nls -la ~/.config/dbsnapper/dbsnapper.yml\n# Should show: -rw------- (read/write for owner only)\n</code></pre>"}, {"location": "commands/config-init/#environment-variables", "title": "Environment Variables", "text": "<pre><code># Use environment variables for sensitive data in CI/CD\nexport DBSNAPPER_AUTHTOKEN=\"your_token_here\"\nexport DBSNAPPER_SECRET_KEY=\"your_secret_key_here\"\n</code></pre>"}, {"location": "commands/config-init/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/config-init/#common-issues", "title": "Common Issues", "text": "<p>Permission Denied <pre><code>Error: permission denied creating configuration directory\n</code></pre> - Check write permissions to configuration directory - Try with appropriate permissions: <code>sudo dbsnapper config init</code> - Use custom location with write access</p> <p>Database Tools Not Found <pre><code>Warning: no PostgreSQL tools found\nWarning: no MySQL tools found\n</code></pre> - Install required database client tools - Check PATH environment variable - Run <code>dbsnapper config discover --save</code> after installation</p> <p>Configuration File Already Exists <pre><code>Error: configuration file already exists\n</code></pre> - The init command won't overwrite existing configurations - Delete existing file if you want to reinitialize: <code>rm ~/.config/dbsnapper/dbsnapper.yml</code> - Use different location: <code>dbsnapper config init --config /path/to/new/config.yml</code></p> <p>Working Directory Creation Failed <pre><code>Error: failed to create working directory\n</code></pre> - Check disk space availability - Verify write permissions to parent directory - Try different working directory location in configuration</p> <p>Invalid Configuration Path <pre><code>Error: invalid configuration file path\n</code></pre> - Ensure parent directories exist - Use absolute paths when possible - Check path syntax and permissions</p>"}, {"location": "commands/config-init/#recovery-steps", "title": "Recovery Steps", "text": "<ol> <li>Check Permissions: Ensure write access to configuration and working directories</li> <li>Verify Tools: Install required PostgreSQL and MySQL client tools</li> <li>Clear State: Remove existing configuration files if reinitializing</li> <li>Use Custom Paths: Try different configuration or working directory locations</li> <li>Check Dependencies: Ensure all required system dependencies are installed</li> </ol> <p>First Command</p> <p><code>config init</code> is typically the first command new users run. It sets up everything needed to start using DBSnapper with sensible defaults.</p> <p>Existing Configuration</p> <p>The init command will not overwrite existing configuration files. Delete the existing file first if you want to reinitialize.</p> <p>Auto-Discovery</p> <p>Database tools are automatically discovered during initialization. Run <code>dbsnapper config discover --save</code> after installing new database clients.</p>"}, {"location": "commands/config-init/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>config check</code> - Validate configuration and dependencies</li> <li><code>config discover</code> - Re-discover database tools</li> <li><code>config validate</code> - Validate configuration file syntax</li> <li><code>auth token</code> - Add cloud authentication</li> </ul>"}, {"location": "commands/config-init/#see-also", "title": "See Also", "text": "<ul> <li>Configuration - Complete configuration reference</li> <li>Database Engines - Supported databases and setup</li> <li>Installation - DBSnapper installation guide</li> <li>Quick Start - Getting started tutorial</li> </ul>"}, {"location": "commands/config-validate/", "title": "Config Validate Command", "text": "<p>The <code>config validate</code> command validates that database client tools specified in your configuration file are properly installed and executable. This focused validation tool ensures your configured tools are functional and ready for DBSnapper operations.</p>"}, {"location": "commands/config-validate/#overview", "title": "Overview", "text": "<p>The config validate command provides targeted tool validation: - Configuration-based validation: Tests only tools specified in your configuration - Executable verification: Confirms tools can be executed and respond correctly - Version checking: Validates tool versions meet DBSnapper requirements - Path verification: Ensures configured paths point to valid executables - Structured output: Provides both human-readable and JSON output formats</p>"}, {"location": "commands/config-validate/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper config validate [flags]\n</code></pre>"}, {"location": "commands/config-validate/#arguments", "title": "Arguments", "text": "<p>No arguments required - validates all configured database tools.</p>"}, {"location": "commands/config-validate/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for validate\n      --json   Output results as JSON\n</code></pre>"}, {"location": "commands/config-validate/#validation-process", "title": "Validation Process", "text": ""}, {"location": "commands/config-validate/#configuration-loading", "title": "Configuration Loading", "text": "<ol> <li>Configuration file: Loads your DBSnapper configuration file</li> <li>Tool extraction: Extracts database tool paths from configuration</li> <li>Path resolution: Resolves relative paths and environment variables</li> <li>Tool identification: Identifies PostgreSQL and MySQL tools to validate</li> </ol>"}, {"location": "commands/config-validate/#tool-validation-steps", "title": "Tool Validation Steps", "text": "<p>For each configured tool, the validation process:</p> <ol> <li>Path verification: Confirms the tool exists at the specified path</li> <li>Permission check: Verifies the tool has executable permissions</li> <li>Execution test: Runs the tool with <code>--version</code> flag to test functionality</li> <li>Version analysis: Analyzes version output for compatibility</li> <li>Status reporting: Reports validation success or failure with details</li> </ol>"}, {"location": "commands/config-validate/#validated-tools", "title": "Validated Tools", "text": ""}, {"location": "commands/config-validate/#postgresql-tools", "title": "PostgreSQL Tools", "text": "<ul> <li><code>psql</code>: PostgreSQL command-line client</li> <li><code>pg_dump</code>: PostgreSQL database backup tool</li> <li><code>pg_restore</code>: PostgreSQL database restore tool</li> </ul>"}, {"location": "commands/config-validate/#mysql-tools", "title": "MySQL Tools", "text": "<ul> <li><code>mysql</code>: MySQL command-line client</li> <li><code>mysqldump</code>: MySQL database backup tool</li> </ul>"}, {"location": "commands/config-validate/#output-formats", "title": "Output Formats", "text": ""}, {"location": "commands/config-validate/#human-readable-output-default", "title": "Human-Readable Output (Default)", "text": "<pre><code>dbsnapper config validate\n</code></pre> <p>Successful Validation Output: <pre><code>DBSnapper v2.7.0-dev - Database Tools Validation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nConfiguration file: ~/.config/dbsnapper/dbsnapper.yml\n\n\u2192 Database Tools Validation\n\n  \u2705 pg_dump: /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\n  \u2705 pg_restore: /Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\n  \u2705 psql: /Applications/Postgres.app/Contents/Versions/17/bin/psql\n  \u2705 mysql: /opt/homebrew/opt/mysql-client/bin/mysql\n  \u2705 mysqldump: /opt/homebrew/opt/mysql-client/bin/mysqldump\n\n  \u2713 All database tools are properly configured and validated\n</code></pre></p> <p>Validation with Issues: <pre><code>DBSnapper v2.7.0-dev - Database Tools Validation\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nConfiguration file: ~/.config/dbsnapper/dbsnapper.yml\n\n\u2192 Database Tools Validation\n\n  \u2705 pg_dump: /usr/local/bin/pg_dump\n  \u274c pg_restore: /usr/local/bin/pg_restore (not found)\n  \u2705 psql: /usr/local/bin/psql\n  \u274c mysql: /usr/local/bin/mysql (permission denied)\n  \u2705 mysqldump: /usr/local/bin/mysqldump\n\n  \u26a0\ufe0f 2 database tools have configuration issues\n</code></pre></p>"}, {"location": "commands/config-validate/#json-output", "title": "JSON Output", "text": "<pre><code>dbsnapper config validate --json\n</code></pre> <p>Successful Validation JSON: <pre><code>{\n  \"success\": true,\n  \"current_tools\": {\n    \"auto_detect\": true,\n    \"postgresql\": {\n      \"pg_dump\": \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\",\n      \"pg_restore\": \"/Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\",\n      \"psql\": \"/Applications/Postgres.app/Contents/Versions/17/bin/psql\"\n    },\n    \"mysql\": {\n      \"mysql\": \"/opt/homebrew/opt/mysql-client/bin/mysql\",\n      \"mysqldump\": \"/opt/homebrew/opt/mysql-client/bin/mysqldump\"\n    }\n  },\n  \"validation_results\": {\n    \"mysql\": \"\u2705 /opt/homebrew/opt/mysql-client/bin/mysql\",\n    \"mysqldump\": \"\u2705 /opt/homebrew/opt/mysql-client/bin/mysqldump\",\n    \"pg_dump\": \"\u2705 /Applications/Postgres.app/Contents/Versions/17/bin/pg_dump\",\n    \"pg_restore\": \"\u2705 /Applications/Postgres.app/Contents/Versions/17/bin/pg_restore\",\n    \"psql\": \"\u2705 /Applications/Postgres.app/Contents/Versions/17/bin/psql\"\n  },\n  \"missing_tools_count\": 0,\n  \"message\": \"All database tools are properly configured and validated\"\n}\n</code></pre></p> <p>Validation with Issues JSON: <pre><code>{\n  \"success\": false,\n  \"current_tools\": {\n    \"auto_detect\": false,\n    \"postgresql\": {\n      \"pg_dump\": \"/usr/local/bin/pg_dump\",\n      \"pg_restore\": \"/usr/local/bin/pg_restore\",\n      \"psql\": \"/usr/local/bin/psql\"\n    },\n    \"mysql\": {\n      \"mysql\": \"/usr/local/bin/mysql\",\n      \"mysqldump\": \"/usr/local/bin/mysqldump\"\n    }\n  },\n  \"validation_results\": {\n    \"pg_dump\": \"\u2705 /usr/local/bin/pg_dump\",\n    \"pg_restore\": \"\u274c /usr/local/bin/pg_restore (not found)\",\n    \"psql\": \"\u2705 /usr/local/bin/psql\",\n    \"mysql\": \"\u274c /usr/local/bin/mysql (permission denied)\",\n    \"mysqldump\": \"\u2705 /usr/local/bin/mysqldump\"\n  },\n  \"missing_tools_count\": 2,\n  \"message\": \"2 database tools have configuration issues\",\n  \"errors\": [\n    {\n      \"tool\": \"pg_restore\",\n      \"path\": \"/usr/local/bin/pg_restore\",\n      \"error\": \"file not found\"\n    },\n    {\n      \"tool\": \"mysql\", \n      \"path\": \"/usr/local/bin/mysql\",\n      \"error\": \"permission denied\"\n    }\n  ]\n}\n</code></pre></p>"}, {"location": "commands/config-validate/#validation-results-interpretation", "title": "Validation Results Interpretation", "text": ""}, {"location": "commands/config-validate/#status-indicators", "title": "Status Indicators", "text": "<ul> <li>\u2705 Success: Tool is properly configured and functional</li> <li>\u274c Error: Tool has issues that prevent operation</li> <li>\u26a0\ufe0f Warning: Tool works but has non-critical issues</li> <li>\u2139\ufe0f Info: Informational status about the tool</li> </ul>"}, {"location": "commands/config-validate/#common-error-types", "title": "Common Error Types", "text": "<ul> <li>File not found: Tool path doesn't exist or is incorrect</li> <li>Permission denied: Tool exists but lacks executable permissions</li> <li>Version incompatible: Tool version doesn't meet requirements</li> <li>Execution failed: Tool exists but fails to run properly</li> </ul>"}, {"location": "commands/config-validate/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/config-validate/#basic-validation", "title": "Basic Validation", "text": "<pre><code># Validate all configured database tools\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-validate/#json-output-for-automation", "title": "JSON Output for Automation", "text": "<pre><code># Get validation results in JSON format\ndbsnapper config validate --json\n\n# Use in scripts for automated validation\nif dbsnapper config validate --json | jq -e '.success'; then\n    echo \"All tools validated successfully\"\nelse\n    echo \"Tool validation failed\"\nfi\n</code></pre>"}, {"location": "commands/config-validate/#custom-configuration-file", "title": "Custom Configuration File", "text": "<pre><code># Validate specific configuration file\ndbsnapper config validate --config /path/to/custom.yml\n</code></pre>"}, {"location": "commands/config-validate/#validation-in-scripts", "title": "Validation in Scripts", "text": "<pre><code>#!/bin/bash\n# Script to ensure tools are valid before operations\n\nif ! dbsnapper config validate &gt;/dev/null 2&gt;&amp;1; then\n    echo \"Database tools validation failed\"\n    dbsnapper config validate  # Show details\n    exit 1\nfi\n\necho \"Tools validated, proceeding with operations...\"\n</code></pre>"}, {"location": "commands/config-validate/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/config-validate/#pre-operation-validation", "title": "Pre-Operation Validation", "text": "<pre><code># Validate configuration before critical operations\ndbsnapper config validate &amp;&amp; dbsnapper build production-db\ndbsnapper config validate &amp;&amp; dbsnapper sanitize sensitive-data\n</code></pre>"}, {"location": "commands/config-validate/#configuration-file-changes", "title": "Configuration File Changes", "text": "<pre><code># After manual configuration edits\nnano ~/.config/dbsnapper/dbsnapper.yml\ndbsnapper config validate\n\n# Verify changes didn't break tool configuration\n</code></pre>"}, {"location": "commands/config-validate/#system-maintenance", "title": "System Maintenance", "text": "<pre><code># After system updates or tool installations\nsudo apt update &amp;&amp; sudo apt upgrade postgresql-client mysql-client\ndbsnapper config validate\n\n# Check if tool paths are still valid\n</code></pre>"}, {"location": "commands/config-validate/#cicd-pipeline-validation", "title": "CI/CD Pipeline Validation", "text": "<pre><code># In automated pipelines\ndbsnapper config validate --json &gt; validation-results.json\n\n# Check validation status\nif ! cat validation-results.json | jq -e '.success'; then\n    echo \"Tool validation failed in CI\"\n    exit 1\nfi\n</code></pre>"}, {"location": "commands/config-validate/#team-environment-verification", "title": "Team Environment Verification", "text": "<pre><code># Verify team member setup\ndbsnapper config validate\n\n# Ensure consistent tool configuration across team\nfor member in alice bob charlie; do\n    echo \"Validating $member's configuration\"\n    ssh $member \"dbsnapper config validate\"\ndone\n</code></pre>"}, {"location": "commands/config-validate/#troubleshooting-workflow", "title": "Troubleshooting Workflow", "text": "<pre><code># Diagnose tool issues\ndbsnapper config validate\n\n# Get detailed error information\ndbsnapper config validate --json | jq '.errors[]'\n\n# Fix issues and re-validate\ndbsnapper config discover --save\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-validate/#error-resolution", "title": "Error Resolution", "text": ""}, {"location": "commands/config-validate/#common-issues-and-solutions", "title": "Common Issues and Solutions", "text": ""}, {"location": "commands/config-validate/#tool-not-found", "title": "Tool Not Found", "text": "<pre><code>\u274c pg_dump: /usr/local/bin/pg_dump (not found)\n</code></pre> <p>Solutions: 1. Install the tool: Use package manager to install PostgreSQL client 2. Update path: Use <code>config discover</code> to find correct path 3. Fix configuration: Manually update configuration with correct path</p> <pre><code># Auto-fix with discovery\ndbsnapper config discover --save\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-validate/#permission-denied", "title": "Permission Denied", "text": "<pre><code>\u274c mysql: /usr/local/bin/mysql (permission denied)\n</code></pre> <p>Solutions: <pre><code># Fix permissions\nsudo chmod +x /usr/local/bin/mysql\n\n# Or fix ownership\nsudo chown $USER:$USER /usr/local/bin/mysql\n\n# Verify fix\ndbsnapper config validate\n</code></pre></p>"}, {"location": "commands/config-validate/#version-incompatible", "title": "Version Incompatible", "text": "<pre><code>\u26a0\ufe0f pg_dump: /usr/local/bin/pg_dump (version too old)\n</code></pre> <p>Solutions: 1. Update tool: Upgrade to supported version 2. Install newer version: Install alongside existing version 3. Use package manager: Update via package manager</p> <pre><code># Update via Homebrew (macOS)\nbrew upgrade postgresql\n\n# Update via apt (Ubuntu/Debian)\nsudo apt update &amp;&amp; sudo apt upgrade postgresql-client\n\n# Re-validate\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-validate/#execution-failed", "title": "Execution Failed", "text": "<pre><code>\u274c mysqldump: /usr/local/bin/mysqldump (execution failed)\n</code></pre> <p>Solutions: 1. Check dependencies: Ensure required libraries are installed 2. Verify installation: Reinstall the tool if corrupted 3. Test manually: Run the tool manually to diagnose issues</p> <pre><code># Test tool manually\n/usr/local/bin/mysqldump --version\n\n# Reinstall if needed\nsudo apt reinstall mysql-client\n\n# Re-validate\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config-validate/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/config-validate/#validation-workflow", "title": "Validation Workflow", "text": "<pre><code># Complete configuration workflow\ndbsnapper config init           # Initialize configuration\ndbsnapper config discover --save # Auto-discover tools\ndbsnapper config validate       # Validate configuration\ndbsnapper config check          # Comprehensive health check\n</code></pre>"}, {"location": "commands/config-validate/#pre-operation-checks", "title": "Pre-Operation Checks", "text": "<pre><code># Validate before important operations\ndbsnapper config validate &amp;&amp; {\n    dbsnapper build production-db\n    dbsnapper sanitize production-db\n    dbsnapper load staging-db\n}\n</code></pre>"}, {"location": "commands/config-validate/#automated-maintenance", "title": "Automated Maintenance", "text": "<pre><code>#!/bin/bash\n# Automated maintenance script\n\necho \"Validating database tools configuration...\"\nif ! dbsnapper config validate --json | jq -e '.success'; then\n    echo \"Attempting to fix tool configuration...\"\n    dbsnapper config discover --save\n\n    if ! dbsnapper config validate --json | jq -e '.success'; then\n        echo \"Manual intervention required\"\n        exit 1\n    fi\nfi\n\necho \"Configuration validated successfully\"\n</code></pre>"}, {"location": "commands/config-validate/#performance-and-considerations", "title": "Performance and Considerations", "text": ""}, {"location": "commands/config-validate/#validation-speed", "title": "Validation Speed", "text": "<ul> <li>Fast validation: Typically completes in seconds</li> <li>Executable tests: Quick version checks for each tool</li> <li>Minimal system impact: Low CPU and memory usage</li> <li>Network independence: No network connectivity required</li> </ul>"}, {"location": "commands/config-validate/#system-requirements", "title": "System Requirements", "text": "<ul> <li>File system access: Requires read access to tool paths</li> <li>Execute permissions: Needs ability to run tools with --version</li> <li>Path resolution: Requires access to resolve configured paths</li> </ul>"}, {"location": "commands/config-validate/#best-practices", "title": "Best Practices", "text": ""}, {"location": "commands/config-validate/#regular-validation", "title": "Regular Validation", "text": "<ol> <li>After configuration changes: Always validate after manual edits</li> <li>System updates: Validate after OS or database tool updates</li> <li>Team onboarding: Include validation in setup procedures</li> <li>Automated checks: Include in monitoring and CI/CD pipelines</li> </ol>"}, {"location": "commands/config-validate/#configuration-management", "title": "Configuration Management", "text": "<ol> <li>Version control: Track configuration file changes</li> <li>Testing: Test configurations in non-production environments first</li> <li>Documentation: Document custom tool installations and paths</li> <li>Rollback: Have rollback procedures for broken configurations</li> </ol>"}, {"location": "commands/config-validate/#error-handling", "title": "Error Handling", "text": "<ol> <li>Graceful failures: Handle validation failures appropriately</li> <li>Detailed logging: Log validation results for troubleshooting</li> <li>Automated fixing: Use auto-discovery to fix common issues</li> <li>Manual procedures: Document manual resolution steps</li> </ol>"}, {"location": "commands/config-validate/#differences-from-other-config-commands", "title": "Differences from Other Config Commands", "text": ""}, {"location": "commands/config-validate/#config-validate-vs-config-check", "title": "<code>config validate</code> vs <code>config check</code>", "text": "<ul> <li><code>config validate</code>: Validates only configured tools</li> <li><code>config check</code>: Comprehensive system health check including Docker, cloud connectivity</li> </ul>"}, {"location": "commands/config-validate/#config-validate-vs-config-discover", "title": "<code>config validate</code> vs <code>config discover</code>", "text": "<ul> <li><code>config validate</code>: Tests existing configuration</li> <li><code>config discover</code>: Finds new tools and optionally updates configuration</li> </ul>"}, {"location": "commands/config-validate/#when-to-use-each", "title": "When to Use Each", "text": "<pre><code># After manual configuration changes\ndbsnapper config validate\n\n# After installing new database software\ndbsnapper config discover --save\n\n# For comprehensive system health check\ndbsnapper config check\n</code></pre> <p>Configuration Changes</p> <p>Always run <code>dbsnapper config validate</code> after manually editing your configuration file to ensure tool paths are still valid.</p> <p>JSON Output</p> <p>Use <code>--json</code> flag for programmatic integration. The JSON output includes detailed error information for troubleshooting.</p> <p>Tool Dependencies</p> <p>Validation only checks that tools are executable. It doesn't validate that all required libraries and dependencies are available.</p>"}, {"location": "commands/config-validate/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>config check</code> - Comprehensive configuration and system validation</li> <li><code>config discover</code> - Auto-discover and configure database tools</li> <li><code>config init</code> - Initialize configuration with auto-discovery</li> <li><code>config</code> - Overview of configuration management commands</li> </ul>"}, {"location": "commands/config-validate/#see-also", "title": "See Also", "text": "<ul> <li>Configuration Reference - Database tools configuration options</li> <li>Database Engines - Supported databases and requirements</li> <li>Installation Guide - Installing and configuring database client tools</li> </ul>"}, {"location": "commands/config/", "title": "Config Command", "text": "<p>The <code>config</code> command provides comprehensive configuration management for DBSnapper, offering tools to initialize, validate, check, and maintain your DBSnapper environment. This command serves as the central hub for all configuration-related operations.</p>"}, {"location": "commands/config/#overview", "title": "Overview", "text": "<p>The config command encompasses essential configuration management functions: - Initialization: Bootstrap new DBSnapper installations - Validation: Verify configuration file syntax and structure - Health Checking: Test database connectivity and tool availability - Auto-Discovery: Find and configure database client tools automatically - Troubleshooting: Diagnose and resolve configuration issues</p>"}, {"location": "commands/config/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper config [command]\n</code></pre>"}, {"location": "commands/config/#available-subcommands", "title": "Available Subcommands", "text": ""}, {"location": "commands/config/#core-configuration-commands", "title": "Core Configuration Commands", "text": "Command Purpose Use When <code>init</code> Initialize configuration and working directory Setting up new DBSnapper installation <code>check</code> Check configuration and dependencies Troubleshooting or health monitoring <code>discover</code> Auto-discover database tools After installing new database clients <code>validate</code> Validate configuration file syntax After manual configuration edits"}, {"location": "commands/config/#configuration-management-workflow", "title": "Configuration Management Workflow", "text": ""}, {"location": "commands/config/#new-installation-setup", "title": "New Installation Setup", "text": "<pre><code># 1. Initialize DBSnapper configuration\ndbsnapper config init\n\n# 2. Verify installation and auto-discover tools\ndbsnapper config check\n\n# 3. Validate configuration structure\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config/#maintenance-and-updates", "title": "Maintenance and Updates", "text": "<pre><code># Regular health check\ndbsnapper config check\n\n# After installing new database versions\ndbsnapper config discover --save\n\n# After manual configuration changes\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config/#troubleshooting-workflow", "title": "Troubleshooting Workflow", "text": "<pre><code># Comprehensive system check\ndbsnapper config check\n\n# Verify configuration syntax\ndbsnapper config validate\n\n# Re-discover tools if paths changed\ndbsnapper config discover --save --json\n</code></pre>"}, {"location": "commands/config/#configuration-file-management", "title": "Configuration File Management", "text": ""}, {"location": "commands/config/#configuration-file-locations", "title": "Configuration File Locations", "text": "<p>The config commands work with configuration files in this priority order:</p> <ol> <li><code>--config</code> flag: Explicitly specified file path</li> <li><code>DBSNAPPER_CONFIG</code> environment variable: Environment-specified path  </li> <li>Development directory: <code>dbsnapper.yml</code> in current directory (if inside DBSnapper repo)</li> <li>Default location: <code>~/.config/dbsnapper/dbsnapper.yml</code></li> </ol>"}, {"location": "commands/config/#configuration-file-structure", "title": "Configuration File Structure", "text": "<pre><code># Authentication and Security\nauthtoken: \"\"                    # DBSnapper Cloud API token\nsecret_key: \"generated-key\"      # Encryption key for sensitive data\n\n# Database Tools Configuration\ndatabase_tools:\n  auto_detect: true\n  postgresql:\n    pg_dump: \"/path/to/pg_dump\"\n    pg_restore: \"/path/to/pg_restore\"  \n    psql: \"/path/to/psql\"\n  mysql:\n    mysql: \"/path/to/mysql\"\n    mysqldump: \"/path/to/mysqldump\"\n\n# Default Settings\ndefaults:\n  working_directory: \"~/.dbsnapper\"\n  cpus: 2\n  shared_target_dst_db_url: \"\"\n\n# Docker Configuration  \ndocker:\n  images:\n    mysql: \"mysql:9\"\n    postgres: \"postgres:17\"\n\n# Feature Flags\nfeature_flags:\n  development_utilities: false\n\n# Global Overrides\noverride:\n  dst_db_url: \"\"\n  nocloud: false\n  san_query: \"\"\n\n# Storage Profiles (optional)\nstorage_profiles:\n  aws-s3:\n    type: \"s3\"\n    region: \"us-west-2\"\n    bucket: \"my-snapshots\"\n\n# Target Definitions\ntargets:\n  my-target:\n    name: \"My Database\"\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/db\"\n      dst_url: \"postgresql://user:pass@host:5432/db_snap\"\n</code></pre>"}, {"location": "commands/config/#common-configuration-scenarios", "title": "Common Configuration Scenarios", "text": ""}, {"location": "commands/config/#basic-local-setup", "title": "Basic Local Setup", "text": "<pre><code># Initialize for local development\ndbsnapper config init\n\n# The generated configuration includes:\n# - Auto-discovered database tools\n# - Default working directory\n# - Local target definitions\n</code></pre>"}, {"location": "commands/config/#cloud-integration-setup", "title": "Cloud Integration Setup", "text": "<pre><code># Initialize with cloud features\ndbsnapper config init\n\n# Add cloud authentication\ndbsnapper auth token &lt;your_token&gt;\n\n# Add storage profiles for cloud snapshots\n# (Edit configuration file to add storage_profiles section)\n</code></pre>"}, {"location": "commands/config/#teamenterprise-setup", "title": "Team/Enterprise Setup", "text": "<pre><code># Initialize with shared configuration\ndbsnapper config init --config /shared/config/dbsnapper.yml\n\n# Validate team configuration\ndbsnapper config validate\n\n# Ensure all team members have required tools\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config/#development-environment", "title": "Development Environment", "text": "<pre><code># Setup from DBSnapper repository\ncd /path/to/dbsnapper-repo\ndbsnapper config init  # Creates dbsnapper.yml in repo root\n\n# Enable development utilities\n# Edit config to set: feature_flags.development_utilities: true\n</code></pre>"}, {"location": "commands/config/#configuration-components", "title": "Configuration Components", "text": ""}, {"location": "commands/config/#database-tools-configuration", "title": "Database Tools Configuration", "text": "<ul> <li>Auto-detection: Automatically finds PostgreSQL and MySQL client tools</li> <li>Manual configuration: Override auto-detected paths</li> <li>Validation: Verifies tools are executable and compatible</li> <li>Updates: Re-discovery when tools are updated or moved</li> </ul>"}, {"location": "commands/config/#working-directory-management", "title": "Working Directory Management", "text": "<ul> <li>Creation: Automatic creation of working directory structure</li> <li>Permissions: Proper permissions for snapshot storage</li> <li>Location: Configurable location with sensible defaults</li> <li>Cleanup: Management of temporary files and old snapshots</li> </ul>"}, {"location": "commands/config/#cloud-integration-settings", "title": "Cloud Integration Settings", "text": "<ul> <li>Authentication: API token management for DBSnapper Cloud</li> <li>Storage profiles: S3-compatible cloud storage configuration</li> <li>Shared targets: Okta SSO integration for team collaboration</li> <li>Security: Encryption keys and secure credential storage</li> </ul>"}, {"location": "commands/config/#target-configuration", "title": "Target Configuration", "text": "<ul> <li>Database connections: Source and destination database URLs</li> <li>Schema filtering: PostgreSQL schema inclusion/exclusion rules</li> <li>Sanitization: Query files and sanitization settings</li> <li>Subsetting: Table subsetting and relationship configuration</li> </ul>"}, {"location": "commands/config/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "commands/config/#configuration-file-protection", "title": "Configuration File Protection", "text": "<pre><code># Set appropriate permissions\nchmod 600 ~/.config/dbsnapper/dbsnapper.yml\n\n# Verify permissions\nls -la ~/.config/dbsnapper/dbsnapper.yml\n# Should show: -rw------- (read/write for owner only)\n</code></pre>"}, {"location": "commands/config/#sensitive-data-management", "title": "Sensitive Data Management", "text": "<ul> <li>Secret key: Generated automatically, keep secure</li> <li>Database credentials: Use environment variables when possible</li> <li>API tokens: Store securely, rotate regularly</li> <li>Version control: Never commit configuration files with secrets</li> </ul>"}, {"location": "commands/config/#environment-variable-usage", "title": "Environment Variable Usage", "text": "<pre><code># Use environment variables for sensitive data\nexport DBSNAPPER_SECRET_KEY=\"your-secret-key\"\nexport DBSNAPPER_AUTHTOKEN=\"your-api-token\"\nexport DATABASE_URL=\"postgresql://user:pass@host:5432/db\"\n\n# Reference in configuration\ntargets:\n  my-target:\n    snapshot:\n      src_url: \"${DATABASE_URL}\"\n</code></pre>"}, {"location": "commands/config/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/config/#initial-setup", "title": "Initial Setup", "text": "<pre><code># Complete new installation setup\ndbsnapper config init\ndbsnapper config check\ndbsnapper config validate\n\n# Check results\ncat ~/.config/dbsnapper/dbsnapper.yml\n</code></pre>"}, {"location": "commands/config/#regular-maintenance", "title": "Regular Maintenance", "text": "<pre><code># Monthly configuration health check\ndbsnapper config check\n\n# After database software updates\ndbsnapper config discover --save\n\n# Before important operations\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config/#troubleshooting", "title": "Troubleshooting", "text": "<pre><code># Comprehensive troubleshooting\ndbsnapper config check --json &gt; config-status.json\ndbsnapper config validate --json &gt; config-validation.json\n\n# Fix issues and re-check\ndbsnapper config discover --save\ndbsnapper config check\n</code></pre>"}, {"location": "commands/config/#team-collaboration", "title": "Team Collaboration", "text": "<pre><code># Validate shared team configuration\ndbsnapper config --config /team/shared/dbsnapper.yml validate\n\n# Check tool availability across team\ndbsnapper config --config /team/shared/dbsnapper.yml check\n</code></pre>"}, {"location": "commands/config/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/config/#configuration-dependent-operations", "title": "Configuration-Dependent Operations", "text": "<p>Most DBSnapper commands depend on proper configuration:</p> <pre><code># These commands require valid configuration\ndbsnapper targets        # Needs target definitions\ndbsnapper build my-db    # Needs target and tool configuration\ndbsnapper sanitize my-db # Needs sanitization queries and tools\n</code></pre>"}, {"location": "commands/config/#configuration-verification-before-operations", "title": "Configuration Verification Before Operations", "text": "<pre><code># Verify configuration before critical operations\ndbsnapper config check &amp;&amp; dbsnapper build production-db\ndbsnapper config validate &amp;&amp; dbsnapper sanitize sensitive-data\n</code></pre>"}, {"location": "commands/config/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/config/#development-team-onboarding", "title": "Development Team Onboarding", "text": "<pre><code># New developer setup\ndbsnapper config init\n# Manual: Add team-specific targets to configuration\ndbsnapper config check    # Verify everything works\n</code></pre>"}, {"location": "commands/config/#cicd-pipeline-setup", "title": "CI/CD Pipeline Setup", "text": "<pre><code># Automated pipeline configuration\nDBSNAPPER_CONFIG=/ci/config/dbsnapper.yml dbsnapper config init\ndbsnapper config check --json | jq '.status' # Verify in scripts\n</code></pre>"}, {"location": "commands/config/#database-migration-projects", "title": "Database Migration Projects", "text": "<pre><code># Setup for migration project\ndbsnapper config init --config ./migration-config.yml\n# Manual: Configure source and destination databases\ndbsnapper config validate\n</code></pre>"}, {"location": "commands/config/#multi-environment-management", "title": "Multi-Environment Management", "text": "<pre><code># Separate configurations per environment\ndbsnapper config init --config ~/.config/dbsnapper/dev.yml\ndbsnapper config init --config ~/.config/dbsnapper/staging.yml\ndbsnapper config init --config ~/.config/dbsnapper/prod.yml\n\n# Validate each environment\nfor env in dev staging prod; do\n  dbsnapper config --config ~/.config/dbsnapper/$env.yml validate\ndone\n</code></pre>"}, {"location": "commands/config/#disaster-recovery-preparation", "title": "Disaster Recovery Preparation", "text": "<pre><code># Verify disaster recovery configuration\ndbsnapper config --config /backup/dr-config.yml check\ndbsnapper config --config /backup/dr-config.yml validate\n</code></pre>"}, {"location": "commands/config/#best-practices", "title": "Best Practices", "text": ""}, {"location": "commands/config/#configuration-management", "title": "Configuration Management", "text": "<ol> <li>Version control: Keep configuration templates in version control (without secrets)</li> <li>Environment separation: Use different config files for different environments</li> <li>Regular validation: Run <code>config check</code> regularly to catch issues early</li> <li>Documentation: Document custom configuration choices</li> <li>Backup: Keep secure backups of working configurations</li> </ol>"}, {"location": "commands/config/#security-best-practices", "title": "Security Best Practices", "text": "<ol> <li>File permissions: Restrict configuration file access to owner only</li> <li>Environment variables: Use env vars for sensitive data</li> <li>Secret rotation: Regularly rotate API tokens and secret keys</li> <li>Audit trail: Log configuration changes and access</li> <li>Principle of least privilege: Configure minimal required permissions</li> </ol>"}, {"location": "commands/config/#operational-excellence", "title": "Operational Excellence", "text": "<ol> <li>Health monitoring: Integrate <code>config check</code> into monitoring</li> <li>Change management: Test configuration changes before deployment</li> <li>Automation: Automate configuration validation in CI/CD</li> <li>Troubleshooting: Use JSON output for programmatic analysis</li> <li>Team coordination: Share configuration standards across team</li> </ol>"}, {"location": "commands/config/#troubleshooting-common-issues", "title": "Troubleshooting Common Issues", "text": ""}, {"location": "commands/config/#configuration-file-issues", "title": "Configuration File Issues", "text": "<pre><code># File not found\nError: configuration file not found\n# Solution: Run `dbsnapper config init` or specify correct path\n\n# Permission denied  \nError: permission denied reading configuration\n# Solution: Check file permissions with `ls -la`\n\n# Invalid YAML syntax\nError: configuration file syntax error\n# Solution: Run `dbsnapper config validate` for details\n</code></pre>"}, {"location": "commands/config/#database-tool-issues", "title": "Database Tool Issues", "text": "<pre><code># Tools not found\nWarning: PostgreSQL tools not found\n# Solution: Install tools, then run `dbsnapper config discover --save`\n\n# Tool version incompatible\nError: pg_dump version too old\n# Solution: Update PostgreSQL client tools\n</code></pre>"}, {"location": "commands/config/#permission-and-access-issues", "title": "Permission and Access Issues", "text": "<pre><code># Working directory creation failed\nError: cannot create working directory\n# Solution: Check parent directory permissions and disk space\n\n# Cloud authentication failed  \nError: invalid API token\n# Solution: Update token with `dbsnapper auth token &lt;new_token&gt;`\n</code></pre> <p>Regular Health Checks</p> <p>Run <code>dbsnapper config check</code> regularly to catch configuration issues before they impact operations. Consider adding it to your regular maintenance routine.</p> <p>Subcommand Details</p> <p>Each config subcommand has detailed documentation. Use <code>dbsnapper config [subcommand] --help</code> for specific options and examples.</p> <p>Configuration Security</p> <p>Never commit configuration files containing secrets to version control. Use environment variables or secure secret management systems for sensitive data.</p>"}, {"location": "commands/config/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>config init</code> - Initialize new DBSnapper configuration</li> <li><code>config check</code> - Check configuration and dependencies</li> <li><code>config discover</code> - Auto-discover database tools</li> <li><code>config validate</code> - Validate configuration syntax</li> </ul>"}, {"location": "commands/config/#see-also", "title": "See Also", "text": "<ul> <li>Configuration Reference - Complete configuration file reference</li> <li>Database Engines - Supported databases and client setup</li> <li>Installation Guide - DBSnapper installation instructions  </li> <li>Cloud Integration - Cloud features and setup</li> </ul>"}, {"location": "commands/load/", "title": "Load Command", "text": "<p>The <code>load</code> command restores database snapshots and executes SQL files in target databases. This versatile command supports both snapshot restoration and direct SQL file execution with comprehensive safety features and execution modes.</p>"}, {"location": "commands/load/#overview", "title": "Overview", "text": "<p>The load command handles two primary operations: 1. Snapshot Loading: Restores database snapshots created with the <code>build</code> command 2. SQL File Loading: Executes SQL files directly against target databases</p> <p>Both operations support advanced configuration options, safety features, and schema filtering for PostgreSQL databases.</p>"}, {"location": "commands/load/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper load &lt;target_name&gt; [snapshot_index|file.sql] [flags]\n</code></pre>"}, {"location": "commands/load/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> <li><code>[snapshot_index|file.sql]</code>: Optional snapshot index (default: 0 for most recent) or SQL file path</li> </ul>"}, {"location": "commands/load/#options", "title": "Options", "text": ""}, {"location": "commands/load/#command-specific-flags", "title": "Command-Specific Flags", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will overwrite any existing destination database\n      --dry-run         Show execution plan without running (SQL files only)\n      --no-drop         Skip dropping/recreating database (SQL files only)\n      --original        Use the original snapshot instead of the sanitized version (snapshots only)\n      --source          Load SQL file to source database instead of destination\n  -h, --help           Show help for the load command\n</code></pre>"}, {"location": "commands/load/#global-flags", "title": "Global Flags", "text": "<pre><code>      --config string   Config file (default is ~/.config/dbsnapper/dbsnapper.yml)\n      --nocloud         Disable cloud mode to speed up operations by skipping cloud API calls\n</code></pre>"}, {"location": "commands/load/#target-configuration", "title": "Target Configuration", "text": "<p>The load command uses the same target configuration as the build command. Here's what you need:</p>"}, {"location": "commands/load/#required-configuration", "title": "Required Configuration", "text": "<p>Minimal Target Setup:</p> <pre><code>targets:\n  my-target:\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/dbname\"\n      dst_url: \"postgresql://user:pass@dev:5432/dbname_copy\"\n</code></pre>"}, {"location": "commands/load/#complete-target-configuration", "title": "Complete Target Configuration", "text": "<p>Target with Schema Filtering:</p> <pre><code>targets:\n  postgres-target:\n    name: \"PostgreSQL with Schema Control\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_dev\"\n      schema_config:\n        include_schemas: [\"public\", \"app_data\", \"reports\"]\n        # Exclude temporary and debug schemas from loading\n</code></pre>"}, {"location": "commands/load/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/load/#snapshot-loading-examples", "title": "Snapshot Loading Examples", "text": "<pre><code># Basic snapshot operations\ndbsnapper load production-db              # Load latest snapshot\ndbsnapper load production-db 5            # Load snapshot at index 5\ndbsnapper load production-db --original   # Load original (non-sanitized) snapshot\n\n# Custom destination\ndbsnapper load prod-db --destdb \"postgresql://user:pass@test-host:5432/test_db\"\n</code></pre>"}, {"location": "commands/load/#sql-file-loading-examples", "title": "SQL File Loading Examples", "text": "<pre><code># Basic SQL file operations\ndbsnapper load dev-db schema.sql          # Load schema to dst_url\ndbsnapper load dev-db data.sql --source   # Load data to src_url\ndbsnapper load dev-db migration.sql --dry-run  # Preview migration\n\n# Advanced SQL file operations\ndbsnapper load prod-db update.sql --no-drop    # Execute without dropping database\ndbsnapper load staging-db backup.sql --destdb \"postgres://user:pass@host/custom_db\"\n</code></pre>"}, {"location": "commands/load/#automation-examples", "title": "Automation Examples", "text": "<pre><code># CI/CD pipeline usage\nCI=true dbsnapper load production-db\nDBSNAPPER_NO_CONFIRM=true dbsnapper load staging-db migration.sql --no-drop\n\n# Batch processing\nfor file in migrations/*.sql; do\n  dbsnapper load dev-db \"$file\" --no-drop\ndone\n</code></pre>"}, {"location": "commands/load/#how-it-works", "title": "How It Works", "text": ""}, {"location": "commands/load/#snapshot-loading-process", "title": "Snapshot Loading Process", "text": "<ol> <li>Snapshot Selection: Identifies the snapshot to load based on index (0 = most recent)</li> <li>Database Preparation: Drops and recreates the destination database</li> <li>Schema Analysis: For PostgreSQL, analyzes snapshot contents and applies schema filtering</li> <li>Data Restoration: Restores schema, data, indexes, and constraints from the snapshot</li> <li>Verification: Confirms successful restoration</li> </ol>"}, {"location": "commands/load/#sql-file-loading-process", "title": "SQL File Loading Process", "text": "<ol> <li>File Detection: Automatically detects <code>.sql</code> files by extension</li> <li>Database Preparation: Optionally drops/recreates database (unless <code>--no-drop</code>)</li> <li>SQL Execution: Executes SQL file against target database</li> <li>Transaction Management: Wraps execution in transactions for rollback capability</li> </ol>"}, {"location": "commands/load/#snapshot-priority-system", "title": "Snapshot Priority System", "text": "<p>DBSnapper automatically prioritizes sanitized snapshots for safety:</p> <ol> <li>Sanitized Snapshot: Loads <code>&lt;timestamp&gt;_&lt;target_name&gt;.san.zip</code> if available</li> <li>Original Snapshot: Loads <code>&lt;timestamp&gt;_&lt;target_name&gt;.zip</code> if no sanitized version exists  </li> <li>Override: Use <code>--original</code> flag to force loading the original snapshot</li> </ol>"}, {"location": "commands/load/#sql-file-operations", "title": "SQL File Operations", "text": ""}, {"location": "commands/load/#file-detection", "title": "File Detection", "text": "<p>SQL files are automatically detected by the <code>.sql</code> extension (case-insensitive): - <code>schema.sql</code>, <code>data.SQL</code>, <code>migration.Sql</code> are all valid - Non-SQL files will be treated as snapshot indices</p>"}, {"location": "commands/load/#execution-modes", "title": "Execution Modes", "text": "<p>Replace Mode (default) - Drops and recreates the target database before loading - Ensures clean database state - Use for complete database refreshes</p> <p>Append Mode (<code>--no-drop</code>) - Executes SQL against the existing database without dropping it - Preserves existing data and structure - Use for incremental updates, migrations, or data additions</p>"}, {"location": "commands/load/#database-url-resolution-priority", "title": "Database URL Resolution Priority", "text": "<p>Database URL resolution follows this priority order:</p> <ol> <li><code>--destdb</code> flag (highest priority)</li> <li><code>--source</code> flag (determines src_url vs dst_url)</li> <li>Configuration overrides</li> <li>Target configuration defaults</li> </ol>"}, {"location": "commands/load/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>When loading snapshots, the load command respects schema configuration for PostgreSQL databases:</p>"}, {"location": "commands/load/#schema-configuration-behavior", "title": "Schema Configuration Behavior", "text": "Configuration Load Behavior Notes No <code>schema_config</code> Loads <code>public</code> schema only Default behavior <code>use_default_schema: true</code> Loads default schema from snapshot Explicit default <code>use_default_schema: false</code> Loads all schemas present in snapshot Multi-schema applications <code>include_schemas: [\"public\", \"app\"]</code> Loads only listed schemas from snapshot Selective inclusion <code>exclude_schemas: [\"temp\", \"logs\"]</code> Loads all schemas except excluded ones Exclude temporary data"}, {"location": "commands/load/#schema-configuration-examples", "title": "Schema Configuration Examples", "text": "<pre><code>targets:\n  postgres-target:\n    snapshot:\n      schema_config:\n        # Option 1: Load all schemas from snapshot\n        use_default_schema: false\n\n        # Option 2: Load only specific schemas\n        # include_schemas: [\"public\", \"app_data\"]\n\n        # Option 3: Load all except excluded schemas  \n        # exclude_schemas: [\"temp_schema\", \"debug_logs\"]\n</code></pre>"}, {"location": "commands/load/#important-schema-notes", "title": "Important Schema Notes", "text": "<ul> <li>PostgreSQL Only: Schema filtering only applies to PostgreSQL snapshot loading</li> <li>Snapshot Contents: Only schemas that exist in the snapshot will be loaded</li> <li>No Empty Schemas: No empty schemas are created if they don't exist in the snapshot</li> <li>Warnings Displayed: If requested schemas don't exist in the snapshot, warnings are shown</li> <li>MySQL Limitation: MySQL databases use standard restore without schema filtering</li> <li>SQL Files Excluded: Schema filtering does not apply to SQL file loading</li> </ul>"}, {"location": "commands/load/#safety-features", "title": "Safety Features", "text": "<p>The load command includes comprehensive safety features:</p>"}, {"location": "commands/load/#interactive-confirmation", "title": "Interactive Confirmation", "text": "<ul> <li>Prompts before destructive operations</li> <li>Shows operation details and target information</li> <li>Can be disabled for automation</li> </ul>"}, {"location": "commands/load/#dry-run-mode", "title": "Dry Run Mode", "text": "<ul> <li>Preview SQL execution without making changes (<code>--dry-run</code>)</li> <li>Shows execution plan and affected objects</li> <li>Available for SQL file loading only</li> </ul>"}, {"location": "commands/load/#file-validation", "title": "File Validation", "text": "<ul> <li>Validates SQL files before execution</li> <li>Checks file existence and readability</li> <li>Prevents execution of invalid files</li> </ul>"}, {"location": "commands/load/#transaction-support", "title": "Transaction Support", "text": "<ul> <li>Wraps SQL execution in transactions for rollback capability</li> <li>Ensures atomicity of operations</li> <li>Provides rollback on errors</li> </ul>"}, {"location": "commands/load/#confirmation-control", "title": "Confirmation Control", "text": "<p>Skip confirmations in automated environments:</p> <pre><code># Environment variable (recommended for CI/CD)\nexport CI=true\ndbsnapper load target_name file.sql\n\n# Alternative environment variable\nexport DBSNAPPER_NO_CONFIRM=true\ndbsnapper load target_name file.sql\n</code></pre>"}, {"location": "commands/load/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/load/#development-environment-setup", "title": "Development Environment Setup", "text": "<pre><code># Refresh development database with latest production snapshot\ndbsnapper build myapp-prod\ndbsnapper load myapp-dev\n</code></pre>"}, {"location": "commands/load/#testing-with-clean-data", "title": "Testing with Clean Data", "text": "<pre><code># Load sanitized snapshot for testing\ndbsnapper load test-env 0\n</code></pre>"}, {"location": "commands/load/#database-migrations", "title": "Database Migrations", "text": "<pre><code># Preview migration before applying\ndbsnapper load staging-db migration_v2.sql --dry-run\n\n# Apply migration without dropping database\ndbsnapper load staging-db migration_v2.sql --no-drop\n</code></pre>"}, {"location": "commands/load/#feature-branch-testing", "title": "Feature Branch Testing", "text": "<pre><code># Use specific snapshot state for consistent testing\ndbsnapper load feature-branch 3\n</code></pre>"}, {"location": "commands/load/#schema-management", "title": "Schema Management", "text": "<pre><code># Load only application schemas, excluding logs and temp data\n# (configured via schema_config in target definition)\ndbsnapper load filtered-target\n</code></pre>"}, {"location": "commands/load/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/load/#common-issues", "title": "Common Issues", "text": "<p>Snapshot Not Found <pre><code>Error: snapshot index 5 not found for target myapp-dev\n</code></pre> - List available snapshots with <code>dbsnapper target myapp-dev</code> - Use valid snapshot index (0 = most recent)</p> <p>Database Connection Failed <pre><code>Error: failed to connect to destination database\n</code></pre> - Verify <code>dst_url</code> or <code>--destdb</code> connection string - Check database server availability - Confirm credentials and permissions</p> <p>Permission Denied <pre><code>Error: insufficient privileges to drop/create database\n</code></pre> - Ensure database user has CREATE/DROP privileges - Check database server permissions - Verify connection string includes correct credentials</p> <p>SQL File Not Found <pre><code>Error: SQL file not found: missing.sql\n</code></pre> - Verify file path is correct - Check file exists and is readable - Use absolute or relative paths as appropriate</p> <p>Schema Configuration Errors <pre><code>Warning: schema 'non_existent' specified but not found in snapshot\n</code></pre> - Verify schema names in configuration - Check that schemas exist in the original snapshot - Review schema configuration syntax</p>"}, {"location": "commands/load/#recovery-steps", "title": "Recovery Steps", "text": "<ol> <li>Verify Configuration: Check target configuration and connection strings</li> <li>Test Connectivity: Ensure database servers are accessible</li> <li>Check Permissions: Verify database user privileges</li> <li>Validate Files: Confirm SQL files exist and are readable</li> <li>Use Dry Run: Preview operations with <code>--dry-run</code> flag</li> </ol> <p>Destructive Operation</p> <p>The load command will drop and recreate the destination database by default. All existing data in the destination database will be permanently lost unless using <code>--no-drop</code> flag with SQL files.</p> <p>Sanitized Snapshots Priority</p> <p>If a sanitized snapshot exists at the specified index, it will be loaded unless the <code>--original</code> flag is set. This ensures sensitive data protection by default.</p> <p>Database URL Priority</p> <p>Database URL resolution follows this priority: 1. <code>--destdb</code> flag (highest priority) 2. <code>--source</code> flag (determines src_url vs dst_url) 3. Configuration overrides 4. Target configuration defaults</p>"}, {"location": "commands/load/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>build</code> - Create new database snapshots</li> <li><code>sanitize</code> - Create sanitized versions of snapshots</li> <li><code>target</code> - View target details and available snapshots</li> <li><code>targets</code> - List all available targets</li> </ul>"}, {"location": "commands/load/#see-also", "title": "See Also", "text": "<ul> <li>Configuration - Target and schema configuration</li> <li>Database Engines - Supported databases</li> <li>Snapshot Management - Snapshot concepts and workflows</li> <li>Sanitization - Data sanitization features</li> </ul>"}, {"location": "commands/mcp/", "title": "MCP Command", "text": "<p>The <code>mcp</code> command starts the Model Context Protocol (MCP) server, enabling seamless integration between DBSnapper and AI applications like Claude Desktop. This powerful integration allows you to manage database snapshots, targets, and configurations through natural language conversations with AI assistants.</p>"}, {"location": "commands/mcp/#overview", "title": "Overview", "text": "<p>The MCP command provides AI application integration: - Model Context Protocol: Standards-based integration with LLM applications - Tool exposure: Makes DBSnapper operations available as AI tools - Resource access: Provides structured access to configuration and snapshot data - Natural language interface: Control DBSnapper through conversational AI - Automated workflows: Enable AI-assisted database management tasks</p>"}, {"location": "commands/mcp/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper mcp [flags]\n</code></pre>"}, {"location": "commands/mcp/#arguments", "title": "Arguments", "text": "<p>None - the MCP server runs without additional arguments.</p>"}, {"location": "commands/mcp/#options", "title": "Options", "text": "<pre><code>  -h, --help                     help for mcp\n      --path string             HTTP endpoint path (default \"/mcp\")\n      --port int                HTTP server port (default 8080)\n      --stdio                   Use stdio transport (legacy mode for Claude Code)\n  -t, --transport string        Transport type: http, sse, or stdio (default \"http\")\n\nGlobal Flags:\n      --config string   config file (default is ~/.config/dbsnapper/dbsnapper.yml)\n      --nocloud         Disable cloud mode to speed up operations by skipping cloud API calls\n</code></pre>"}, {"location": "commands/mcp/#what-is-model-context-protocol-mcp", "title": "What is Model Context Protocol (MCP)?", "text": "<p>The Model Context Protocol is a standardized way for AI applications to access external tools and data sources. The DBSnapper MCP Server exposes DBSnapper's functionality through this protocol, enabling AI assistants to:</p>"}, {"location": "commands/mcp/#core-capabilities", "title": "Core Capabilities", "text": "<ul> <li>Database operations: Create, load, and manage database snapshots</li> <li>Target management: List and inspect database targets</li> <li>Configuration access: View and validate DBSnapper configuration</li> <li>Tool discovery: Automatically discover and configure database client tools</li> <li>Resource browsing: Access snapshot metadata and build history</li> </ul>"}, {"location": "commands/mcp/#integration-benefits", "title": "Integration Benefits", "text": "<ul> <li>Natural language control: Use conversational language instead of command-line syntax</li> <li>Context awareness: AI understands your database environment and suggests appropriate actions</li> <li>Automated workflows: Chain multiple operations together intelligently</li> <li>Error assistance: Get intelligent help with troubleshooting and configuration issues</li> </ul>"}, {"location": "commands/mcp/#how-it-works", "title": "How It Works", "text": ""}, {"location": "commands/mcp/#mcp-server-architecture", "title": "MCP Server Architecture", "text": "<ol> <li>Multiple Transport Options: HTTP (default), Server-Sent Events (SSE), or stdio communication</li> <li>Tool Registry: Exposes DBSnapper commands as callable tools</li> <li>Resource System: Provides structured access to configuration and data</li> <li>State Management: Maintains connection to DBSnapper configuration and working directory</li> </ol>"}, {"location": "commands/mcp/#integration-flow", "title": "Integration Flow", "text": "<pre><code>AI Application / Web Client\n        \u2195 HTTP/SSE/stdio (JSON-RPC)\nDBSnapper MCP Server\n        \u2195 direct calls\nDBSnapper Core Operations\n        \u2195 database connections\nYour Databases\n</code></pre>"}, {"location": "commands/mcp/#security-model", "title": "Security Model", "text": "<ul> <li>Credential sanitization: Automatically removes sensitive information from AI responses</li> <li>Read-only access: AI cannot modify configuration files directly</li> <li>Operation permissions: All operations go through DBSnapper's existing security mechanisms</li> <li>No direct database access: AI cannot bypass DBSnapper's authentication and access controls</li> </ul>"}, {"location": "commands/mcp/#setup-and-configuration", "title": "Setup and Configuration", "text": ""}, {"location": "commands/mcp/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>DBSnapper Agent installed and configured</li> <li>At least one database target configured</li> <li>AI application supporting MCP (like Claude Desktop)</li> </ul>"}, {"location": "commands/mcp/#integration-options", "title": "Integration Options", "text": ""}, {"location": "commands/mcp/#http-integration-default-recommended", "title": "HTTP Integration (Default - Recommended)", "text": "<p>The MCP server now defaults to HTTP transport, making it accessible from web applications, APIs, and other HTTP clients:</p> <pre><code># Start HTTP server (default)\ndbsnapper mcp\n\n# Custom port and path\ndbsnapper mcp --port 3001 --path /api/mcp\n\n# Server-Sent Events transport\ndbsnapper mcp --transport sse\n</code></pre> <p>HTTP Access: - MCP endpoint: <code>http://localhost:8080/mcp</code> - Server info: <code>http://localhost:8080/</code></p>"}, {"location": "commands/mcp/#claude-desktop-integration-stdio-recommended", "title": "Claude Desktop Integration (Stdio - Recommended)", "text": "<p>Claude Desktop currently works best with stdio transport for local MCP servers:</p> <ol> <li>Locate Claude Desktop Configuration</li> <li>macOS: <code>~/Library/Application Support/Claude/claude_desktop_config.json</code></li> <li>Windows: <code>%APPDATA%\\Claude\\claude_desktop_config.json</code> </li> <li> <p>Linux: <code>~/.config/claude-desktop/claude_desktop_config.json</code></p> </li> <li> <p>Add DBSnapper MCP Server (Stdio Mode) <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"dbsnapper\",\n      \"args\": [\"mcp\", \"--stdio\"],\n      \"description\": \"DBSnapper database snapshot management\"\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Restart Claude Desktop and start a new conversation</p> </li> </ol>"}, {"location": "commands/mcp/#claude-code-integration-http-recommended", "title": "Claude Code Integration (HTTP - Recommended)", "text": "<p>Claude Code supports HTTP transport with streaming capabilities for local MCP servers:</p> <ol> <li> <p>Start DBSnapper HTTP server: <code>dbsnapper mcp</code> (runs on <code>http://localhost:8080/mcp</code>)</p> </li> <li> <p>Create Claude Code MCP configuration (<code>.mcp.json</code> in your project root):    <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper-http\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8080/mcp\"\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Start using DBSnapper in Claude Code with natural language commands</p> </li> </ol> <p>Benefits: - Real-time HTTP streaming support - Better performance for complex operations - Built-in debugging capabilities - Web-standard protocols</p>"}, {"location": "commands/mcp/#streamable-http-with-mcp-remote-adapter", "title": "Streamable HTTP with mcp-remote Adapter", "text": "<p>For clients that don't yet support native Streamable HTTP, you can use the <code>mcp-remote</code> adapter to connect any MCP client to DBSnapper's HTTP server:</p> <p>What is mcp-remote? The <code>mcp-remote</code> adapter enables MCP clients like Claude Desktop (which otherwise only support local connections) to work with remote MCP servers using the new Streamable HTTP transport.</p> <p>Setup Instructions:</p> <ol> <li> <p>Install mcp-remote adapter:    <pre><code>npm install -g mcp-remote\n</code></pre></p> </li> <li> <p>Start DBSnapper HTTP server:     <pre><code>dbsnapper mcp  # runs on http://localhost:8080/mcp\n</code></pre></p> </li> <li> <p>Configure your MCP client with mcp-remote:    <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper-remote\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"http://localhost:8080/mcp\"],\n      \"description\": \"DBSnapper via Streamable HTTP adapter\"\n    }\n  }\n}\n</code></pre></p> </li> </ol> <p>Benefits: - Enables Streamable HTTP for any MCP client - Better performance than traditional stdio transport - Future-proof as clients adopt native Streamable HTTP - Real-time streaming capabilities</p> <p>Use Cases: - Testing Streamable HTTP with Claude Desktop - Connecting legacy MCP clients to modern HTTP servers - Bridge between traditional MCP and new HTTP transport</p>"}, {"location": "commands/mcp/#advanced-configuration-options", "title": "Advanced Configuration Options", "text": "<p>Full Path Configuration (Stdio - Recommended for Claude Desktop): <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"/usr/local/bin/dbsnapper\",\n      \"args\": [\"mcp\", \"--stdio\"],\n      \"description\": \"DBSnapper Agent MCP Server\",\n      \"env\": {\n        \"DBSNAPPER_CONFIG\": \"/etc/dbsnapper/config.yml\",\n        \"DBSNAPPER_LOG_LEVEL\": \"info\"\n      }\n    }\n  }\n}\n</code></pre></p> <p>Debug Mode Configuration (Stdio): <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"/path/to/dbsnapper\",\n      \"args\": [\"mcp\", \"--stdio\"],\n      \"description\": \"DBSnapper with debug logging\",\n      \"env\": {\n        \"DBSNAPPER_HTTP_DEBUG\": \"true\",\n        \"DBSNAPPER_LOG_LEVEL\": \"debug\"\n      }\n    }\n  }\n}\n</code></pre></p> <p>Database Tools Configuration (Stdio): <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"dbsnapper\",\n      \"args\": [\"mcp\", \"--stdio\"],\n      \"description\": \"DBSnapper with PostgreSQL tools\",\n      \"env\": {\n        \"PATH\": \"/Applications/Postgres.app/Contents/Versions/latest/bin:/opt/homebrew/bin:/usr/local/bin:/usr/bin:/bin\"\n      }\n    }\n  }\n}\n</code></pre></p>"}, {"location": "commands/mcp/#environment-variables", "title": "Environment Variables", "text": "<ul> <li><code>DBSNAPPER_CONFIG</code>: Custom configuration file path</li> <li><code>DBSNAPPER_WORKING_DIR</code>: Override working directory location</li> <li><code>DBSNAPPER_LOG_LEVEL</code>: Set logging verbosity (debug, info, warn, error)</li> <li><code>DBSNAPPER_HTTP_DEBUG</code>: Enable detailed HTTP debugging (true/false)</li> <li><code>DBSNAPPER_MCP_MODE</code>: Enable MCP-specific features (automatically set when using mcp command)</li> <li><code>PATH</code>: Include database client tool directories</li> </ul>"}, {"location": "commands/mcp/#transport-methods", "title": "Transport Methods", "text": ""}, {"location": "commands/mcp/#stdio-transport-recommended-for-claude-desktop", "title": "Stdio Transport (Recommended for Claude Desktop)", "text": "<p>Best for: Claude Desktop, legacy direct AI assistant integration</p> <p>Benefits: - Direct process-based communication - No network ports or certificates needed - Native Claude Desktop support - Lower resource usage - Simple configuration</p> <p>Usage: <pre><code># Stdio transport for Claude Desktop\ndbsnapper mcp --stdio\n</code></pre></p>"}, {"location": "commands/mcp/#http-transport-default-best-for-apiswebclaude-code", "title": "HTTP Transport (Default - Best for APIs/Web/Claude Code)", "text": "<p>Best for: Claude Code, web applications, APIs, development integrations, debugging</p> <p>Benefits: - Accessible from any HTTP client - RESTful endpoints for integration - Better debugging and monitoring - Easy testing with curl or Postman - Future-proof for remote MCP servers</p> <p>Usage: <pre><code># Default HTTP transport\ndbsnapper mcp\n\n# Custom port and endpoint\ndbsnapper mcp --port 3001 --path /api/dbsnapper\n</code></pre></p> <p>Claude Desktop Usage (Experimental): Requires <code>mcp-remote</code> adapter: <code>npx mcp-remote http://localhost:8080/mcp</code></p>"}, {"location": "commands/mcp/#server-sent-events-sse-transport", "title": "Server-Sent Events (SSE) Transport", "text": "<p>Best for: Real-time applications, streaming data needs</p> <p>Benefits: - Real-time streaming capabilities - Event-driven architecture - Web-compatible streaming - Lower latency for continuous operations</p> <p>Usage: <pre><code># SSE transport\ndbsnapper mcp --transport sse --port 8080\n</code></pre></p>"}, {"location": "commands/mcp/#available-tools-and-resources", "title": "Available Tools and Resources", "text": ""}, {"location": "commands/mcp/#target-management-tools", "title": "Target Management Tools", "text": ""}, {"location": "commands/mcp/#list-targets-list_targets", "title": "List Targets (<code>list_targets</code>)", "text": "<p>Lists all configured database targets with filtering options.</p> <p>Parameters: - <code>includeShared</code> (boolean): Include shared targets from cloud/SSO - <code>format</code> (string): Output format - \"json\" or \"table\"</p> <p>AI Usage Examples: - \"Show me all my database targets\" - \"List targets including shared ones\" - \"Display targets in table format\"</p>"}, {"location": "commands/mcp/#target-details-get_target_details", "title": "Target Details (<code>get_target_details</code>)", "text": "<p>Provides comprehensive information about a specific target.</p> <p>Parameters: - <code>targetName</code> (string, required): Name of the target - <code>format</code> (string): Output format - \"json\" or \"table\"</p> <p>AI Usage Examples: - \"Show details for my production database\" - \"Get information about the analytics-db target\" - \"Display target configuration for staging-env\"</p>"}, {"location": "commands/mcp/#snapshot-management-tools", "title": "Snapshot Management Tools", "text": ""}, {"location": "commands/mcp/#build-snapshot-build_snapshot", "title": "Build Snapshot (<code>build_snapshot</code>)", "text": "<p>Creates a new database snapshot for specified target.</p> <p>Parameters: - <code>targetName</code> (string, required): Target to snapshot - <code>timestamp</code> (integer): Custom timestamp for naming - <code>format</code> (string): Output format preference</p> <p>AI Usage Examples: - \"Create a snapshot of my production database\" - \"Build a snapshot for the sales-db target\" - \"Take a backup of the development environment\"</p>"}, {"location": "commands/mcp/#load-snapshot-load_snapshot", "title": "Load Snapshot (<code>load_snapshot</code>)", "text": "<p>Loads a database snapshot into target database.</p> <p>Parameters: - <code>targetName</code> (string, required): Target name - <code>snapshotIndex</code> (integer): Snapshot index (0 for latest) - <code>destinationUrl</code> (string): Override destination URL - <code>useOriginal</code> (boolean): Use original vs sanitized snapshot - <code>format</code> (string): Output format preference</p> <p>AI Usage Examples: - \"Load the latest production snapshot into staging\" - \"Restore snapshot index 2 for the test database\" - \"Load the original (unsanitized) snapshot for development\"</p>"}, {"location": "commands/mcp/#list-snapshots-list_snapshots", "title": "List Snapshots (<code>list_snapshots</code>)", "text": "<p>Shows available snapshots with metadata and statistics.</p> <p>Parameters: - <code>targetName</code> (string): Filter for specific target - <code>format</code> (string): Output format preference</p> <p>AI Usage Examples: - \"Show me all available snapshots\" - \"List snapshots for the production target\" - \"Display snapshot history with sizes and dates\"</p>"}, {"location": "commands/mcp/#configuration-management-tools", "title": "Configuration Management Tools", "text": ""}, {"location": "commands/mcp/#database-tool-discovery-discover_database_tools", "title": "Database Tool Discovery (<code>discover_database_tools</code>)", "text": "<p>Automatically finds and configures database client tools.</p> <p>Parameters: - <code>autoSave</code> (boolean): Save tool paths to configuration</p> <p>AI Usage Examples: - \"Find and configure my database tools\" - \"Discover PostgreSQL and MySQL clients\" - \"Set up database tools automatically\"</p> <p>Key Benefits: - Solves \"executable file not found in $PATH\" errors - Finds tools like <code>pg_dump</code>, <code>pg_restore</code>, <code>psql</code>, <code>mysql</code>, <code>mysqldump</code> - Saves configuration permanently for future use - No manual PATH configuration required</p>"}, {"location": "commands/mcp/#validate-database-tools-validate_database_tools", "title": "Validate Database Tools (<code>validate_database_tools</code>)", "text": "<p>Checks current database tools configuration status.</p> <p>AI Usage Examples: - \"Check my database tools setup\" - \"Validate PostgreSQL client configuration\" - \"Show database tool status\"</p>"}, {"location": "commands/mcp/#configuration-information-get_config_info", "title": "Configuration Information (<code>get_config_info</code>)", "text": "<p>Provides comprehensive configuration file and setup information.</p> <p>Parameters: - <code>format</code> (string): Output format - \"json\" or \"table\"</p> <p>AI Usage Examples: - \"What configuration file is DBSnapper using?\" - \"Show my DBSnapper setup information\" - \"Display configuration details in JSON format\"</p>"}, {"location": "commands/mcp/#advanced-operations", "title": "Advanced Operations", "text": ""}, {"location": "commands/mcp/#sanitize-snapshot-sanitize_snapshot", "title": "Sanitize Snapshot (<code>sanitize_snapshot</code>)", "text": "<p>Creates sanitized snapshot with sensitive data removed.</p> <p>Parameters: - <code>targetName</code> (string, required): Target name - <code>snapshotIndex</code> (integer): Snapshot index to sanitize - <code>ephemeral</code> (boolean): Force ephemeral sanitization mode - <code>newSet</code> (boolean): Create new snapshot set - <code>format</code> (string): Output format preference</p> <p>AI Usage Examples: - \"Sanitize the latest production snapshot\" - \"Create a sanitized version for sharing\" - \"Remove sensitive data from snapshot index 1\"</p>"}, {"location": "commands/mcp/#mcp-resources", "title": "MCP Resources", "text": ""}, {"location": "commands/mcp/#configuration-resources", "title": "Configuration Resources", "text": ""}, {"location": "commands/mcp/#dbsnapperconfig", "title": "<code>dbsnapper://config</code>", "text": "<p>Provides sanitized DBSnapper configuration including: - Target configurations (credentials automatically sanitized) - Storage profiles (sensitive data removed) - Working directory and settings - Cloud integration status</p>"}, {"location": "commands/mcp/#dbsnappertargets", "title": "<code>dbsnapper://targets</code>", "text": "<p>Lists all configured targets with summary information: - Target names and types (local, cloud, shared) - Database engines and connection status - Snapshot counts and latest activity - Configuration validation status</p>"}, {"location": "commands/mcp/#snapshot-resources", "title": "Snapshot Resources", "text": ""}, {"location": "commands/mcp/#dbsnappersnapshots", "title": "<code>dbsnapper://snapshots</code>", "text": "<p>Comprehensive snapshot information: - File names, sizes, and creation timestamps - Original vs sanitized snapshot indicators - Storage locations (local and cloud) - Metadata and build statistics</p>"}, {"location": "commands/mcp/#dbsnapperbuild-history", "title": "<code>dbsnapper://build-history</code>", "text": "<p>Historical snapshot build information: - Build success/failure status - Operation timing and performance metrics - Error logs and troubleshooting information - Target usage patterns</p>"}, {"location": "commands/mcp/#example-usage-scenarios", "title": "Example Usage Scenarios", "text": ""}, {"location": "commands/mcp/#getting-started-with-ai-integration", "title": "Getting Started with AI Integration", "text": ""}, {"location": "commands/mcp/#initial-setup-and-discovery", "title": "Initial Setup and Discovery", "text": "<pre><code>User: \"What database targets do I have configured?\"\nAI: I'll check your configured database targets.\n\n[AI uses list_targets tool]\n\nAI: You have 3 targets configured:\n- production-db (PostgreSQL, cloud target)\n- staging-db (PostgreSQL, local target)  \n- analytics-db (MySQL, local target)\n\nWould you like to see detailed information about any of these targets?\n</code></pre>"}, {"location": "commands/mcp/#database-tool-setup", "title": "Database Tool Setup", "text": "<pre><code>User: \"I'm getting errors about pg_dump not being found\"\nAI: I can help you set up your database tools automatically.\n\n[AI uses discover_database_tools with autoSave: true]\n\nAI: \u2705 Database tools discovered and configured:\n- PostgreSQL: pg_dump, pg_restore, psql found at /opt/homebrew/bin/\n- MySQL: mysql, mysqldump found at /usr/local/bin/\n- Configuration saved to your DBSnapper config file\n\nYour database tools are now properly configured for future operations.\n</code></pre>"}, {"location": "commands/mcp/#snapshot-management-workflows", "title": "Snapshot Management Workflows", "text": ""}, {"location": "commands/mcp/#creating-production-snapshots", "title": "Creating Production Snapshots", "text": "<pre><code>User: \"Create a snapshot of my production database and load it into staging\"\nAI: I'll create a production snapshot and then load it into staging.\n\n[AI uses build_snapshot with targetName: \"production-db\"]\n\nAI: \u2705 Production snapshot created successfully!\n- Snapshot file: /snapshots/1705123456_production-db.zip\n- Size: 2.1 GB\n- Build time: 45.2 seconds\n\n[AI uses load_snapshot with targetName: \"production-db\", destinationUrl from staging config]\n\nAI: \u2705 Snapshot loaded into staging environment!\n- Loaded sanitized snapshot (index 0)\n- Destination: staging-db  \n- Load time: 12.3 seconds\n\nYour staging environment now has the latest production data.\n</code></pre>"}, {"location": "commands/mcp/#snapshot-history-and-analysis", "title": "Snapshot History and Analysis", "text": "<pre><code>User: \"Show me the history of snapshots for my production database\"\nAI: I'll get the snapshot history for your production database.\n\n[AI uses list_snapshots with targetName: \"production-db\"]\n\nAI: Here's your production-db snapshot history:\n- Index 0 (latest): 1705123456_production-db.zip (2.1 GB, sanitized available)\n- Index 1: 1705037056_production-db.zip (2.0 GB, sanitized available)  \n- Index 2: 1704950656_production-db.zip (1.9 GB, sanitized available)\n\nAll snapshots have sanitized versions available for safe sharing.\nWould you like to load any specific snapshot?\n</code></pre>"}, {"location": "commands/mcp/#configuration-management", "title": "Configuration Management", "text": ""}, {"location": "commands/mcp/#configuration-validation", "title": "Configuration Validation", "text": "<pre><code>User: \"Is my DBSnapper configuration set up correctly?\"\nAI: I'll check your DBSnapper configuration and setup.\n\n[AI uses get_config_info and validate_database_tools]\n\nAI: Your DBSnapper configuration looks good:\n\u2705 Configuration file: ~/.config/dbsnapper/dbsnapper.yml\n\u2705 Working directory: /Users/you/.dbsnapper  \n\u2705 Database tools: PostgreSQL and MySQL tools configured\n\u2705 Cloud mode: Enabled with valid authentication\n\u2705 Targets: 3 configured (2 local, 1 cloud)\n\u2705 Storage profiles: 1 S3 profile configured\n\nEverything is properly configured for database snapshot operations.\n</code></pre>"}, {"location": "commands/mcp/#multi-environment-setup", "title": "Multi-Environment Setup", "text": "<pre><code>User: \"Help me set up snapshots for my development workflow\"\nAI: I'll help you create an efficient development snapshot workflow.\n\n[AI uses list_targets to understand environment]\n\nAI: I see you have production-db and staging-db targets. Here's a recommended workflow:\n\n1. Create daily production snapshots (sanitized for security)\n2. Load sanitized snapshots into staging for testing\n3. Use sanitized data for development environments\n\nWould you like me to create a sanitized snapshot from production now?\n\n[User agrees]\n\n[AI uses build_snapshot then sanitize_snapshot]\n\nAI: \u2705 Development-ready data prepared:\n- Original production snapshot: 1705123456_production-db.zip\n- Sanitized version: 1705123456_production-db.san.zip  \n- Sensitive data removed, ready for development use\n\nYou can now safely use this data across development environments.\n</code></pre>"}, {"location": "commands/mcp/#advanced-workflows", "title": "Advanced Workflows", "text": ""}, {"location": "commands/mcp/#automated-testing-setup", "title": "Automated Testing Setup", "text": "<pre><code>User: \"I need to set up automated testing with fresh data each day\"\nAI: I'll help you understand how to set up automated testing with fresh snapshots.\n\n[AI uses get_config_info and list_snapshots]\n\nAI: Based on your configuration, here's an automated testing approach:\n\nCurrent setup:\n- Production snapshots: Available with sanitization\n- Test environments: Configured for automated loading\n- Database tools: Properly configured\n\nRecommended daily workflow:\n1. Build fresh production snapshot (automated via CI/CD)\n2. Sanitize snapshot for test data safety  \n3. Load sanitized snapshot into test databases\n4. Run automated test suite\n\nThe MCP integration can be used with GitHub Actions or other CI/CD tools.\nWould you like me to help set up any part of this workflow?\n</code></pre>"}, {"location": "commands/mcp/#data-sharing-and-collaboration", "title": "Data Sharing and Collaboration", "text": "<pre><code>User: \"I need to share database data with my team, but it contains sensitive information\"\nAI: I can help you create a sanitized snapshot that's safe for team sharing.\n\n[AI uses list_snapshots to see available data]\n\nAI: I see you have recent production snapshots. I'll create a sanitized version for safe sharing.\n\n[AI uses sanitize_snapshot with appropriate configuration]\n\nAI: \u2705 Team-safe snapshot created:\n- Original: 1705123456_production-db.zip (contains sensitive data)\n- Sanitized: 1705123456_production-db.san.zip (safe for team use)\n- Sanitization: PII removed, emails masked, IDs preserved for referential integrity\n\nThe sanitized snapshot can be safely shared with your team or loaded into shared development environments. The data structure and relationships are preserved while sensitive information is protected.\n</code></pre>"}, {"location": "commands/mcp/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/mcp/#common-setup-issues", "title": "Common Setup Issues", "text": ""}, {"location": "commands/mcp/#mcp-server-not-starting", "title": "MCP Server Not Starting", "text": "<pre><code># Test MCP server directly\ndbsnapper mcp --help\n\n# Start HTTP MCP server (default) with debug output  \nDBSNAPPER_LOG_LEVEL=debug dbsnapper mcp\n\n# Start stdio MCP server with debug output\nDBSNAPPER_LOG_LEVEL=debug dbsnapper mcp --stdio\n\n# Check if binary is accessible\nwhich dbsnapper\nls -la /path/to/dbsnapper\n</code></pre> <p>Solutions: 1. Verify DBSnapper binary exists and is executable 2. Check configuration file exists and is readable 3. Ensure proper permissions on binary and config files 4. Use absolute paths in Claude Desktop configuration</p>"}, {"location": "commands/mcp/#claude-desktop-integration-issues", "title": "Claude Desktop Integration Issues", "text": "<pre><code>// Verify JSON syntax is correct (HTTP transport - recommended)\n{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"/full/path/to/dbsnapper\",\n      \"args\": [\"mcp\"],\n      \"description\": \"DBSnapper database management\"\n    }\n  }\n}\n</code></pre> <p>Solutions: 1. Validate JSON syntax in configuration file 2. Use absolute path to DBSnapper binary 3. Restart Claude Desktop completely after configuration changes 4. Check Claude Desktop logs for error messages</p>"}, {"location": "commands/mcp/#database-tools-not-found", "title": "Database Tools Not Found", "text": "<pre><code>Error: \"pg_dump\": executable file not found in $PATH\n</code></pre> <p>Recommended Solution: Use MCP Tool Discovery <pre><code>AI Prompt: \"Find and configure my database tools automatically\"\n</code></pre></p> <p>Alternative: Manual PATH Configuration <pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"dbsnapper\",\n      \"args\": [\"mcp\"],\n      \"env\": {\n        \"PATH\": \"/opt/homebrew/bin:/usr/local/bin:/usr/bin:/bin\"\n      }\n    }\n  }\n}\n</code></pre></p>"}, {"location": "commands/mcp/#configuration-issues", "title": "Configuration Issues", "text": ""}, {"location": "commands/mcp/#configuration-file-not-found", "title": "Configuration File Not Found", "text": "<pre><code># Check default configuration location\nls -la ~/.config/dbsnapper/dbsnapper.yml\n\n# Initialize configuration if missing\ndbsnapper config init\n\n# Use custom configuration path\nexport DBSNAPPER_CONFIG=/path/to/custom/config.yml\n</code></pre>"}, {"location": "commands/mcp/#permission-issues", "title": "Permission Issues", "text": "<pre><code># Fix binary permissions\nchmod +x /path/to/dbsnapper\n\n# Fix configuration file permissions\nchmod 600 ~/.config/dbsnapper/dbsnapper.yml\n\n# Check working directory permissions\nls -la ~/.dbsnapper/\n</code></pre>"}, {"location": "commands/mcp/#ai-integration-issues", "title": "AI Integration Issues", "text": ""}, {"location": "commands/mcp/#ai-cant-execute-tools", "title": "AI Can't Execute Tools", "text": "<p>Symptoms: - AI says tools are not available - Operations fail with \"tool not found\" errors</p> <p>Solutions: 1. Restart Claude Desktop after configuration changes 2. Verify MCP server configuration in Claude Desktop settings 3. Test MCP server independently: <code>dbsnapper mcp --debug</code> 4. Check for configuration file syntax errors</p>"}, {"location": "commands/mcp/#operations-fail-or-timeout", "title": "Operations Fail or Timeout", "text": "<p>Symptoms: - Database operations hang or timeout - \"Connection refused\" or similar database errors</p> <p>Solutions: 1. Test database connectivity independently 2. Verify target configurations are correct 3. Check network connectivity and firewall settings 4. Ensure database credentials are valid</p>"}, {"location": "commands/mcp/#inconsistent-results", "title": "Inconsistent Results", "text": "<p>Symptoms: - Same operation produces different results - Configuration appears to change between runs</p> <p>Solutions: 1. Use absolute paths in all configurations 2. Set explicit environment variables 3. Check for multiple configuration files 4. Verify working directory consistency</p>"}, {"location": "commands/mcp/#debugging-and-diagnostics", "title": "Debugging and Diagnostics", "text": ""}, {"location": "commands/mcp/#enable-debug-mode", "title": "Enable Debug Mode", "text": "<pre><code>{\n  \"mcpServers\": {\n    \"dbsnapper\": {\n      \"command\": \"dbsnapper\",\n      \"args\": [\"mcp\"],\n      \"env\": {\n        \"DBSNAPPER_LOG_LEVEL\": \"debug\",\n        \"DBSNAPPER_HTTP_DEBUG\": \"true\"\n      }\n    }\n  }\n}\n</code></pre>"}, {"location": "commands/mcp/#test-mcp-server-independently", "title": "Test MCP Server Independently", "text": "<pre><code># Start HTTP MCP server with debug output (default)\nDBSNAPPER_LOG_LEVEL=debug dbsnapper mcp\n\n# Start stdio MCP server with debug output (for Claude Desktop)\nDBSNAPPER_LOG_LEVEL=debug dbsnapper mcp --stdio\n\n# Test specific operations\ndbsnapper targets --help\ndbsnapper build --help\ndbsnapper load --help\n</code></pre>"}, {"location": "commands/mcp/#verify-configuration", "title": "Verify Configuration", "text": "<pre><code># Validate configuration file\ndbsnapper config validate\n\n# Check configuration information\ndbsnapper config check\n\n# Test database connectivity\ndbsnapper targets\n</code></pre>"}, {"location": "commands/mcp/#performance-considerations", "title": "Performance Considerations", "text": ""}, {"location": "commands/mcp/#memory-usage", "title": "Memory Usage", "text": "<ul> <li>Snapshot operations: MCP server uses same memory as direct DBSnapper operations</li> <li>Tool discovery: Minimal memory overhead for tool registry</li> <li>Resource caching: Configuration and metadata cached for performance</li> </ul>"}, {"location": "commands/mcp/#response-times", "title": "Response Times", "text": "<ul> <li>Simple queries: Near-instantaneous (target lists, configuration info)</li> <li>Snapshot operations: Same performance as direct DBSnapper commands</li> <li>Large snapshots: Progress may not be visible to AI during long operations</li> </ul>"}, {"location": "commands/mcp/#concurrent-operations", "title": "Concurrent Operations", "text": "<ul> <li>Single operation limit: MCP server processes one operation at a time</li> <li>AI request queuing: Multiple AI requests are queued and processed sequentially</li> <li>Database connections: Respects DBSnapper's existing connection management</li> </ul>"}, {"location": "commands/mcp/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "commands/mcp/#data-protection", "title": "Data Protection", "text": "<ul> <li>Automatic sanitization: Sensitive configuration data automatically removed from AI responses</li> <li>Credential isolation: Database credentials never exposed to AI applications</li> <li>Read-only metadata: AI can access metadata but cannot modify sensitive configurations</li> <li>Snapshot security: Original vs sanitized snapshot handling preserved</li> </ul>"}, {"location": "commands/mcp/#access-control", "title": "Access Control", "text": "<ul> <li>Operation permissions: All operations subject to DBSnapper's existing permissions</li> <li>Configuration access: AI cannot modify configuration files directly</li> <li>Network isolation: HTTP/SSE bind to localhost only, stdio uses no network ports</li> <li>Audit trail: All operations logged through DBSnapper's logging system</li> </ul>"}, {"location": "commands/mcp/#best-practices", "title": "Best Practices", "text": "<ol> <li>Use sanitized snapshots: Default to sanitized snapshots for AI-assisted operations</li> <li>Monitor operations: Review MCP server logs for unusual activity</li> <li>Limit scope: Configure only necessary targets for AI access</li> <li>Regular updates: Keep DBSnapper and AI applications updated</li> <li>Configuration security: Maintain restrictive permissions on configuration files</li> </ol> <p>Quick Start</p> <p>The fastest way to get started is to add DBSnapper to your Claude Desktop configuration, restart Claude, and ask: \"What database targets do I have?\" This will verify the integration is working correctly.</p> <p>Tool Discovery</p> <p>If you encounter \"executable file not found\" errors, use the AI prompt \"Find and configure my database tools automatically\" to resolve PATH issues permanently.</p> <p>Security</p> <p>While DBSnapper automatically sanitizes configuration data in AI responses, always review snapshot contents before sharing or using in development environments.</p>"}, {"location": "commands/mcp/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>config</code> - Configuration management (used by MCP for setup validation)</li> <li><code>targets</code> - Target management (exposed as MCP tool)</li> <li><code>build</code> - Snapshot creation (exposed as MCP tool)</li> <li><code>load</code> - Snapshot loading (exposed as MCP tool)</li> <li><code>sanitize</code> - Data sanitization (exposed as MCP tool)</li> </ul>"}, {"location": "commands/mcp/#see-also", "title": "See Also", "text": "<ul> <li>AI Integrations Overview - Full AI capabilities and future roadmap</li> <li>Model Context Protocol - Official MCP specification</li> <li>Claude Desktop - Primary AI application supporting MCP integration</li> <li>DBSnapper GitHub Repository - Source code and additional technical documentation</li> </ul>"}, {"location": "commands/pull/", "title": "Pull Command", "text": "<p>The <code>pull</code> command downloads snapshots from cloud storage to your local working directory. This essential cloud integration command enables access to snapshots stored in S3-compatible storage, facilitating team collaboration and cross-environment snapshot sharing.</p>"}, {"location": "commands/pull/#overview", "title": "Overview", "text": "<p>The pull command bridges cloud and local storage by: - Downloading snapshots from configured cloud storage profiles - Automatically handling sanitized vs original snapshot selection - Providing fast access to team-shared snapshots - Supporting both cloud targets and shared targets from Okta SSO - Maintaining local cache for improved performance</p>"}, {"location": "commands/pull/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper pull &lt;target_name&gt; [snapshot_index] [flags]\n</code></pre>"}, {"location": "commands/pull/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> <li><code>[snapshot_index]</code>: Optional index of the snapshot to pull (default: 0 for most recent)</li> </ul>"}, {"location": "commands/pull/#options", "title": "Options", "text": ""}, {"location": "commands/pull/#command-specific-flags", "title": "Command-Specific Flags", "text": "<pre><code>  -o, --original   Use the original snapshot instead of the sanitized version\n  -h, --help       help for pull\n</code></pre>"}, {"location": "commands/pull/#global-flags", "title": "Global Flags", "text": "<pre><code>      --config string   Config file (default is ~/.config/dbsnapper/dbsnapper.yml)\n      --nocloud         Disable cloud mode to speed up operations by skipping cloud API calls\n</code></pre>"}, {"location": "commands/pull/#target-configuration", "title": "Target Configuration", "text": "<p>The pull command requires targets with cloud storage configuration:</p>"}, {"location": "commands/pull/#required-configuration", "title": "Required Configuration", "text": "<p>Target with Storage Profile:</p> <pre><code>storage_profiles:\n  aws-s3-prod:\n    type: s3\n    region: us-west-2\n    bucket: company-snapshots\n    access_key_id: ${AWS_ACCESS_KEY_ID}\n    secret_access_key: ${AWS_SECRET_ACCESS_KEY}\n\ntargets:\n  production-db:\n    name: \"Production Database\"\n    storage_profile: \"aws-s3-prod\"  # Links to storage profile\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/app\"\n      dst_url: \"postgresql://user:pass@dev:5432/app_copy\"\n</code></pre>"}, {"location": "commands/pull/#authentication-requirements", "title": "Authentication Requirements", "text": "<ul> <li>Cloud Targets: Requires DBSnapper Cloud API token</li> <li>Storage Access: Requires valid storage credentials (AWS keys, etc.)</li> <li>Shared Targets: Requires Okta SSO authentication</li> </ul>"}, {"location": "commands/pull/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/pull/#basic-pull-operations", "title": "Basic Pull Operations", "text": "<pre><code># Pull the most recent snapshot (sanitized priority)\ndbsnapper pull production-db\n\n# Pull specific snapshot by index\ndbsnapper pull production-db 3\n\n# Pull original snapshot instead of sanitized\ndbsnapper pull production-db --original\ndbsnapper pull production-db 2 --original\n</code></pre>"}, {"location": "commands/pull/#team-collaboration-examples", "title": "Team Collaboration Examples", "text": "<pre><code># Pull shared team snapshot\ndbsnapper pull team-database\n\n# Pull from different environments\ndbsnapper pull staging-db 0        # Latest staging snapshot\ndbsnapper pull production-db 5     # Specific production snapshot\n</code></pre>"}, {"location": "commands/pull/#how-it-works", "title": "How It Works", "text": "<ol> <li>Target Validation: Verifies the target exists and has cloud storage configured</li> <li>Snapshot Selection: Identifies the requested snapshot by index (0 = most recent)</li> <li>Type Resolution: Determines whether to pull original or sanitized version</li> <li>Cloud Download: Downloads the snapshot from S3-compatible storage</li> <li>Local Storage: Saves the snapshot to your working directory</li> <li>Verification: Confirms successful download and file integrity</li> </ol>"}, {"location": "commands/pull/#sanitized-vs-original-snapshots", "title": "Sanitized vs Original Snapshots", "text": ""}, {"location": "commands/pull/#default-behavior-sanitized-priority", "title": "Default Behavior (Sanitized Priority)", "text": "<p><pre><code>dbsnapper pull production-db 0\n</code></pre> - Prioritizes sanitized snapshots for data protection - Downloads <code>.san.zip</code> version if available - Falls back to original if no sanitized version exists - Use case: Safe data sharing, development environments</p>"}, {"location": "commands/pull/#original-snapshot-access", "title": "Original Snapshot Access", "text": "<p><pre><code>dbsnapper pull production-db --original\n</code></pre> - Forces original snapshot download (<code>.zip</code> version) - Bypasses sanitized version even if available - Use case: Full data access, production debugging</p>"}, {"location": "commands/pull/#snapshot-type-resolution", "title": "Snapshot Type Resolution", "text": "Sanitized Available Original Available Default Behavior With <code>--original</code> \u2705 Yes \u2705 Yes Downloads sanitized Downloads original \u2705 Yes \u274c No Downloads sanitized Downloads sanitized \u274c No \u2705 Yes Downloads original Downloads original \u274c No \u274c No Error: snapshot not found Error: snapshot not found"}, {"location": "commands/pull/#cloud-storage-integration", "title": "Cloud Storage Integration", "text": ""}, {"location": "commands/pull/#supported-storage-types", "title": "Supported Storage Types", "text": "<ul> <li>Amazon S3: Native S3 API support</li> <li>S3-Compatible: MinIO, Cloudflare R2, DigitalOcean Spaces</li> <li>Future Support: Azure Blob, Google Cloud Storage</li> </ul>"}, {"location": "commands/pull/#storage-profile-configuration", "title": "Storage Profile Configuration", "text": "<pre><code>storage_profiles:\n  # Amazon S3\n  aws-production:\n    type: s3\n    region: us-east-1\n    bucket: prod-snapshots\n    access_key_id: ${AWS_ACCESS_KEY_ID}\n    secret_access_key: ${AWS_SECRET_ACCESS_KEY}\n\n  # Cloudflare R2\n  cloudflare-r2:\n    type: s3\n    region: auto\n    bucket: team-snapshots\n    endpoint_url: https://account-id.r2.cloudflarestorage.com\n    access_key_id: ${CF_R2_ACCESS_KEY_ID}\n    secret_access_key: ${CF_R2_SECRET_ACCESS_KEY}\n</code></pre>"}, {"location": "commands/pull/#authentication-setup", "title": "Authentication Setup", "text": "<p>Environment Variables (Recommended): <pre><code>export AWS_ACCESS_KEY_ID=\"your_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_secret_key\"\nexport DBSNAPPER_API_TOKEN=\"your_cloud_token\"\n</code></pre></p> <p>Configuration File: <pre><code>auth:\n  cloud_token: ${DBSNAPPER_API_TOKEN}\n</code></pre></p>"}, {"location": "commands/pull/#target-types", "title": "Target Types", "text": ""}, {"location": "commands/pull/#cloud-targets", "title": "Cloud Targets", "text": "<p>Targets managed through DBSnapper Cloud with full API integration:</p> <pre><code>targets:\n  cloud-managed:\n    name: \"Cloud Managed Target\"\n    storage_profile: \"aws-s3-prod\"\n    # Additional configuration managed through cloud interface\n</code></pre>"}, {"location": "commands/pull/#shared-targets", "title": "Shared Targets", "text": "<p>Targets shared via Okta SSO integration for team access:</p> <pre><code>targets:\n  shared-team-db:\n    name: \"Shared Team Database\" \n    storage_profile: \"team-storage\"\n    # Requires Okta SSO authentication\n</code></pre>"}, {"location": "commands/pull/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/pull/#development-environment-sync", "title": "Development Environment Sync", "text": "<pre><code># Keep development in sync with latest production snapshot\ndbsnapper pull production-db\ndbsnapper load dev-environment\n</code></pre>"}, {"location": "commands/pull/#cross-team-collaboration", "title": "Cross-Team Collaboration", "text": "<pre><code># Access snapshots created by other team members\ndbsnapper pull shared-analytics-db\ndbsnapper load local-analytics\n</code></pre>"}, {"location": "commands/pull/#environment-promotion", "title": "Environment Promotion", "text": "<pre><code># Pull specific tested snapshot from staging\ndbsnapper pull staging-db 5\ndbsnapper load production-db\n</code></pre>"}, {"location": "commands/pull/#historical-data-access", "title": "Historical Data Access", "text": "<pre><code># Access older snapshots for analysis\ndbsnapper pull user-db 10    # 10 snapshots ago\ndbsnapper load analysis-env\n</code></pre>"}, {"location": "commands/pull/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/pull/#common-issues", "title": "Common Issues", "text": "<p>Target Not Found <pre><code>Error: target 'production-db' not found\n</code></pre> - Verify target exists in configuration - Check target spelling and case sensitivity - Ensure target has storage_profile configured</p> <p>Storage Authentication Failed <pre><code>Error: failed to authenticate with cloud storage\n</code></pre> - Verify storage profile credentials - Check environment variables are set - Confirm access keys have appropriate permissions</p> <p>Snapshot Not Found <pre><code>Error: snapshot index 3 not found for target production-db\n</code></pre> - List available snapshots with <code>dbsnapper target production-db</code> - Use valid snapshot index (0 = most recent) - Check if snapshots exist in cloud storage</p> <p>Network Connection Issues <pre><code>Error: failed to download snapshot from cloud storage\n</code></pre> - Check internet connectivity - Verify cloud storage endpoint accessibility - Confirm firewall/proxy settings allow cloud access</p> <p>Insufficient Permissions <pre><code>Error: access denied to cloud storage bucket\n</code></pre> - Verify storage credentials have read permissions - Check bucket-level policies and access controls - Ensure API token has appropriate cloud access</p>"}, {"location": "commands/pull/#recovery-steps", "title": "Recovery Steps", "text": "<ol> <li>Verify Configuration: Check target and storage profile setup</li> <li>Test Authentication: Confirm cloud credentials are valid</li> <li>Check Connectivity: Ensure cloud storage is accessible</li> <li>List Snapshots: Use <code>dbsnapper target &lt;name&gt;</code> to see available snapshots</li> <li>Try Different Index: Access different snapshots if specific index fails</li> </ol>"}, {"location": "commands/pull/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "commands/pull/#local-caching", "title": "Local Caching", "text": "<ul> <li>Downloaded snapshots are cached locally</li> <li>Subsequent pulls check local cache first</li> <li>Use <code>--force</code> flag to bypass cache (when available)</li> </ul>"}, {"location": "commands/pull/#parallel-downloads", "title": "Parallel Downloads", "text": "<ul> <li>Large snapshots downloaded in parallel chunks</li> <li>Configurable chunk size for optimal performance</li> <li>Automatic retry on failed chunks</li> </ul>"}, {"location": "commands/pull/#bandwidth-management", "title": "Bandwidth Management", "text": "<ul> <li>Progress indicators for large downloads</li> <li>Resumable downloads for interrupted transfers</li> <li>Compression optimization during transfer</li> </ul> <p>Sanitized Priority</p> <p>By default, pull prioritizes sanitized snapshots (<code>.san.zip</code>) over original snapshots (<code>.zip</code>) for data protection. Use <code>--original</code> to override this behavior.</p> <p>Team Collaboration</p> <p>Use shared targets with Okta SSO integration for seamless team access to snapshots without managing individual credentials.</p>"}, {"location": "commands/pull/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>build</code> - Create snapshots for cloud storage</li> <li><code>load</code> - Load pulled snapshots into databases</li> <li><code>target</code> - View available snapshots for a target</li> <li><code>targets</code> - List all configured targets</li> </ul>"}, {"location": "commands/pull/#see-also", "title": "See Also", "text": "<ul> <li>Cloud Storage Setup - Configure storage profiles</li> <li>Team Collaboration - Shared targets and SSO</li> <li>Configuration - Target and storage configuration</li> </ul>"}, {"location": "commands/sanitize/", "title": "Sanitize Command", "text": "<p>The <code>sanitize</code> command creates sanitized versions of database snapshots by removing or modifying sensitive data. This critical security feature ensures that production data can be safely shared with development teams while maintaining referential integrity and realistic test data.</p>"}, {"location": "commands/sanitize/#overview", "title": "Overview", "text": "<p>The sanitize command takes existing snapshots created with the <code>build</code> command and applies sanitization queries to remove sensitive information like personally identifiable information (PII), passwords, financial data, or any other confidential content. The sanitized snapshots maintain the database structure and relationships while replacing sensitive data with safe alternatives.</p>"}, {"location": "commands/sanitize/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper sanitize [flags] &lt;target_name&gt; [snapshot_index]\n</code></pre>"}, {"location": "commands/sanitize/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> <li><code>[snapshot_index]</code>: Optional index of the snapshot to sanitize (default: 0 for most recent)</li> </ul>"}, {"location": "commands/sanitize/#options", "title": "Options", "text": ""}, {"location": "commands/sanitize/#command-specific-flags", "title": "Command-Specific Flags", "text": "<pre><code>  -e, --ephemeral   Create a snapshot using an ephemeral database via docker containers\n  -n, --new         Create a new snapshot set for the target\n  -h, --help        help for sanitize  \n</code></pre>"}, {"location": "commands/sanitize/#global-flags", "title": "Global Flags", "text": "<pre><code>      --config string   Config file (default is ~/.config/dbsnapper/dbsnapper.yml)\n      --nocloud         Disable cloud mode to speed up operations by skipping cloud API calls\n</code></pre>"}, {"location": "commands/sanitize/#target-configuration", "title": "Target Configuration", "text": "<p>The sanitize command requires targets with sanitization configuration:</p>"}, {"location": "commands/sanitize/#required-configuration", "title": "Required Configuration", "text": "<p>Minimal Target Setup:</p> <pre><code>targets:\n  my-target:\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/dbname\"\n      dst_url: \"postgresql://user:pass@dev:5432/dbname_copy\"\n    sanitize:\n      query_file: \"sanitize.sql\"\n</code></pre>"}, {"location": "commands/sanitize/#complete-target-configuration", "title": "Complete Target Configuration", "text": "<p>Target with All Sanitization Options:</p> <pre><code>targets:\n  production-db:\n    name: \"Production Database\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/users\"\n      dst_url: \"postgresql://user:pass@dev:5432/users_snap\"\n    sanitize:\n      dst_url: \"postgresql://user:pass@sanitize:5432/users_clean\"\n      query_file: \"user_sanitize.sql\"\n      # Optional: Override with direct SQL\n      override_query: |\n        UPDATE users SET \n          email = 'user' || id || '@example.com',\n          phone = '555-0100',\n          ssn = NULL;\n        DELETE FROM user_sessions;\n</code></pre>"}, {"location": "commands/sanitize/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/sanitize/#basic-sanitization", "title": "Basic Sanitization", "text": "<pre><code># Sanitize the most recent snapshot\ndbsnapper sanitize production-db\n\n# Sanitize a specific snapshot by index\ndbsnapper sanitize production-db 2\n</code></pre>"}, {"location": "commands/sanitize/#ephemeral-sanitization", "title": "Ephemeral Sanitization", "text": "<pre><code># Force ephemeral mode regardless of configuration\ndbsnapper sanitize production-db --ephemeral\n\n# Combine ephemeral with new snapshot set\ndbsnapper sanitize production-db --ephemeral --new\n</code></pre>"}, {"location": "commands/sanitize/#new-snapshot-set-creation", "title": "New Snapshot Set Creation", "text": "<pre><code># Create a new sanitized snapshot set\ndbsnapper sanitize user-db --new\n\n# Create multiple sanitized versions for testing\ndbsnapper sanitize production-db --new\ndbsnapper sanitize production-db --new  # Different timestamp\n</code></pre>"}, {"location": "commands/sanitize/#how-it-works", "title": "How It Works", "text": "<ol> <li>Snapshot Selection: Identifies the snapshot to sanitize based on index (0 = most recent)</li> <li>Database Preparation: Sets up the sanitization environment (ephemeral or configured database)</li> <li>Snapshot Loading: Loads the original snapshot into the sanitization database</li> <li>Query Execution: Applies sanitization queries to remove/modify sensitive data</li> <li>Sanitized Snapshot Creation: Creates a new <code>.san.zip</code> file with the sanitized data</li> <li>Cleanup: Removes temporary files and databases (for ephemeral mode)</li> </ol>"}, {"location": "commands/sanitize/#output-files", "title": "Output Files", "text": ""}, {"location": "commands/sanitize/#sanitized-snapshots", "title": "Sanitized Snapshots", "text": "<ul> <li>Format: <code>&lt;timestamp&gt;_&lt;target_name&gt;.san.zip</code></li> <li>Location: Working directory specified in configuration file</li> <li>Example: <code>1752444532_production.san.zip</code></li> </ul>"}, {"location": "commands/sanitize/#snapshot-set-behavior", "title": "Snapshot Set Behavior", "text": "<p>Default Behavior - Original: <code>1234567890_target.zip</code> - Sanitized: <code>1234567890_target.san.zip</code> (same timestamp)</p> <p>New Snapshot Set (<code>--new</code> flag) - Original: <code>1234567890_target.zip</code> (unchanged) - New Sanitized: <code>9876543210_target.san.zip</code> (new timestamp)</p>"}, {"location": "commands/sanitize/#sanitization-query-priority-system", "title": "Sanitization Query Priority System", "text": "<p>DBSnapper uses a three-level priority system for specifying sanitization queries, allowing flexible configuration from global defaults to target-specific overrides:</p>"}, {"location": "commands/sanitize/#priority-levels-highest-to-lowest", "title": "Priority Levels (Highest to Lowest)", "text": "<ol> <li>Target-level <code>override_query</code> (Highest Priority)</li> <li>Specified in individual target's sanitize configuration</li> <li>Overrides all other query sources</li> <li> <p>Use for target-specific sanitization requirements</p> </li> <li> <p>Global <code>override.san_query</code> (Medium Priority) </p> </li> <li>Specified in global override configuration</li> <li>Applied when no target-level override exists</li> <li> <p>Use for organization-wide sanitization standards</p> </li> <li> <p>Target <code>query_file</code> (Lowest Priority)</p> </li> <li>SQL file path in target configuration</li> <li>Used when no override queries are specified</li> <li>Supports absolute and relative file paths</li> </ol>"}, {"location": "commands/sanitize/#configuration-examples", "title": "Configuration Examples", "text": "<p>Target-Level Override Query:</p> <pre><code>targets:\n  financial-db:\n    name: \"Financial Database\"\n    sanitize:\n      dst_url: \"postgresql://user:pass@host:5432/finance_clean\"\n      override_query: |\n        UPDATE accounts SET account_number = 'ACCT-' || id;\n        UPDATE transactions SET amount = ROUND(RANDOM() * 1000, 2);\n        DELETE FROM audit_logs WHERE created_at &lt; NOW() - INTERVAL '30 days';\n</code></pre> <p>Global Override Configuration:</p> <pre><code># Global sanitization standard for all targets\noverride:\n  san_query: |\n    -- Standard PII sanitization\n    UPDATE users SET \n      email = 'user' || id || '@example.com',\n      phone = '555-0100',\n      ssn = NULL;\n    DELETE FROM user_sessions;\n\ntargets:\n  app-db:\n    sanitize:\n      dst_url: \"postgresql://user:pass@host:5432/app_clean\"\n      # Uses global override.san_query\n</code></pre> <p>Base64 Encoded Complex Query:</p> <pre><code>targets:\n  complex-db:\n    sanitize:\n      # Base64 encoded multi-line query with special characters\n      override_query: \"VVBEQVRFIHVzZXJzIFNFVCBlbWFpbCA9ICd1c2VyJyB8fCBpZCB8fCAnQGV4YW1wbGUuY29tJywgcGFzc3dvcmQgPSAnZmFrZV9oYXNoJzsKVVBEQVRFIGN1c3RvbWVycyBTRVQgY2NfbnVtYmVyID0gJyoqKioqKioqKioqKioqJyBXSEVSRSBjY19udW1iZXIgSVMgTk9UIE5VTEw7\"\n</code></pre>"}, {"location": "commands/sanitize/#query-file-path-resolution", "title": "Query File Path Resolution", "text": "<p>The <code>query_file</code> path resolution follows these rules:</p> <ul> <li>Absolute paths (e.g., <code>/path/to/sanitize.sql</code>): Used as-is</li> <li>Relative paths (e.g., <code>scripts/sanitize.sql</code>): Resolved relative to the working directory</li> <li>Filenames only (e.g., <code>sanitize.sql</code>): Looked up in the working directory</li> </ul>"}, {"location": "commands/sanitize/#sanitization-modes", "title": "Sanitization Modes", "text": ""}, {"location": "commands/sanitize/#persistent-database-mode-default", "title": "Persistent Database Mode (Default)", "text": "<p>Uses a configured database specified in <code>sanitize.dst_url</code>:</p> <pre><code>targets:\n  production-db:\n    sanitize:\n      dst_url: \"postgresql://user:pass@host:5432/sanitize_db\"\n      query_file: \"sanitize.sql\"\n</code></pre> <p>Advantages: - Faster execution (no container startup) - Persistent database for debugging - Full control over database configuration</p> <p>Disadvantages: - Requires pre-configured database - Database will be dropped/recreated - Manual cleanup required</p>"}, {"location": "commands/sanitize/#ephemeral-database-mode-ephemeral-flag", "title": "Ephemeral Database Mode (<code>--ephemeral</code> flag)", "text": "<p>Uses temporary Docker containers for sanitization:</p> <pre><code>dbsnapper sanitize production-db --ephemeral\n</code></pre> <p>Advantages: - No database setup required - Automatic cleanup after completion - Isolated sanitization environment - No risk to existing databases</p> <p>Disadvantages: - Slower execution (container overhead) - Requires Docker installation - Limited debugging capabilities</p>"}, {"location": "commands/sanitize/#automatic-ephemeral-mode", "title": "Automatic Ephemeral Mode", "text": "<p>If no <code>sanitize.dst_url</code> is configured, the command automatically uses ephemeral mode:</p> <pre><code>targets:\n  simple-target:\n    sanitize:\n      query_file: \"basic_sanitize.sql\"\n      # No dst_url specified - will use ephemeral mode\n</code></pre>"}, {"location": "commands/sanitize/#base64-query-encoding", "title": "Base64 Query Encoding", "text": "<p>All sanitization queries support base64 encoding for complex SQL containing special characters, multi-line queries, or YAML formatting issues:</p> <pre><code>targets:\n  secure-target:\n    sanitize:\n      # Base64-encoded complex sanitization query\n      override_query: \"VVBEQVRFIHVzZXJzIFNFVCBlbWFpbCA9ICd1c2VyJyB8fCBpZCB8fCAnQGV4YW1wbGUuY29tJywgcGFzc3dvcmQgPSAnZmFrZV9oYXNoJzsKVVBEQVRFIGN1c3RvbWVycyBTRVQgY2NfbnVtYmVyID0gJyoqKioqKioqKioqKioqJyBXSEVSRSBjY19udW1iZXIgSVMgTk9UIE5VTEw7\"\n</code></pre> <p>The system automatically detects and decodes base64 content, making it transparent to users while enabling complex query handling.</p>"}, {"location": "commands/sanitize/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/sanitize/#development-environment-protection", "title": "Development Environment Protection", "text": "<pre><code># Create sanitized snapshots for safe development use\ndbsnapper build production-db\ndbsnapper sanitize production-db\ndbsnapper load dev-db  # Loads sanitized version by default\n</code></pre>"}, {"location": "commands/sanitize/#multiple-sanitization-levels", "title": "Multiple Sanitization Levels", "text": "<pre><code># Create different sanitization levels\ndbsnapper sanitize user-data --new    # Light sanitization\n# Update sanitize config for heavy sanitization\ndbsnapper sanitize user-data --new    # Heavy sanitization\n</code></pre>"}, {"location": "commands/sanitize/#testing-sanitization-queries", "title": "Testing Sanitization Queries", "text": "<pre><code># Test sanitization with ephemeral mode\ndbsnapper sanitize test-db --ephemeral\n# Review results, adjust query, repeat\n</code></pre>"}, {"location": "commands/sanitize/#team-sharing", "title": "Team Sharing", "text": "<pre><code># Sanitize and share via cloud storage\ndbsnapper sanitize shared-db\n# Team members can pull sanitized version\n</code></pre>"}, {"location": "commands/sanitize/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/sanitize/#common-issues", "title": "Common Issues", "text": "<p>Query File Not Found <pre><code>Error: sanitization query file not found: sanitize.sql\n</code></pre> - Verify file path is correct - Check file exists in working directory - Use absolute paths if needed</p> <p>Database Connection Failed <pre><code>Error: failed to connect to sanitize database\n</code></pre> - Verify <code>sanitize.dst_url</code> connection string - Check database server availability - Confirm credentials and permissions</p> <p>Docker Not Available (Ephemeral Mode) <pre><code>Error: Docker is not available for ephemeral mode\n</code></pre> - Install Docker Desktop or Docker Engine - Ensure Docker daemon is running - Check Docker permissions</p> <p>SQL Syntax Errors <pre><code>Error: syntax error in sanitization query\n</code></pre> - Review SQL syntax in query_file or override_query - Test queries manually against database - Consider using base64 encoding for complex queries</p> <p>Permission Denied <pre><code>Error: insufficient privileges to modify data\n</code></pre> - Ensure database user has UPDATE/DELETE privileges - Check table-specific permissions - Verify connection credentials</p>"}, {"location": "commands/sanitize/#best-practices", "title": "Best Practices", "text": "<ol> <li>Test Sanitization Queries: Always test on non-production data first</li> <li>Use Ephemeral Mode: For testing and one-off sanitizations</li> <li>Version Control: Keep sanitization SQL files in version control</li> <li>Data Integrity: Maintain referential integrity after sanitization</li> <li>Regular Updates: Keep sanitization queries updated with schema changes</li> </ol> <p>Irreversible Operation</p> <p>Sanitization permanently modifies data in the sanitized snapshot. The original snapshot remains unchanged, but the sanitized data cannot be recovered without re-running from the original.</p> <p>Query Priority System</p> <p>Use the three-level priority system effectively: - Global queries for organization standards - Query files for target-specific needs - Override queries for special cases</p>"}, {"location": "commands/sanitize/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>build</code> - Create snapshots to sanitize</li> <li><code>load</code> - Load sanitized snapshots (prioritizes .san.zip by default)</li> <li><code>target</code> - View available snapshots including sanitized versions</li> <li><code>targets</code> - List all available targets</li> </ul>"}, {"location": "commands/sanitize/#see-also", "title": "See Also", "text": "<ul> <li>Sanitization Guide - Comprehensive sanitization strategies</li> <li>Configuration - Target and sanitization configuration</li> <li>Security Best Practices - Safe sanitization practices</li> </ul>"}, {"location": "commands/target/", "title": "Target Command", "text": "<p>The <code>target</code> command displays detailed information about a specific target, including all snapshots, database statistics, and table relationships. This command provides comprehensive insight into a target's snapshot history, database structure, and operational status.</p>"}, {"location": "commands/target/#overview", "title": "Overview", "text": "<p>The target command serves as the primary inspection tool for individual targets, showing: - Complete snapshot history with creation dates, sizes, and sanitization status - Database statistics including size, table counts, and relationship mapping - Snapshot locations (local vs cloud storage) - Table structure and foreign key relationships - Database health and connectivity status</p>"}, {"location": "commands/target/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper target &lt;target_name&gt; [flags]\n</code></pre>"}, {"location": "commands/target/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> </ul>"}, {"location": "commands/target/#options", "title": "Options", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will overwrite any existing destination database\n  -h, --help            help for target\n      --info            Output database and table stats\n  -o, --original        Use the original snapshot instead of the sanitized version\n  -u, --update          Update the db info for all targets (default: true)\n</code></pre>"}, {"location": "commands/target/#snapshot-information-display", "title": "Snapshot Information Display", "text": "<p>The target command displays snapshots in a comprehensive table format:</p>"}, {"location": "commands/target/#snapshot-table-columns", "title": "Snapshot Table Columns", "text": ""}, {"location": "commands/target/#index", "title": "INDEX", "text": "<p>Zero-based index number for referencing snapshots in other commands. - 0 = Most recent snapshot - Higher numbers = Older snapshots - Used in <code>load</code>, <code>sanitize</code>, and other commands</p>"}, {"location": "commands/target/#created", "title": "CREATED", "text": "<p>Human-readable creation timestamp showing when the snapshot was built. - Format: <code>DD-Mon-YY HH:MM</code> - Example: <code>25-Aug-07 03:50</code> - Timezone: Displayed in local timezone</p>"}, {"location": "commands/target/#name", "title": "NAME", "text": "<p>Unique snapshot identifier combining timestamp and target name. - Format: <code>{timestamp}_{target_name}</code> or custom names - Examples:    - <code>1754538604_dvdrental-cloud</code> (standard format)   - <code>test_snapshot</code> (custom name)</p>"}, {"location": "commands/target/#filename", "title": "FILENAME", "text": "<p>Actual file name as stored in local working directory or cloud storage. - Local Format: <code>{timestamp}_{target_name}.zip</code> - Cloud Format: UUID-based filenames (e.g., <code>d7786d0b-9c5d-43c6-8744-e92893c4dc9a.zip</code>) - Sanitized Suffix: <code>.san.zip</code> for sanitized snapshots</p>"}, {"location": "commands/target/#size", "title": "SIZE", "text": "<p>Snapshot file size in human-readable format. - Units: Bytes, kB, MB, GB as appropriate - Accuracy: Actual compressed file size - Comparison: Useful for capacity planning and performance estimation</p>"}, {"location": "commands/target/#sanitized", "title": "SANITIZED?", "text": "<p>Indicates whether a sanitized version of the snapshot exists. - <code>true</code> = Sanitized snapshot available (preferred for loading) - <code>false</code> = Only original snapshot available - Priority: Sanitized snapshots are loaded by default unless <code>--original</code> is specified</p>"}, {"location": "commands/target/#location", "title": "LOCATION", "text": "<p>Shows where the snapshot is stored. - <code>local</code> = Stored in working directory on local machine - <code>cloud</code> = Stored in cloud storage (S3, R2, etc.) - Hybrid: Some snapshots may exist in both locations</p>"}, {"location": "commands/target/#database-statistics-info-flag", "title": "Database Statistics (<code>--info</code> flag)", "text": "<p>The <code>--info</code> flag provides comprehensive database analysis and health information:</p>"}, {"location": "commands/target/#database-stats-table", "title": "Database Stats Table", "text": ""}, {"location": "commands/target/#core-database-information", "title": "Core Database Information", "text": "<pre><code>+---------------------+---------+\n|     STATISTIC       |  VALUE  |\n+---------------------+---------+\n| Database Size       | 8613011 |\n| Database Status     | OK      |\n| Database Messages   |         |\n</code></pre> <ul> <li>Database Size: Total database size in bytes</li> <li>Database Status: Connection and health status (<code>OK</code>, <code>ERROR</code>, etc.)</li> <li>Database Messages: Any warnings or status messages</li> </ul>"}, {"location": "commands/target/#table-analysis", "title": "Table Analysis", "text": "<pre><code>| Subset Tables       |       0 |\n| Tables Connected    |       0 |\n| Tables Disconnected |      14 |\n| Tables Upstream     |       0 |\n| Tables Downstream   |       0 |\n| Total Tables        |      14 |\n</code></pre> <ul> <li>Subset Tables: Tables included in subset operations</li> <li>Tables Connected: Tables with foreign key relationships</li> <li>Tables Disconnected: Standalone tables without relationships</li> <li>Tables Upstream: Tables referenced by others</li> <li>Tables Downstream: Tables that reference others</li> <li>Total Tables: Complete table count</li> </ul>"}, {"location": "commands/target/#table-details", "title": "Table Details", "text": "<p>The <code>--info</code> flag shows individual table information:</p> <pre><code>+------+---------------------------+------+\n| TYPE |           TABLE           | ROWS |\n+------+---------------------------+------+\n| X    | public.users              |   -1 |\n| X    | temp_schema.cache_entries |   -1 |\n| X    | analytics.reports         |   -1 |\n</code></pre> <ul> <li>TYPE: Table classification (<code>X</code> = Excluded/Disconnected, others TBD)</li> <li>TABLE: Full schema-qualified table name</li> <li>ROWS: Row count (-1 indicates not calculated)</li> </ul>"}, {"location": "commands/target/#relationship-mapping", "title": "Relationship Mapping", "text": "<p>Foreign key relationships are displayed in detail:</p> <pre><code>+------------------------+------+-------------------------+--------------+---------------------+------------+\n|          NAME          | TYPE |         FKTABLE         |  FKCOLUMNS   |      REFTABLE       | REFCOLUMNS |\n+------------------------+------+-------------------------+--------------+---------------------+------------+\n| Relationships          |      |                         |              |                     |            |\n|                        | X    | analytics.events        | user_id      | public.users        | id         |\n|                        | X    | public.posts            | user_id      | public.users        | id         |\n| Excluded Relationships |      |                         |              |                     |            |\n</code></pre> <ul> <li>Relationships: Active foreign key constraints</li> <li>Excluded Relationships: Relationships ignored in subset operations</li> <li>FKTABLE: Table containing the foreign key</li> <li>FKCOLUMNS: Foreign key column names</li> <li>REFTABLE: Referenced table</li> <li>REFCOLUMNS: Referenced column names</li> </ul>"}, {"location": "commands/target/#location-types-and-storage", "title": "Location Types and Storage", "text": ""}, {"location": "commands/target/#local-snapshots", "title": "Local Snapshots", "text": "<ul> <li>Storage: Working directory (default: <code>~/.dbsnapper</code>)</li> <li>File Format: <code>{timestamp}_{target_name}.zip</code></li> <li>Advantages: Fast access, no network dependency</li> <li>Use Cases: Development, local testing, offline scenarios</li> </ul>"}, {"location": "commands/target/#cloud-snapshots", "title": "Cloud Snapshots", "text": "<ul> <li>Storage: S3-compatible cloud storage (AWS S3, Cloudflare R2, etc.)</li> <li>File Format: UUID-based names for security</li> <li>Advantages: Team sharing, backup, cross-environment access</li> <li>Requirements: Storage profile configuration and authentication</li> </ul>"}, {"location": "commands/target/#sanitization-status-and-priority", "title": "Sanitization Status and Priority", "text": ""}, {"location": "commands/target/#sanitization-indicators", "title": "Sanitization Indicators", "text": "<ul> <li><code>SANITIZED? = true</code>: Sanitized snapshot exists</li> <li><code>SANITIZED? = false</code>: Only original snapshot available</li> </ul>"}, {"location": "commands/target/#loading-priority", "title": "Loading Priority", "text": "<pre><code># Default behavior - loads sanitized version if available\ndbsnapper load target_name 0\n\n# Force original snapshot\ndbsnapper load target_name 0 --original\n</code></pre>"}, {"location": "commands/target/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/target/#basic-target-inspection", "title": "Basic Target Inspection", "text": "<pre><code># View all snapshots for a target\ndbsnapper target production-db\n\n# View snapshots with database statistics\ndbsnapper target production-db --info\n</code></pre>"}, {"location": "commands/target/#database-analysis-and-planning", "title": "Database Analysis and Planning", "text": "<pre><code># Get comprehensive database analysis\ndbsnapper target analytics-db --info\n# Use output for subset planning, schema analysis, capacity planning\n</code></pre>"}, {"location": "commands/target/#snapshot-history-review", "title": "Snapshot History Review", "text": "<pre><code># Review snapshot history for target\ndbsnapper target user-data\n# Identify suitable snapshots for specific needs\n</code></pre>"}, {"location": "commands/target/#example-output", "title": "Example Output", "text": ""}, {"location": "commands/target/#basic-target-view", "title": "Basic Target View", "text": "<pre><code>dbsnapper target production-app\n</code></pre> <p>Output: <pre><code>DBSnapper v2.7.0-dev - Target Snapshots\n[CLOUD] DBSnapper Cloud: Enabled\n[TARGET] Target: production-app\n[CONFIG] Config: ~/.config/dbsnapper/dbsnapper.yml\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n+-------+-----------------+--------------------------+--------------------------------+--------+------------+----------+\n| INDEX |     CREATED     |           NAME           |            FILENAME            |  SIZE  | SANITIZED? | LOCATION |\n+-------+-----------------+--------------------------+--------------------------------+--------+------------+----------+\n|     0 | 25-Aug-07 09:15 | 1754558129_production-app| 1754558129_production-app.zip  | 2.3 GB | true       | local    |\n|     1 | 25-Aug-06 09:15 | 1754471729_production-app| b84d2c91-4e32-4a91-b123-...   | 2.2 GB | true       | cloud    |\n|     2 | 25-Aug-05 09:15 | 1754385329_production-app| a72f1b82-3d21-4b82-a012-...   | 2.1 GB | false      | cloud    |\n|     3 | 25-Aug-04 09:15 | 1754298929_production-app| 9e8c4a73-2c12-4c73-9901-...   | 2.0 GB | false      | cloud    |\n+-------+-----------------+--------------------------+--------------------------------+--------+------------+----------+\n</code></pre></p>"}, {"location": "commands/target/#detailed-database-analysis", "title": "Detailed Database Analysis", "text": "<pre><code>dbsnapper target production-app --info\n</code></pre> <p>Output: <pre><code>DBSnapper v2.7.0-dev - Target Snapshots\n[CLOUD] DBSnapper Cloud: Enabled\n[TARGET] Target: production-app\n[CONFIG] Config: ~/.config/dbsnapper/dbsnapper.yml\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nDatabase Stats:\n+---------------------+-----------+\n|      STATISTIC      |   VALUE   |\n+---------------------+-----------+\n| Database Size       | 2461820152|\n| Database Status     | OK        |\n| Database Messages   |           |\n| Subset Tables       |         3 |\n| Tables Connected    |        12 |\n| Tables Disconnected |         8 |\n| Tables Upstream     |         5 |\n| Tables Downstream   |         7 |\n| Total Tables        |        20 |\n+---------------------+-----------+\n\nTable Stats:\n+------+----------------------+--------+\n| TYPE |        TABLE         |  ROWS  |\n+------+----------------------+--------+\n| \u2713    | public.users         | 125000 |\n| \u2713    | public.orders        |  45000 |\n| \u2713    | public.products      |   1500 |\n| X    | public.audit_logs    | 890000 |\n| X    | temp.cache_entries   |  12000 |\n+------+----------------------+--------+\n\nRelationships:\n+------------------------+------+-------------------+-----------+-----------------+------------+\n|          NAME          | TYPE |      FKTABLE      | FKCOLUMNS |    REFTABLE     | REFCOLUMNS |\n+------------------------+------+-------------------+-----------+-----------------+------------+\n| Active Relationships   |      |                   |           |                 |            |\n|                        | \u2713    | public.orders     | user_id   | public.users    | id         |\n|                        | \u2713    | public.order_items| order_id  | public.orders   | id         |\n|                        | \u2713    | public.order_items| product_id| public.products | id         |\n+------------------------+------+-------------------+-----------+-----------------+------------+\n\n[Snapshot listing follows...]\n</code></pre></p>"}, {"location": "commands/target/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/target/#development-environment-planning", "title": "Development Environment Planning", "text": "<pre><code># Analyze production database before creating development snapshot\ndbsnapper target production-db --info\n# Review table sizes, relationships for subset planning\ndbsnapper subset production-db  # Based on analysis\n</code></pre>"}, {"location": "commands/target/#snapshot-selection-for-operations", "title": "Snapshot Selection for Operations", "text": "<pre><code># Find appropriate snapshot for specific date/time\ndbsnapper target user-data\n# Select snapshot by index for loading\ndbsnapper load staging-env 3  # Load snapshot at index 3\n</code></pre>"}, {"location": "commands/target/#database-health-monitoring", "title": "Database Health Monitoring", "text": "<pre><code># Regular database health checks\ndbsnapper target production-db --info\n# Monitor table growth, relationship changes over time\n</code></pre>"}, {"location": "commands/target/#sanitization-planning", "title": "Sanitization Planning", "text": "<pre><code># Identify which snapshots have sanitized versions\ndbsnapper target sensitive-data\n# Plan sanitization for snapshots that need it\ndbsnapper sanitize sensitive-data 2  # Sanitize specific snapshot\n</code></pre>"}, {"location": "commands/target/#cloud-storage-management", "title": "Cloud Storage Management", "text": "<pre><code># Review cloud vs local snapshot distribution\ndbsnapper target distributed-app\n# Plan pull operations for cloud snapshots\ndbsnapper pull distributed-app 5  # Pull specific cloud snapshot\n</code></pre>"}, {"location": "commands/target/#capacity-planning-and-analysis", "title": "Capacity Planning and Analysis", "text": "<pre><code># Analyze database growth trends\ndbsnapper target analytics-db --info\n# Plan storage and backup strategies based on sizes\n</code></pre>"}, {"location": "commands/target/#performance-considerations", "title": "Performance Considerations", "text": ""}, {"location": "commands/target/#update-behavior", "title": "Update Behavior", "text": "<ul> <li>With Update (default): Connects to database for current information</li> <li>Without Update (<code>--update=false</code>): Uses cached data for faster response</li> </ul>"}, {"location": "commands/target/#information-depth", "title": "Information Depth", "text": "<ul> <li>Basic View: Fast snapshot listing</li> <li>With <code>--info</code>: Comprehensive analysis (slower due to database queries)</li> </ul>"}, {"location": "commands/target/#optimization-strategies", "title": "Optimization Strategies", "text": "<pre><code># Fast snapshot review for automation\ndbsnapper target production-db --update=false\n\n# Comprehensive analysis for planning\ndbsnapper target production-db --info --update\n\n# Focus on recent snapshots for quick decisions\ndbsnapper target production-db | head -20\n</code></pre>"}, {"location": "commands/target/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/target/#snapshot-selection-workflow", "title": "Snapshot Selection Workflow", "text": "<pre><code># 1. Review available snapshots\ndbsnapper target production-db\n\n# 2. Choose appropriate snapshot by index\ndbsnapper load development-db 2\n\n# 3. Verify loaded snapshot\ndbsnapper target development-db --info\n</code></pre>"}, {"location": "commands/target/#sanitization-workflow", "title": "Sanitization Workflow", "text": "<pre><code># 1. Check sanitization status\ndbsnapper target sensitive-db\n\n# 2. Sanitize unsanitized snapshots\ndbsnapper sanitize sensitive-db 3\n\n# 3. Verify sanitization completed\ndbsnapper target sensitive-db\n</code></pre>"}, {"location": "commands/target/#cloud-management-workflow", "title": "Cloud Management Workflow", "text": "<pre><code># 1. Review cloud snapshot availability\ndbsnapper target shared-data\n\n# 2. Pull needed snapshots locally\ndbsnapper pull shared-data 1\n\n# 3. Confirm local availability\ndbsnapper target shared-data\n</code></pre>"}, {"location": "commands/target/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/target/#common-issues", "title": "Common Issues", "text": "<p>Target Not Found <pre><code>Error: target 'unknown-target' not found\n</code></pre> - Verify target name with <code>dbsnapper targets</code> - Check configuration file for target definition - Ensure correct configuration file is being used</p> <p>Database Connection Failed <pre><code>Error: failed to connect to source database\n</code></pre> - Check database server availability - Verify connection string in target configuration - Test connectivity with database client tools - Use <code>--update=false</code> to skip database connections</p> <p>No Snapshots Found <pre><code>No snapshots found for target: test-target\n</code></pre> - Build first snapshot: <code>dbsnapper build test-target</code> - Check working directory for snapshot files - Verify storage profile configuration for cloud targets</p> <p>Permission Denied <pre><code>Error: insufficient privileges to analyze database\n</code></pre> - Check database user permissions - Ensure user has SELECT privileges on system catalogs - Verify authentication credentials</p> <p>Incomplete Statistics with <code>--info</code> <pre><code>Database Stats show -1 for row counts\n</code></pre> - Run with <code>--update</code> to refresh statistics - Check database user has privileges to query system tables - Some statistics may require elevated database privileges</p>"}, {"location": "commands/target/#recovery-steps", "title": "Recovery Steps", "text": "<ol> <li>Verify Configuration: Check target configuration and connection strings</li> <li>Test Connectivity: Ensure database servers are accessible</li> <li>Check Permissions: Verify database user privileges</li> <li>Use Minimal Options: Try without <code>--info</code> or <code>--update</code> flags</li> <li>Review Logs: Check for detailed error messages</li> </ol>"}, {"location": "commands/target/#performance-and-optimization", "title": "Performance and Optimization", "text": ""}, {"location": "commands/target/#command-execution-speed", "title": "Command Execution Speed", "text": "<ul> <li>Fastest: <code>dbsnapper target name --update=false</code> (cached data only)</li> <li>Medium: <code>dbsnapper target name</code> (snapshot list + basic database check)</li> <li>Slowest: <code>dbsnapper target name --info</code> (full database analysis)</li> </ul>"}, {"location": "commands/target/#large-snapshot-lists", "title": "Large Snapshot Lists", "text": "<p>For targets with many snapshots, consider: <pre><code># Limit output to recent snapshots\ndbsnapper target production-db | head -20\n\n# Use grep to find specific snapshots\ndbsnapper target production-db | grep \"25-Aug-07\"\n\n# Focus on sanitized snapshots only\ndbsnapper target production-db | grep \"true\"\n</code></pre></p> <p>Snapshot Selection</p> <p>Use the INDEX column values directly in other commands. Index 0 is always the most recent snapshot.</p> <p>Database Analysis</p> <p>The <code>--info</code> flag provides valuable insights for subset planning, schema analysis, and capacity planning. Use it when detailed database understanding is needed.</p> <p>Update Behavior</p> <p>The <code>--update</code> flag (default: true) connects to the source database. Use <code>--update=false</code> for faster execution when current database state isn't needed.</p>"}, {"location": "commands/target/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>targets</code> - List all available targets</li> <li><code>build</code> - Create new snapshots for targets</li> <li><code>load</code> - Load target snapshots into databases</li> <li><code>sanitize</code> - Create sanitized versions of snapshots</li> <li><code>pull</code> - Retrieve cloud snapshots locally</li> </ul>"}, {"location": "commands/target/#see-also", "title": "See Also", "text": "<ul> <li>Snapshot Management - Snapshot concepts and workflows</li> <li>Target Configuration - How to configure targets</li> <li>Database Engines - Supported databases</li> <li>Cloud Storage - Cloud storage integration</li> </ul>"}, {"location": "commands/targets/", "title": "Targets Command", "text": "<p>The <code>targets</code> command lists all available database targets from local configuration, cloud sources, and shared targets. This is the primary discovery command for understanding what databases are available for snapshot operations, providing a comprehensive overview of your DBSnapper environment.</p>"}, {"location": "commands/targets/#overview", "title": "Overview", "text": "<p>The targets command aggregates targets from multiple sources and displays them in a unified table format. It shows essential information about each target including connection details, storage profiles, and target types, helping users quickly identify available databases for snapshot operations.</p>"}, {"location": "commands/targets/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper targets [flags]\n</code></pre>"}, {"location": "commands/targets/#arguments", "title": "Arguments", "text": "<p>No arguments required - lists all available targets by default.</p>"}, {"location": "commands/targets/#options", "title": "Options", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will overwrite any existing destination database\n  -h, --help            help for targets  \n  -o, --original        Use the original snapshot instead of the sanitized version\n  -u, --update          Update the db info for all targets (default: true)\n</code></pre>"}, {"location": "commands/targets/#target-sources-and-types", "title": "Target Sources and Types", "text": "<p>DBSnapper aggregates targets from three different sources, clearly identifying each type in the output:</p>"}, {"location": "commands/targets/#local-targets-local", "title": "Local Targets (<code>local</code>)", "text": "<ul> <li>Source: Local configuration file (<code>~/.config/dbsnapper/dbsnapper.yml</code>)</li> <li>Configuration: Manually defined in the <code>targets</code> section</li> <li>Availability: Always available regardless of network connectivity</li> <li>Control: Full control over configuration and settings</li> </ul>"}, {"location": "commands/targets/#cloud-targets-cloud", "title": "Cloud Targets (<code>cloud</code>)", "text": "<ul> <li>Source: DBSnapper Cloud API</li> <li>Configuration: Defined via DBSnapper Cloud web interface</li> <li>Availability: Requires internet connectivity and authentication</li> <li>Control: Managed through cloud interface, shared with team members</li> </ul>"}, {"location": "commands/targets/#shared-targets-share", "title": "Shared Targets (<code>share</code>)", "text": "<ul> <li>Source: Okta SSO integration via DBSnapper Cloud</li> <li>Configuration: Shared by other organization members</li> <li>Availability: Requires cloud authentication and SSO setup</li> <li>Control: Read-only access to shared configurations</li> <li>Limitation: Only sanitized snapshots are accessible</li> </ul>"}, {"location": "commands/targets/#target-information-display", "title": "Target Information Display", "text": "<p>The targets command displays information in a tabular format with these columns:</p>"}, {"location": "commands/targets/#name", "title": "NAME", "text": "<p>The unique identifier for each target, used in other DBSnapper commands.</p>"}, {"location": "commands/targets/#type", "title": "TYPE", "text": "<ul> <li><code>local</code> - Locally configured target</li> <li><code>cloud</code> - Cloud-managed target  </li> <li><code>share</code> - Shared target via SSO</li> </ul>"}, {"location": "commands/targets/#src", "title": "SRC", "text": "<p>The source database connection string where snapshots are built from. - Format: <code>protocol://host:port/database</code> - Protocols: <code>pgl://</code> (PostgreSQL), <code>myl://</code> (MySQL), <code>pgd://</code> (PostgreSQL Docker), <code>myd://</code> (MySQL Docker)</p>"}, {"location": "commands/targets/#size", "title": "SIZE", "text": "<p>Current database size information (when available). - Updates: Controlled by <code>--update</code> flag - Performance: Disable updates for faster listing</p>"}, {"location": "commands/targets/#dst", "title": "DST", "text": "<p>The destination database connection string where snapshots are loaded to. - Optional: May be empty for shared targets - Override: Can be overridden with <code>--destdb</code> flag</p>"}, {"location": "commands/targets/#storage-profile", "title": "STORAGE PROFILE", "text": "<p>Associated cloud storage configuration for snapshot storage. - Empty: Local-only snapshots - Named: Cloud storage integration enabled</p>"}, {"location": "commands/targets/#target-update-behavior", "title": "Target Update Behavior", "text": "<p>The <code>--update</code> flag controls whether database information is refreshed:</p>"}, {"location": "commands/targets/#with-update-default-updatetrue", "title": "With Update (Default: <code>--update=true</code>)", "text": "<p><pre><code>dbsnapper targets\ndbsnapper targets --update\n</code></pre> - Queries each database to get current size and schema information - Slower execution due to database connections - Current information reflecting real-time database state - Use for: Planning operations, capacity planning, health checks</p>"}, {"location": "commands/targets/#without-update-updatefalse", "title": "Without Update (<code>--update=false</code>)", "text": "<p><pre><code>dbsnapper targets --update=false\n</code></pre> - Uses cached information from previous queries - Faster execution with no database connections required - Cached information may be outdated - Use for: Quick target discovery, automated scripts, offline environments</p>"}, {"location": "commands/targets/#cloud-integration", "title": "Cloud Integration", "text": ""}, {"location": "commands/targets/#authentication-required", "title": "Authentication Required", "text": "<p>Cloud and shared targets require DBSnapper Cloud authentication:</p> <pre><code># Set up cloud authentication first\ndbsnapper auth token &lt;your_api_token&gt;\n\n# Then list all targets including cloud and shared\ndbsnapper targets\n</code></pre>"}, {"location": "commands/targets/#cloud-mode-control", "title": "Cloud Mode Control", "text": "<pre><code># Disable cloud integration for faster local-only listing\ndbsnapper targets --nocloud\n</code></pre>"}, {"location": "commands/targets/#example-output", "title": "Example Output", "text": ""}, {"location": "commands/targets/#multi-source-target-listing", "title": "Multi-Source Target Listing", "text": "<pre><code>dbsnapper targets\n</code></pre> <p>Output: <pre><code>DBSnapper v2.7.0-dev - List Targets\n[CLOUD] DBSnapper Cloud: Enabled\n[CONFIG] Config: ~/.config/dbsnapper/dbsnapper.yml\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nListing all targets, update = true\n\n+----------------------+-------+----------------------------------+--------+----------------------------------+------------------+\n|        NAME          | TYPE  |               SRC                |  SIZE  |               DST                | STORAGE PROFILE  |\n+----------------------+-------+----------------------------------+--------+----------------------------------+------------------+\n| production-db        | cloud | pgl://prod:5432/app              | 2.3 GB | pgl://staging:5432/app_snap      | aws-s3-prod      |\n| analytics-db         | cloud | pgl://analytics:5432/data        | 847 MB | pgl://dev:5432/analytics_snap    | aws-s3-analytics |\n|                      | ----- |                                  |        |                                  |                  |\n| dev-postgres         | local | pgl://localhost:5432/myapp       | 45 MB  | pgl://localhost:5432/myapp_test  |                  |\n| dev-mysql            | local | myl://localhost:3306/webapp      | 123 MB | myl://localhost:3306/webapp_test |                  |\n| docker-test          | local | pgd://postgres17:5432/testdb     | 12 MB  | pgd://postgres17:5432/testdb_snap|                  |\n|                      | ----- |                                  |        |                                  |                  |\n| team-shared-prod     | share |                                  |        | pgl://local:5432/shared_prod     | r2-shared-prod   |\n| marketing-data       | share |                                  |        | pgl://local:5432/marketing       | s3-marketing     |\n+----------------------+-------+----------------------------------+--------+----------------------------------+------------------+\n</code></pre></p>"}, {"location": "commands/targets/#local-only-target-listing", "title": "Local-Only Target Listing", "text": "<pre><code>dbsnapper targets --nocloud\n</code></pre> <p>Output: <pre><code>DBSnapper v2.7.0-dev - List Targets\n[CLOUD] DBSnapper Cloud: Disabled\n[CONFIG] Config: ~/.config/dbsnapper/dbsnapper.yml\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n+----------------------+-------+----------------------------------+--------+----------------------------------+------------------+\n|        NAME          | TYPE  |               SRC                |  SIZE  |               DST                | STORAGE PROFILE  |\n+----------------------+-------+----------------------------------+--------+----------------------------------+------------------+\n| dev-postgres         | local | pgl://localhost:5432/myapp       | 45 MB  | pgl://localhost:5432/myapp_test  |                  |\n| dev-mysql            | local | myl://localhost:3306/webapp      | 123 MB | myl://localhost:3306/webapp_test |                  |\n| docker-test          | local | pgd://postgres17:5432/testdb     | 12 MB  | pgd://postgres17:5432/testdb_snap|                  |\n+----------------------+-------+----------------------------------+--------+----------------------------------+------------------+\n</code></pre></p>"}, {"location": "commands/targets/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/targets/#basic-target-discovery", "title": "Basic Target Discovery", "text": "<pre><code># List all available targets with current information\ndbsnapper targets\n\n# Quick target listing without database updates\ndbsnapper targets --update=false\n\n# Local targets only (no cloud connectivity required)\ndbsnapper targets --nocloud\n</code></pre>"}, {"location": "commands/targets/#target-validation-and-planning", "title": "Target Validation and Planning", "text": "<pre><code># Get current database sizes for capacity planning\ndbsnapper targets --update\n\n# Check available targets before building snapshots\ndbsnapper targets\ndbsnapper build production-db\n</code></pre>"}, {"location": "commands/targets/#automation-and-scripting", "title": "Automation and Scripting", "text": "<pre><code># Fast target enumeration for automated scripts\ndbsnapper targets --update=false --nocloud\n\n# Pipeline validation - ensure required targets exist\nif dbsnapper targets --nocloud | grep -q \"production-db\"; then\n    echo \"Production target available\"\n    dbsnapper build production-db\nfi\n</code></pre>"}, {"location": "commands/targets/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/targets/#development-environment-setup", "title": "Development Environment Setup", "text": "<pre><code># Discover available development targets\ndbsnapper targets --nocloud\n# Choose appropriate target for development database refresh\ndbsnapper load dev-postgres\n</code></pre>"}, {"location": "commands/targets/#production-database-management", "title": "Production Database Management", "text": "<pre><code># View production targets with current sizes\ndbsnapper targets | grep cloud\n# Plan snapshot operations based on database sizes\n</code></pre>"}, {"location": "commands/targets/#team-collaboration", "title": "Team Collaboration", "text": "<pre><code># Discover shared targets from team members\ndbsnapper targets | grep share\n# Access shared sanitized snapshots\ndbsnapper load team-shared-prod\n</code></pre>"}, {"location": "commands/targets/#cicd-pipeline-integration", "title": "CI/CD Pipeline Integration", "text": "<pre><code># Fast target validation in automated pipelines\ntargets_count=$(dbsnapper targets --update=false --nocloud | grep -c local)\nif [ \"$targets_count\" -gt 0 ]; then\n    echo \"Targets available, proceeding with snapshot operations\"\nfi\n</code></pre>"}, {"location": "commands/targets/#capacity-planning-and-monitoring", "title": "Capacity Planning and Monitoring", "text": "<pre><code># Get current database sizes for all targets\ndbsnapper targets --update | tee target_sizes.log\n# Monitor database growth over time\n</code></pre>"}, {"location": "commands/targets/#database-connection-protocols", "title": "Database Connection Protocols", "text": "<p>The targets command displays various database connection protocols:</p>"}, {"location": "commands/targets/#postgresql", "title": "PostgreSQL", "text": "<ul> <li><code>pgl://</code> - Local PostgreSQL instance</li> <li><code>pgd://</code> - PostgreSQL Docker container</li> </ul>"}, {"location": "commands/targets/#mysql", "title": "MySQL", "text": "<ul> <li><code>myl://</code> - Local MySQL instance</li> <li><code>myd://</code> - MySQL Docker container</li> </ul>"}, {"location": "commands/targets/#connection-string-format", "title": "Connection String Format", "text": "<pre><code>protocol://[user[:password]@]host[:port]/database[?options]\n</code></pre> <p>Examples: <pre><code>pgl://localhost:5432/myapp\nmyl://user:pass@prod-host:3306/webapp?tls=true\npgd://postgres17:5432/testdb\n</code></pre></p>"}, {"location": "commands/targets/#cloud-storage-integration", "title": "Cloud Storage Integration", "text": ""}, {"location": "commands/targets/#storage-profile-display", "title": "Storage Profile Display", "text": "<p>The <code>STORAGE PROFILE</code> column indicates cloud storage configuration:</p> <ul> <li>Empty: Snapshots stored locally only</li> <li>Profile Name: Cloud storage enabled with specified profile</li> </ul>"}, {"location": "commands/targets/#common-storage-profile-types", "title": "Common Storage Profile Types", "text": "<ul> <li>AWS S3: <code>aws-s3-prod</code>, <code>s3-backups</code></li> <li>Cloudflare R2: <code>r2-snapshots</code>, <code>r2-shared-prod</code> </li> <li>Custom: Organization-specific naming conventions</li> </ul>"}, {"location": "commands/targets/#performance-considerations", "title": "Performance Considerations", "text": ""}, {"location": "commands/targets/#update-behavior-impact", "title": "Update Behavior Impact", "text": "<ul> <li>With Updates: Slower but current information</li> <li>Without Updates: Faster but potentially stale information</li> </ul>"}, {"location": "commands/targets/#cloud-integration-impact", "title": "Cloud Integration Impact", "text": "<ul> <li>Cloud Enabled: Additional network requests for cloud/shared targets</li> <li>Cloud Disabled: Local targets only, faster execution</li> </ul>"}, {"location": "commands/targets/#optimization-strategies", "title": "Optimization Strategies", "text": "<pre><code># For frequent target checking in scripts\ndbsnapper targets --update=false --nocloud\n\n# For planning and capacity management\ndbsnapper targets --update\n\n# For team collaboration discovery\ndbsnapper targets | grep -E \"(cloud|share)\"\n</code></pre>"}, {"location": "commands/targets/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/targets/#common-issues", "title": "Common Issues", "text": "<p>No Targets Found <pre><code>No targets found\n</code></pre> - Check configuration file exists: <code>~/.config/dbsnapper/dbsnapper.yml</code> - Verify targets are defined in configuration - Run <code>dbsnapper config init</code> if starting fresh</p> <p>Cloud Targets Not Appearing <pre><code>Only local targets shown, no cloud targets\n</code></pre> - Verify cloud authentication: <code>dbsnapper auth token &lt;token&gt;</code> - Check internet connectivity - Ensure cloud mode not disabled: remove <code>--nocloud</code> flag</p> <p>Database Connection Timeouts <pre><code>Error: timeout connecting to database\n</code></pre> - Use <code>--update=false</code> to skip database connections - Verify database servers are running and accessible - Check network connectivity and firewall settings</p> <p>Shared Targets Not Visible <pre><code>No shared targets in listing\n</code></pre> - Verify Okta SSO integration is configured - Check organization membership and permissions - Ensure shared targets have been created by team members</p> <p>Permission Denied Errors <pre><code>Error: permission denied accessing target\n</code></pre> - Check database credentials in target configuration - Verify database user has required permissions - Test connectivity with database client tools</p>"}, {"location": "commands/targets/#recovery-steps", "title": "Recovery Steps", "text": "<ol> <li>Verify Configuration: Check configuration file exists and is valid</li> <li>Test Connectivity: Verify database and cloud connectivity</li> <li>Check Authentication: Ensure cloud authentication is set up correctly</li> <li>Use Minimal Flags: Try <code>--update=false --nocloud</code> for basic operation</li> <li>Validate Targets: Use <code>dbsnapper config validate</code> to check target definitions</li> </ol>"}, {"location": "commands/targets/#integration-with-other-commands", "title": "Integration with Other Commands", "text": ""}, {"location": "commands/targets/#target-selection-for-operations", "title": "Target Selection for Operations", "text": "<pre><code># List targets to find the one you need\ndbsnapper targets\n\n# Use target name in other commands\ndbsnapper build production-db\ndbsnapper load dev-postgres\ndbsnapper sanitize team-data\n</code></pre>"}, {"location": "commands/targets/#target-details-investigation", "title": "Target Details Investigation", "text": "<pre><code># List all targets to find interesting ones\ndbsnapper targets\n\n# Get detailed information about specific target\ndbsnapper target production-db\n</code></pre>"}, {"location": "commands/targets/#snapshot-management-workflow", "title": "Snapshot Management Workflow", "text": "<pre><code># 1. Discover available targets\ndbsnapper targets\n\n# 2. Build snapshot from desired target\ndbsnapper build production-db\n\n# 3. Load snapshot to development target\ndbsnapper load dev-postgres\n</code></pre> <p>Quick Discovery</p> <p>Use <code>dbsnapper targets --update=false</code> for fast target discovery in scripts and automation where current database sizes aren't needed.</p> <p>Target Types</p> <p>The three target types (local, cloud, share) represent different sources and access levels: - Local: Full control, locally configured - Cloud: Team-managed via web interface - Share: Read-only access to team snapshots</p> <p>Database Updates</p> <p>The <code>--update</code> flag (default: true) connects to each database to get current information. Disable with <code>--update=false</code> for faster execution when current sizes aren't needed.</p>"}, {"location": "commands/targets/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>target</code> - View detailed information about a specific target</li> <li><code>build</code> - Create snapshots from targets</li> <li><code>load</code> - Load snapshots into target databases</li> <li><code>config init</code> - Initialize configuration with targets</li> </ul>"}, {"location": "commands/targets/#see-also", "title": "See Also", "text": "<ul> <li>Target Configuration - How to configure targets</li> <li>DBSnapper Cloud - Cloud integration and shared targets</li> <li>Storage Profiles - Cloud storage configuration</li> <li>Database Engines - Supported database types</li> </ul>"}, {"location": "database-engines/docker-integration/", "title": "Docker Integration", "text": ""}, {"location": "database-engines/docker-integration/#overview", "title": "Overview", "text": "<p>DBSnapper provides seamless Docker integration for database operations, whether you're running DBSnapper on your local machine or within containerized environments. The Docker integration handles everything from container lifecycle management to network configuration automatically.</p>"}, {"location": "database-engines/docker-integration/#docker-engine-types", "title": "Docker Engine Types", "text": ""}, {"location": "database-engines/docker-integration/#local-docker-usage", "title": "Local Docker Usage", "text": "<p>When DBSnapper runs on your local machine with Docker engines:</p> <pre><code>targets:\n  local-docker-usage:\n    snapshot:\n      # DBSnapper creates temporary containers for these operations\n      src_url: \"pgdocker://user:pass@remote-db:5432/production\"\n      dst_url: \"mydocker://user:pass@local-mysql:3306/development\"\n</code></pre> <p>How it works: - DBSnapper creates temporary containers with PostgreSQL/MySQL tools - Containers connect to your databases over the network - Data is processed within isolated containers - Containers are automatically cleaned up after operations</p>"}, {"location": "database-engines/docker-integration/#docker-in-docker-dind-support", "title": "Docker-in-Docker (DinD) Support", "text": "<p>When DBSnapper itself runs inside a Docker container:</p> <pre><code># DBSnapper automatically detects Docker-in-Docker scenarios\ntargets:\n  dind-aware:\n    snapshot:\n      src_url: \"pgdocker://user:pass@database:5432/app\"\n      dst_url: \"postgres://user:pass@localhost:5432/app_copy\"\n</code></pre> <p>Automatic adaptations: - Volume mounting strategies adjust for container environments - Network connectivity uses container-to-container communication - File system access adapts to container constraints</p>"}, {"location": "database-engines/docker-integration/#container-lifecycle-management", "title": "Container Lifecycle Management", "text": ""}, {"location": "database-engines/docker-integration/#automatic-container-operations", "title": "Automatic Container Operations", "text": "<p>DBSnapper handles all container operations transparently:</p> <pre><code>targets:\n  managed-containers:\n    snapshot:\n      src_url: \"pgdocker://user:pass@prod:5432/app\"\n      dst_url: \"mydocker://user:pass@dev:3306/app\"\n</code></pre> <p>Lifecycle stages: 1. Image Pull: Automatically pulls required Docker images 2. Container Creation: Creates containers with appropriate configurations 3. Network Setup: Configures isolated networks for operations 4. Tool Execution: Runs database tools within containers 5. Data Transfer: Manages file transfers between containers and host 6. Cleanup: Removes containers and networks after completion</p>"}, {"location": "database-engines/docker-integration/#resource-management", "title": "Resource Management", "text": "<p>Control container resources for optimal performance:</p> <pre><code>targets:\n  resource-controlled:\n    cpus: 6  # Affects container CPU allocation\n\n    snapshot:\n      src_url: \"pgdocker://user:pass@source:5432/large_db\"\n      dst_url: \"mydocker://user:pass@dest:3306/large_db_copy\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#network-configuration", "title": "Network Configuration", "text": ""}, {"location": "database-engines/docker-integration/#automatic-network-creation", "title": "Automatic Network Creation", "text": "<p>DBSnapper creates isolated networks for secure container communication:</p> <pre><code>targets:\n  networked-operation:\n    # Network automatically created and managed\n    snapshot:\n      src_url: \"pgdocker://user:pass@postgres-server:5432/app\"\n      dst_url: \"mydocker://user:pass@mysql-server:3306/app\"\n</code></pre> <p>Network features: - Bridge networks for container-to-container communication - DNS resolution between containers - Port mapping for external database connections - Automatic network cleanup after operations</p>"}, {"location": "database-engines/docker-integration/#custom-network-configuration", "title": "Custom Network Configuration", "text": "<p>For advanced scenarios, DBSnapper adapts to existing networks:</p> <pre><code>targets:\n  existing-network:\n    # DBSnapper works with existing Docker networks\n    snapshot:\n      src_url: \"pgdocker://user:pass@existing-postgres:5432/app\"\n      dst_url: \"mydocker://user:pass@existing-mysql:3306/app\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#volume-management", "title": "Volume Management", "text": ""}, {"location": "database-engines/docker-integration/#file-system-access", "title": "File System Access", "text": "<p>DBSnapper automatically handles file system mounting:</p> <pre><code>targets:\n  file-system-managed:\n    # Snapshot files automatically managed\n    snapshot:\n      src_url: \"pgdocker://user:pass@source:5432/db\"\n      dst_url: \"mydocker://user:pass@dest:3306/db\"\n</code></pre> <p>Volume handling: - Host directories mounted into containers - Temporary files managed automatically - Cross-platform path resolution - Docker-in-Docker volume sharing</p>"}, {"location": "database-engines/docker-integration/#working-directory-configuration", "title": "Working Directory Configuration", "text": "<p>Configure where snapshot files are stored:</p> <pre><code># In dbsnapper configuration\ndefaults:\n  working_directory: \"./snapshots\"\n\ntargets:\n  custom-location:\n    snapshot:\n      # Files stored in configured working directory\n      src_url: \"pgdocker://user:pass@source:5432/app\"\n      dst_url: \"mydocker://user:pass@dest:3306/app\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#docker-image-configuration", "title": "Docker Image Configuration", "text": ""}, {"location": "database-engines/docker-integration/#default-images", "title": "Default Images", "text": "<p>DBSnapper uses optimized default images:</p> <pre><code># Default image configuration (automatic)\ndocker:\n  images:\n    postgres: \"postgres:16-alpine\"  # PostgreSQL operations\n    mysql: \"mysql:8-oracle\"         # MySQL operations\n</code></pre>"}, {"location": "database-engines/docker-integration/#custom-images", "title": "Custom Images", "text": "<p>Configure custom Docker images for specific needs:</p> <pre><code># Custom image configuration\ndocker:\n  images:\n    postgres: \"postgres:15-alpine\"  # Use PostgreSQL 15\n    mysql: \"mysql:8.0\"              # Use different MySQL image\n\ntargets:\n  custom-images:\n    snapshot:\n      # Uses configured custom images\n      src_url: \"pgdocker://user:pass@source:5432/app\"\n      dst_url: \"mydocker://user:pass@dest:3306/app\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#image-requirements", "title": "Image Requirements", "text": "<p>PostgreSQL Images: - Must include <code>pg_dump</code>, <code>pg_restore</code>, and <code>psql</code> - Recommended: <code>postgres:16-alpine</code> or <code>postgres:15-alpine</code> - Compatible with PostgreSQL 9.2+</p> <p>MySQL Images: - Must include <code>mysqldump</code> and <code>mysql</code> commands - Recommended: <code>mysql:8-oracle</code> (includes <code>mysqlsh</code>) - Compatible with MySQL 5.7+</p>"}, {"location": "database-engines/docker-integration/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "database-engines/docker-integration/#container-isolation", "title": "Container Isolation", "text": "<p>Docker engines provide strong security isolation:</p> <pre><code>targets:\n  secure-isolation:\n    snapshot:\n      # Each operation runs in isolated containers\n      src_url: \"pgdocker://readonly:pass@prod:5432/sensitive_db\"\n      dst_url: \"mydocker://dev:pass@dev:3306/sanitized_db\"\n</code></pre> <p>Security features: - Containers run with minimal required permissions - No persistent state between operations - Isolated networks prevent unauthorized access - Automatic credential management</p>"}, {"location": "database-engines/docker-integration/#credential-handling", "title": "Credential Handling", "text": "<p>DBSnapper securely manages database credentials in containers:</p> <pre><code>targets:\n  credential-security:\n    snapshot:\n      # Credentials passed securely to containers\n      src_url: \"pgdocker://app_readonly:${PROD_PASSWORD}@prod:5432/app\"\n      dst_url: \"mydocker://app_dev:${DEV_PASSWORD}@dev:3306/app\"\n</code></pre> <p>Best practices: - Use environment variables for sensitive credentials - Employ read-only database users for source connections - Limit destination database privileges - Rotate credentials regularly</p>"}, {"location": "database-engines/docker-integration/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "database-engines/docker-integration/#container-performance", "title": "Container Performance", "text": "<p>Optimize container performance for large operations:</p> <pre><code>targets:\n  performance-optimized:\n    cpus: 8  # Allocate more CPU resources\n\n    snapshot:\n      src_url: \"pgdocker://user:pass@warehouse:5432/analytics\"\n      dst_url: \"mydocker://user:pass@dest:3306/analytics_copy\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#resource-allocation-strategies", "title": "Resource Allocation Strategies", "text": "<p>For Small Databases (&lt; 1GB): <pre><code>targets:\n  small-db:\n    cpus: 2  # Minimal resources sufficient\n</code></pre></p> <p>For Medium Databases (1-10GB): <pre><code>targets:\n  medium-db:\n    cpus: 4  # Balanced resource allocation\n</code></pre></p> <p>For Large Databases (&gt; 10GB): <pre><code>targets:\n  large-db:\n    cpus: 8  # Maximum resources for best performance\n</code></pre></p>"}, {"location": "database-engines/docker-integration/#memory-management", "title": "Memory Management", "text": "<p>DBSnapper automatically manages container memory: - Containers sized based on data volume - Streaming processing for large datasets - Automatic garbage collection - Memory-efficient data transfer</p>"}, {"location": "database-engines/docker-integration/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "database-engines/docker-integration/#common-docker-issues", "title": "Common Docker Issues", "text": "<p>Docker Not Available: <pre><code># Verify Docker installation\ndocker --version\ndocker info\ndocker ps\n</code></pre></p> <p>Image Pull Failures: <pre><code># Manually pull required images\ndocker pull postgres:16-alpine\ndocker pull mysql:8-oracle\n\n# Verify image availability\ndocker images | grep postgres\ndocker images | grep mysql\n</code></pre></p> <p>Container Creation Problems: <pre><code># Check Docker daemon status\nsudo systemctl status docker  # Linux\n# or\nopen -a Docker  # macOS\n\n# Verify Docker permissions\ndocker run --rm hello-world\n</code></pre></p> <p>Network Connectivity Issues: <pre><code># Test container networking\ndocker network ls\ndocker run --rm postgres:16-alpine ping -c 1 google.com\n\n# Check port availability\nnetstat -tlnp | grep :5432  # PostgreSQL\nnetstat -tlnp | grep :3306  # MySQL\n</code></pre></p>"}, {"location": "database-engines/docker-integration/#resource-issues", "title": "Resource Issues", "text": "<p>Insufficient Memory: <pre><code># Check available system resources\nfree -h  # Linux\ntop      # macOS/Linux\n\n# Monitor Docker resource usage\ndocker stats\n</code></pre></p> <p>Disk Space Problems: <pre><code># Check Docker disk usage\ndocker system df\ndocker system prune  # Clean up unused containers/images\n\n# Check available disk space\ndf -h\n</code></pre></p>"}, {"location": "database-engines/docker-integration/#debug-commands", "title": "Debug Commands", "text": "<pre><code># Verbose operation logging\ndbsnapper build target-name --verbose\n\n# Monitor container creation\ndocker events --filter type=container\n\n# Check container logs (during operation)\ndocker logs $(docker ps -l -q)\n\n# Inspect container configuration\ndocker inspect $(docker ps -l -q)\n</code></pre>"}, {"location": "database-engines/docker-integration/#best-practices", "title": "Best Practices", "text": ""}, {"location": "database-engines/docker-integration/#performance-best-practices", "title": "Performance Best Practices", "text": "<pre><code>targets:\n  optimized-setup:\n    # Match CPU allocation to data size\n    cpus: 4\n\n    snapshot:\n      # Use efficient connection strings\n      src_url: \"pgdocker://user:pass@source:5432/db?connect_timeout=30\"\n      dst_url: \"mydocker://user:pass@dest:3306/db?timeout=60s\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#security-best-practices", "title": "Security Best Practices", "text": "<pre><code>targets:\n  secure-setup:\n    snapshot:\n      # Use read-only credentials for source\n      src_url: \"pgdocker://readonly:${READONLY_PASS}@prod:5432/app\"\n\n      # Use limited credentials for destination\n      dst_url: \"mydocker://dev:${DEV_PASS}@dev:3306/app\"\n</code></pre>"}, {"location": "database-engines/docker-integration/#resource-management_1", "title": "Resource Management", "text": "<ul> <li>Pre-pull Docker images during setup to avoid delays</li> <li>Monitor container resource usage during operations</li> <li>Clean up unused images regularly to save disk space</li> <li>Use appropriate CPU allocation based on database size</li> </ul>"}, {"location": "database-engines/docker-integration/#network-configuration_1", "title": "Network Configuration", "text": "<ul> <li>Use isolated networks for sensitive operations</li> <li>Configure connection timeouts for unreliable networks</li> <li>Test connectivity before running large operations</li> <li>Monitor network traffic during data transfers</li> </ul>"}, {"location": "database-engines/docker-integration/#docker-requirements", "title": "Docker Requirements", "text": ""}, {"location": "database-engines/docker-integration/#system-requirements", "title": "System Requirements", "text": "<ul> <li>Docker Engine: 20.10+ recommended</li> <li>Docker Compose: 2.0+ (for complex setups)</li> <li>System Resources: </li> <li>4GB+ RAM for medium databases</li> <li>8GB+ RAM for large databases</li> <li>2+ CPU cores recommended</li> <li>Network: Stable internet for image pulls</li> <li>Storage: Adequate space for images and temporary data</li> </ul>"}, {"location": "database-engines/docker-integration/#installation-verification", "title": "Installation Verification", "text": "<pre><code># Comprehensive Docker verification\ndocker --version\ndocker info\ndocker run --rm hello-world\n\n# Test required images\ndocker run --rm postgres:16-alpine pg_dump --version\ndocker run --rm mysql:8-oracle mysql --version\n\n# Test networking\ndocker network create test-network\ndocker network rm test-network\n\n# Test volume mounting\ndocker run --rm -v $(pwd):/data postgres:16-alpine ls -la /data\n</code></pre>"}, {"location": "database-engines/docker-integration/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>PostgreSQL Docker Engine - PostgreSQL containerization details</li> <li>MySQL Docker Engine - MySQL containerization details  </li> <li>Ephemeral Databases - Temporary container databases</li> <li>Database Engines Introduction - Engine architecture overview</li> </ul>"}, {"location": "database-engines/engine-configuration/", "title": "Engine Configuration Reference", "text": ""}, {"location": "database-engines/engine-configuration/#overview", "title": "Overview", "text": "<p>This reference provides comprehensive configuration examples for all database engines, covering common scenarios from basic setups to advanced production deployments. Use these examples as starting points for your specific requirements.</p>"}, {"location": "database-engines/engine-configuration/#target-configuration-structure", "title": "Target Configuration Structure", "text": "<p>All database engines use the same basic target structure:</p> <pre><code>targets:\n  target-name:\n    # CPU allocation for parallel operations\n    cpus: 4\n\n    # Snapshot configuration\n    snapshot:\n      src_url: \"engine://user:pass@host:port/database\"\n      dst_url: \"engine://user:pass@host:port/database\" \n\n      # PostgreSQL only: Schema filtering\n      schema_config:\n        include_schemas: [\"schema1\", \"schema2\"]\n        exclude_schemas: [\"temp_schema\"]\n\n    # Optional: Data sanitization\n    sanitize:\n      dst_url: \"engine://user:pass@host:port/sanitized_database\"\n      override_query: |\n        UPDATE sensitive_table SET data = 'sanitized';\n</code></pre>"}, {"location": "database-engines/engine-configuration/#postgresql-local-engine", "title": "PostgreSQL Local Engine", "text": ""}, {"location": "database-engines/engine-configuration/#basic-configuration", "title": "Basic Configuration", "text": "<pre><code>targets:\n  postgres-local-basic:\n    snapshot:\n      src_url: \"postgres://user:password@localhost:5432/production\"\n      dst_url: \"postgres://user:password@localhost:5432/development\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#production-with-schema-filtering", "title": "Production with Schema Filtering", "text": "<pre><code>targets:\n  postgres-production:\n    cpus: 8\n\n    snapshot:\n      src_url: \"postgres://readonly:${PROD_PASSWORD}@prod.db.company.com:5432/application?sslmode=require\"\n      dst_url: \"postgres://dev:${DEV_PASSWORD}@dev.db.company.com:5432/application_dev?sslmode=disable\"\n\n      schema_config:\n        include_schemas:\n          - \"public\"\n          - \"app_data\"\n          - \"user_profiles\"\n        exclude_schemas:\n          - \"audit_logs\"\n          - \"temp_processing\"\n          - \"migration_backup\"\n\n    sanitize:\n      dst_url: \"postgres://dev:${DEV_PASSWORD}@dev.db.company.com:5432/application_sanitized\"\n      override_query: |\n        -- Remove sensitive user data\n        UPDATE users SET \n          email = 'user' || id || '@example.com',\n          phone = regexp_replace(phone, '\\d', 'X', 'g'),\n          ssn = NULL,\n          date_of_birth = '1990-01-01';\n\n        -- Anonymize addresses\n        UPDATE addresses SET \n          street_address = 'Test Street ' || id,\n          city = 'Test City',\n          postal_code = '00000',\n          latitude = NULL,\n          longitude = NULL;\n\n        -- Clear payment information\n        UPDATE payment_methods SET\n          card_number = 'XXXX-XXXX-XXXX-1234',\n          expiry_date = '12/25',\n          cvv = NULL;\n</code></pre>"}, {"location": "database-engines/engine-configuration/#ssl-configuration", "title": "SSL Configuration", "text": "<pre><code>targets:\n  postgres-ssl:\n    snapshot:\n      src_url: \"postgres://app:pass@secure.db.com:5432/app?sslmode=require&amp;sslcert=client.crt&amp;sslkey=client.key&amp;sslrootcert=ca.crt\"\n      dst_url: \"postgres://dev:pass@localhost:5432/app_copy?sslmode=disable\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#high-performance-large-database", "title": "High-Performance Large Database", "text": "<pre><code>targets:\n  postgres-warehouse:\n    cpus: 16  # Maximum parallel processing\n\n    snapshot:\n      src_url: \"postgres://analytics:${WAREHOUSE_PASSWORD}@warehouse.company.com:5432/analytics?connect_timeout=60&amp;statement_timeout=300000\"\n      dst_url: \"postgres://dev:${DEV_PASSWORD}@dev-warehouse:5432/analytics_sample\"\n\n      schema_config:\n        # Only include essential schemas for development\n        include_schemas:\n          - \"public\"\n          - \"core_data\"  \n          - \"reporting\"\n        exclude_schemas:\n          - \"raw_imports\"      # Large temporary data\n          - \"historical_archive\"  # Old data not needed for dev\n          - \"audit_trail\"      # Sensitive audit data\n</code></pre>"}, {"location": "database-engines/engine-configuration/#postgresql-docker-engine", "title": "PostgreSQL Docker Engine", "text": ""}, {"location": "database-engines/engine-configuration/#basic-docker-configuration", "title": "Basic Docker Configuration", "text": "<pre><code>targets:\n  postgres-docker-basic:\n    snapshot:\n      src_url: \"pgdocker://user:password@remote.db.com:5432/production\"\n      dst_url: \"pgdocker://user:password@localhost:5432/development\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#cicd-pipeline-configuration", "title": "CI/CD Pipeline Configuration", "text": "<pre><code>targets:\n  postgres-ci-pipeline:\n    cpus: 4\n\n    snapshot:\n      src_url: \"pgdocker://ci_user:${CI_DB_PASSWORD}@test-db:5432/integration_tests\"\n      dst_url: \"pgdocker://ci_user:${CI_DB_PASSWORD}@localhost:5432/test_results\"\n\n      schema_config:\n        include_schemas: [\"public\", \"test_data\"]\n\n    sanitize:\n      dst_url: \"pgdocker://ci_user:${CI_DB_PASSWORD}@localhost:5432/sanitized_tests\"\n      override_query: |\n        -- Sanitize test data for sharing\n        UPDATE test_users SET \n          email = 'test' || id || '@testcompany.com',\n          name = 'Test User ' || id;\n</code></pre>"}, {"location": "database-engines/engine-configuration/#custom-docker-image", "title": "Custom Docker Image", "text": "<pre><code># Global Docker configuration\ndocker:\n  images:\n    postgres: \"postgres:15-alpine\"  # Use PostgreSQL 15\n\ntargets:\n  postgres-custom-image:\n    snapshot:\n      src_url: \"pgdocker://user:pass@source:5432/app\"\n      dst_url: \"pgdocker://user:pass@dest:5432/app_copy\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#mysql-local-engine", "title": "MySQL Local Engine", "text": ""}, {"location": "database-engines/engine-configuration/#basic-configuration_1", "title": "Basic Configuration", "text": "<pre><code>targets:\n  mysql-local-basic:\n    snapshot:\n      src_url: \"mysql://user:password@localhost:3306/production\"\n      dst_url: \"mysql://user:password@localhost:3306/development\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#production-configuration", "title": "Production Configuration", "text": "<pre><code>targets:\n  mysql-production:\n    cpus: 6\n\n    snapshot:\n      src_url: \"mysql://readonly:${PROD_PASSWORD}@prod-mysql.company.com:3306/ecommerce?tls=true&amp;charset=utf8mb4\"\n      dst_url: \"mysql://dev:${DEV_PASSWORD}@dev-mysql.company.com:3306/ecommerce_dev?charset=utf8mb4\"\n\n    sanitize:\n      dst_url: \"mysql://dev:${DEV_PASSWORD}@dev-mysql.company.com:3306/ecommerce_sanitized\"\n      override_query: |\n        -- Sanitize customer data\n        UPDATE customers SET \n          email = CONCAT('customer', id, '@example.com'),\n          phone = REGEXP_REPLACE(phone, '[0-9]', 'X'),\n          first_name = CONCAT('Customer', id),\n          last_name = 'Test',\n          date_of_birth = '1990-01-01';\n\n        -- Clear sensitive order information\n        UPDATE orders SET\n          billing_address = 'Test Address',\n          shipping_address = 'Test Address',\n          payment_method = 'Test Card';\n\n        -- Anonymize payment data\n        UPDATE payment_transactions SET\n          card_last_four = '1234',\n          transaction_id = CONCAT('TEST_', id),\n          processor_response = 'TEST_APPROVED';\n</code></pre>"}, {"location": "database-engines/engine-configuration/#multi-database-configuration", "title": "Multi-Database Configuration", "text": "<pre><code>targets:\n  mysql-customers:\n    snapshot:\n      src_url: \"mysql://app:${PROD_PASSWORD}@prod-cluster:3306/customers\"\n      dst_url: \"mysql://dev:${DEV_PASSWORD}@dev-mysql:3306/customers_dev\"\n\n  mysql-orders:\n    snapshot:\n      src_url: \"mysql://app:${PROD_PASSWORD}@prod-cluster:3306/orders\"\n      dst_url: \"mysql://dev:${DEV_PASSWORD}@dev-mysql:3306/orders_dev\"\n\n  mysql-inventory:\n    snapshot:\n      src_url: \"mysql://app:${PROD_PASSWORD}@prod-cluster:3306/inventory\"\n      dst_url: \"mysql://dev:${DEV_PASSWORD}@dev-mysql:3306/inventory_dev\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#performance-optimized-configuration", "title": "Performance-Optimized Configuration", "text": "<pre><code>targets:\n  mysql-performance:\n    cpus: 8\n\n    snapshot:\n      src_url: \"mysql://analytics:${WAREHOUSE_PASSWORD}@mysql-warehouse:3306/analytics?timeout=300s&amp;readTimeout=120s&amp;writeTimeout=120s\"\n      dst_url: \"mysql://dev:${DEV_PASSWORD}@dev-mysql:3306/analytics_sample\"\n\n    sanitize:\n      override_query: |\n        -- Batch process large tables for better performance\n        UPDATE user_events SET \n          user_id = FLOOR(RAND() * 10000),\n          ip_address = '192.168.1.1',\n          user_agent = 'Test Browser'\n        WHERE created_date &gt;= '2024-01-01'\n        LIMIT 100000;\n</code></pre>"}, {"location": "database-engines/engine-configuration/#mysql-docker-engine", "title": "MySQL Docker Engine", "text": ""}, {"location": "database-engines/engine-configuration/#basic-docker-configuration_1", "title": "Basic Docker Configuration", "text": "<pre><code>targets:\n  mysql-docker-basic:\n    snapshot:\n      src_url: \"mydocker://user:password@remote.mysql.com:3306/production\"\n      dst_url: \"mydocker://user:password@localhost:3306/development\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#development-team-configuration", "title": "Development Team Configuration", "text": "<pre><code>targets:\n  mysql-team-dev:\n    snapshot:\n      src_url: \"mydocker://developer:${SHARED_DEV_PASSWORD}@shared-mysql:3306/team_app\"\n      dst_url: \"mydocker://developer:${SHARED_DEV_PASSWORD}@localhost:3306/local_app\"\n\n    sanitize:\n      dst_url: \"mydocker://developer:${SHARED_DEV_PASSWORD}@localhost:3306/local_app_sanitized\"\n      override_query: |\n        -- Standard development data sanitization\n        UPDATE users SET \n          email = CONCAT('dev', id, '@devcompany.com'),\n          password = '$2y$10$test.hash.for.development.use',\n          api_token = NULL;\n</code></pre>"}, {"location": "database-engines/engine-configuration/#custom-mysql-image", "title": "Custom MySQL Image", "text": "<pre><code># Global Docker configuration  \ndocker:\n  images:\n    mysql: \"mysql:8.0\"  # Use standard MySQL 8.0\n\ntargets:\n  mysql-custom-image:\n    snapshot:\n      src_url: \"mydocker://user:pass@source:3306/app\"\n      dst_url: \"mydocker://user:pass@dest:3306/app_copy\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#mixed-engine-configurations", "title": "Mixed Engine Configurations", "text": ""}, {"location": "database-engines/engine-configuration/#cross-database-migration", "title": "Cross-Database Migration", "text": "<pre><code>targets:\n  mysql-to-postgres-migration:\n    snapshot:\n      # Source: MySQL production with local tools for performance\n      src_url: \"mysql://migration:${MYSQL_PASSWORD}@legacy-mysql:3306/legacy_app\"\n      # Destination: PostgreSQL with Docker for consistent tooling\n      dst_url: \"pgdocker://migration:${POSTGRES_PASSWORD}@new-postgres:5432/modern_app\"\n\n    sanitize:\n      # Use PostgreSQL engine for destination sanitization\n      dst_url: \"pgdocker://dev:${DEV_PASSWORD}@new-postgres:5432/modern_app_dev\"\n      override_query: |\n        -- PostgreSQL-specific sanitization after migration\n        UPDATE users SET \n          email = 'migrated_user' || id || '@newcompany.com',\n          legacy_id = NULL,\n          migration_date = NOW();\n</code></pre>"}, {"location": "database-engines/engine-configuration/#environment-specific-engine-selection", "title": "Environment-Specific Engine Selection", "text": "<pre><code># Local development: Use whatever tools are available\nlocal-development:\n  snapshot:\n    src_url: \"postgres://dev:dev@localhost:5432/app\"          # Local PostgreSQL\n    dst_url: \"mydocker://dev:dev@localhost:3306/app_mysql\"    # MySQL via Docker\n\n# Staging: Use Docker for consistency  \nstaging-refresh:\n  snapshot:\n    src_url: \"pgdocker://staging:${STAGING_PASSWORD}@staging-db:5432/app\"\n    dst_url: \"pgdocker://staging:${STAGING_PASSWORD}@staging-db:5432/app_refreshed\"\n\n# Production: Use local tools for maximum performance\nproduction-backup:\n  cpus: 12\n  snapshot:\n    src_url: \"postgres://backup:${BACKUP_PASSWORD}@prod-primary:5432/app\"\n    dst_url: \"postgres://backup:${BACKUP_PASSWORD}@backup-server:5432/app_backup\"\n\n    schema_config:\n      exclude_schemas: [\"temp_data\", \"session_store\"]\n</code></pre>"}, {"location": "database-engines/engine-configuration/#advanced-configuration-patterns", "title": "Advanced Configuration Patterns", "text": ""}, {"location": "database-engines/engine-configuration/#multi-target-operations", "title": "Multi-Target Operations", "text": "<pre><code># Backup multiple environments\nbackup-all-environments:\n  snapshot:\n    src_url: \"postgres://backup:${PROD_PASSWORD}@prod:5432/app\"\n    dst_url: \"postgres://backup:${BACKUP_PASSWORD}@backup:5432/prod_backup_${DATE}\"\n\nbackup-staging:\n  snapshot:\n    src_url: \"postgres://backup:${STAGING_PASSWORD}@staging:5432/app\"  \n    dst_url: \"postgres://backup:${BACKUP_PASSWORD}@backup:5432/staging_backup_${DATE}\"\n\nbackup-dev:\n  snapshot:\n    src_url: \"postgres://backup:${DEV_PASSWORD}@dev:5432/app\"\n    dst_url: \"postgres://backup:${BACKUP_PASSWORD}@backup:5432/dev_backup_${DATE}\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#conditional-environment-configuration", "title": "Conditional Environment Configuration", "text": "<pre><code># Production-like performance for staging\nstaging-performance-test:\n  cpus: 8\n\n  snapshot:\n    src_url: \"postgres://perf_test:${PERF_PASSWORD}@prod-replica:5432/app\"\n    dst_url: \"postgres://staging:${STAGING_PASSWORD}@staging-performance:5432/app\"\n\n    schema_config:\n      # Include all schemas for realistic performance testing\n      exclude_schemas: [\"audit_logs\"]  # Only exclude truly unnecessary data\n\n  sanitize:\n    dst_url: \"postgres://staging:${STAGING_PASSWORD}@staging-performance:5432/app_sanitized\"\n    override_query: |\n      -- Minimal sanitization to preserve performance characteristics\n      UPDATE users SET \n        email = 'perf_test' || id || '@staging.com'\n      WHERE email LIKE '%@production-domain.com';\n</code></pre>"}, {"location": "database-engines/engine-configuration/#resource-optimized-configuration", "title": "Resource-Optimized Configuration", "text": "<pre><code># Small database - minimal resources\nsmall-app-refresh:\n  cpus: 2\n\n  snapshot:\n    src_url: \"pgdocker://user:pass@small-db:5432/small_app\"\n    dst_url: \"pgdocker://user:pass@localhost:5432/small_app_dev\"\n\n# Medium database - balanced resources  \nmedium-app-refresh:\n  cpus: 4\n\n  snapshot:\n    src_url: \"postgres://user:pass@medium-db:5432/medium_app\"\n    dst_url: \"postgres://user:pass@dev:5432/medium_app_dev\"\n\n# Large database - maximum resources\nlarge-app-refresh:\n  cpus: 16\n\n  snapshot:\n    src_url: \"postgres://user:pass@warehouse:5432/analytics\"\n    dst_url: \"postgres://user:pass@dev-warehouse:5432/analytics_sample\"\n\n    schema_config:\n      include_schemas: [\"public\", \"core_analytics\"]  # Reduce data volume\n</code></pre>"}, {"location": "database-engines/engine-configuration/#global-configuration", "title": "Global Configuration", "text": ""}, {"location": "database-engines/engine-configuration/#default-settings", "title": "Default Settings", "text": "<pre><code># Global defaults applied to all targets\ndefaults:\n  cpus: 4\n  working_directory: \"./snapshots\"\n\n# Docker image configuration\ndocker:\n  images:\n    postgres: \"postgres:16-alpine\"\n    mysql: \"mysql:8-oracle\"\n\n# Database tool paths (if using local engines)\ndatabase_tools:\n  postgresql:\n    pg_dump: \"/usr/local/bin/pg_dump\"\n    pg_restore: \"/usr/local/bin/pg_restore\"\n    psql: \"/usr/local/bin/psql\"\n  mysql:\n    mysql: \"/usr/local/bin/mysql\"\n    mysqldump: \"/usr/local/bin/mysqldump\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#environment-variables", "title": "Environment Variables", "text": "<pre><code>targets:\n  secure-config:\n    snapshot:\n      # Use environment variables for sensitive data\n      src_url: \"postgres://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}\"\n      dst_url: \"postgres://${DEV_USER}:${DEV_PASSWORD}@${DEV_HOST}:${DEV_PORT}/${DEV_NAME}\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#validation-and-testing", "title": "Validation and Testing", "text": ""}, {"location": "database-engines/engine-configuration/#configuration-validation", "title": "Configuration Validation", "text": "<pre><code># Test configuration without executing\ndbsnapper build target-name --dry-run\n\n# Verbose output for troubleshooting\ndbsnapper build target-name --dry-run --verbose\n\n# Validate multiple targets\ndbsnapper config check\n</code></pre>"}, {"location": "database-engines/engine-configuration/#connection-testing", "title": "Connection Testing", "text": "<pre><code>targets:\n  connection-test:\n    # Minimal configuration for testing connectivity\n    snapshot:\n      src_url: \"postgres://user:pass@host:5432/db?connect_timeout=10\"\n      dst_url: \"postgres://user:pass@localhost:5432/test_db\"\n</code></pre>"}, {"location": "database-engines/engine-configuration/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>PostgreSQL Local Engine - PostgreSQL local configuration details</li> <li>PostgreSQL Docker Engine - PostgreSQL Docker setup  </li> <li>MySQL Local Engine - MySQL local configuration details</li> <li>MySQL Docker Engine - MySQL Docker setup</li> <li>Engine Selection Guide - Choosing the right engines</li> <li>Database Engines Introduction - Architecture overview</li> </ul>"}, {"location": "database-engines/engine-selection/", "title": "Engine Selection Guide", "text": ""}, {"location": "database-engines/engine-selection/#overview", "title": "Overview", "text": "<p>Choosing the right database engine for your DBSnapper operations depends on your environment, requirements, and constraints. This guide helps you select the optimal engine combination for your specific use case.</p>"}, {"location": "database-engines/engine-selection/#quick-selection-matrix", "title": "Quick Selection Matrix", "text": "Scenario PostgreSQL MySQL Recommendation Local tools installed <code>postgres://</code> <code>mysql://</code> Use local engines for best performance No local tools <code>pgdocker://</code> <code>mydocker://</code> Use Docker engines for convenience CI/CD pipelines <code>pgdocker://</code> <code>mydocker://</code> Docker engines for consistency Schema filtering needed <code>postgres://</code> or <code>pgdocker://</code> Not available PostgreSQL only feature Maximum performance <code>postgres://</code> <code>mysql://</code> Local engines avoid container overhead Consistent environments <code>pgdocker://</code> <code>mydocker://</code> Docker ensures version consistency"}, {"location": "database-engines/engine-selection/#postgresql-engine-selection", "title": "PostgreSQL Engine Selection", "text": ""}, {"location": "database-engines/engine-selection/#postgresql-local-postgres", "title": "PostgreSQL Local (<code>postgres://</code>)", "text": "<p>Choose when: - PostgreSQL client tools are installed locally - Maximum performance is required - Direct network access to databases - Schema filtering is needed - Working with large databases requiring parallel operations</p> <pre><code>targets:\n  postgres-local-optimal:\n    cpus: 8  # Parallel processing available\n\n    snapshot:\n      src_url: \"postgres://user:pass@prod:5432/app\"\n      dst_url: \"postgres://user:pass@dev:5432/app\"\n\n      schema_config:\n        exclude_schemas: [\"audit_logs\", \"temp_data\"]\n</code></pre> <p>Advantages: - Fastest performance (no container overhead) - Full schema filtering support - Parallel processing with <code>cpus</code> configuration - Direct tool execution</p> <p>Requirements: - <code>pg_dump</code>, <code>pg_restore</code>, <code>psql</code> in system PATH - Network connectivity to PostgreSQL servers - Compatible PostgreSQL client version</p>"}, {"location": "database-engines/engine-selection/#postgresql-docker-pgdocker", "title": "PostgreSQL Docker (<code>pgdocker://</code>)", "text": "<p>Choose when: - PostgreSQL tools not installed locally - Running in containerized environments - Need consistent tool versions across team/environments - Working in CI/CD pipelines</p> <pre><code>targets:\n  postgres-docker-optimal:\n    snapshot:\n      src_url: \"pgdocker://user:pass@prod:5432/app\"\n      dst_url: \"pgdocker://user:pass@dev:5432/app\"\n\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n</code></pre> <p>Advantages: - No local tool installation required - Consistent PostgreSQL version across environments - Full schema filtering support - Automatic container management</p> <p>Requirements: - Docker Engine 20.10+ - Internet access to pull images - Adequate system resources for containers</p>"}, {"location": "database-engines/engine-selection/#mysql-engine-selection", "title": "MySQL Engine Selection", "text": ""}, {"location": "database-engines/engine-selection/#mysql-local-mysql", "title": "MySQL Local (<code>mysql://</code>)", "text": "<p>Choose when: - MySQL client tools are installed locally - Maximum performance is critical - Working with local MySQL instances - Direct database connectivity available</p> <pre><code>targets:\n  mysql-local-optimal:\n    cpus: 4  # Affects tool execution\n\n    snapshot:\n      src_url: \"mysql://user:pass@prod:3306/app\"\n      dst_url: \"mysql://user:pass@dev:3306/app\"\n</code></pre> <p>Advantages: - Best performance (no containerization overhead) - Direct tool execution - Familiar local tool behavior</p> <p>Requirements: - <code>mysqldump</code>, <code>mysql</code> commands in system PATH - Network connectivity to MySQL servers - Compatible MySQL client version</p>"}, {"location": "database-engines/engine-selection/#mysql-docker-mydocker", "title": "MySQL Docker (<code>mydocker://</code>)", "text": "<p>Choose when: - MySQL tools not available locally - Need consistent MySQL tool versions - Working in containerized environments - Simplified setup requirements</p> <pre><code>targets:\n  mysql-docker-optimal:\n    snapshot:\n      src_url: \"mydocker://user:pass@prod:3306/app\"\n      dst_url: \"mydocker://user:pass@dev:3306/app\"\n</code></pre> <p>Advantages: - No local MySQL installation needed - Includes MySQL Shell (<code>mysqlsh</code>) for advanced operations - Consistent tool versions - Automatic container lifecycle management</p> <p>Requirements: - Docker Engine 20.10+ - System resources for MySQL containers - Network access for image pulls</p>"}, {"location": "database-engines/engine-selection/#mixed-engine-scenarios", "title": "Mixed Engine Scenarios", "text": ""}, {"location": "database-engines/engine-selection/#cross-database-migrations", "title": "Cross-Database Migrations", "text": "<p>Combine different engines for database migrations:</p> <pre><code>targets:\n  mysql-to-postgres:\n    snapshot:\n      # Source: MySQL with local tools\n      src_url: \"mysql://user:pass@legacy:3306/old_app\"\n      # Destination: PostgreSQL with Docker tools  \n      dst_url: \"pgdocker://user:pass@new:5432/modern_app\"\n\n    sanitize:\n      override_query: |\n        -- Transform data during migration\n        ALTER TABLE users ALTER COLUMN id TYPE BIGINT;\n</code></pre>"}, {"location": "database-engines/engine-selection/#environment-specific-choices", "title": "Environment-Specific Choices", "text": "<p>Different engines for different environments:</p> <pre><code># Production backup (local tools for performance)\nproduction-backup:\n  snapshot:\n    src_url: \"postgres://readonly:pass@prod:5432/app\"\n    dst_url: \"postgres://backup:pass@backup:5432/app_backup\"\n\n# Development refresh (Docker for consistency)  \ndev-refresh:\n  snapshot:\n    src_url: \"pgdocker://user:pass@staging:5432/app\"\n    dst_url: \"pgdocker://dev:pass@localhost:5432/app_dev\"\n</code></pre>"}, {"location": "database-engines/engine-selection/#decision-framework", "title": "Decision Framework", "text": ""}, {"location": "database-engines/engine-selection/#performance-requirements", "title": "Performance Requirements", "text": "<p>High Performance Needs: <pre><code># Use local engines for maximum speed\ntargets:\n  high-performance:\n    snapshot:\n      src_url: \"postgres://user:pass@source:5432/large_db\"\n      dst_url: \"mysql://user:pass@dest:3306/large_db\"\n</code></pre></p> <p>Standard Performance Needs: <pre><code># Docker engines acceptable for regular operations\ntargets:\n  standard-performance:\n    snapshot:\n      src_url: \"pgdocker://user:pass@source:5432/app\"  \n      dst_url: \"mydocker://user:pass@dest:3306/app\"\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#environment-constraints", "title": "Environment Constraints", "text": "<p>Local Development: <pre><code># Mix of local and Docker based on tool availability\ntargets:\n  local-dev:\n    snapshot:\n      src_url: \"postgres://user:pass@localhost:5432/app\"      # Local PostgreSQL\n      dst_url: \"mydocker://user:pass@localhost:3306/app\"      # MySQL via Docker\n</code></pre></p> <p>CI/CD Pipelines: <pre><code># Docker engines for consistent, reproducible builds\ntargets:\n  ci-pipeline:\n    snapshot:\n      src_url: \"pgdocker://user:pass@test-db:5432/app\"\n      dst_url: \"mydocker://user:pass@staging-db:3306/app\"\n</code></pre></p> <p>Production Operations: <pre><code># Local engines for performance, with proper monitoring\ntargets:\n  production-ops:\n    cpus: 8\n    snapshot:\n      src_url: \"postgres://readonly:pass@prod:5432/app\"\n      dst_url: \"postgres://backup:pass@backup:5432/app\"\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#feature-requirements", "title": "Feature Requirements", "text": "<p>Schema Filtering Required: <pre><code># Must use PostgreSQL engines (local or Docker)\ntargets:\n  schema-filtering:\n    snapshot:\n      src_url: \"postgres://user:pass@source:5432/app\"  # or pgdocker://\n      dst_url: \"postgres://user:pass@dest:5432/app\"    # or pgdocker://\n\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n</code></pre></p> <p>No Schema Filtering: <pre><code># Any engine type works\ntargets:\n  no-filtering:\n    snapshot:\n      src_url: \"mysql://user:pass@source:3306/app\"      # or mydocker://\n      dst_url: \"postgres://user:pass@dest:5432/app\"     # or pgdocker://\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#performance-comparison", "title": "Performance Comparison", "text": ""}, {"location": "database-engines/engine-selection/#local-vs-docker-performance", "title": "Local vs Docker Performance", "text": "<p>Local Engines: - 10-20% faster for small databases (&lt; 1GB) - 20-30% faster for medium databases (1-10GB) - 30-40% faster for large databases (&gt; 10GB) - No container startup overhead - Direct tool execution</p> <p>Docker Engines: - Consistent performance across environments - Container startup adds 5-15 seconds - Network overhead for database connections - Memory overhead for container runtime</p>"}, {"location": "database-engines/engine-selection/#resource-usage", "title": "Resource Usage", "text": "<p>Local Engine Resources: - Minimal memory overhead - CPU usage only during operations - Direct disk I/O access</p> <p>Docker Engine Resources: - Container memory overhead (100-500MB) - Docker daemon CPU usage - Volume mounting overhead</p>"}, {"location": "database-engines/engine-selection/#troubleshooting-engine-issues", "title": "Troubleshooting Engine Issues", "text": ""}, {"location": "database-engines/engine-selection/#local-engine-problems", "title": "Local Engine Problems", "text": "<p>Tools Not Found: <pre><code># PostgreSQL\nwhich pg_dump pg_restore psql\n\n# MySQL  \nwhich mysqldump mysql\n</code></pre></p> <p>Version Compatibility: <pre><code># Check tool versions\npg_dump --version\nmysql --version\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#docker-engine-problems", "title": "Docker Engine Problems", "text": "<p>Docker Not Available: <pre><code>docker --version\ndocker ps\n</code></pre></p> <p>Image Pull Issues: <pre><code>docker pull postgres:16-alpine\ndocker pull mysql:8-oracle\n</code></pre></p> <p>Resource Constraints: <pre><code>docker stats\ndocker system df\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#best-practices", "title": "Best Practices", "text": ""}, {"location": "database-engines/engine-selection/#development-workflow", "title": "Development Workflow", "text": "<pre><code># Development: Use Docker for consistency\ndev-target:\n  snapshot:\n    src_url: \"pgdocker://user:pass@dev-db:5432/app\"\n    dst_url: \"pgdocker://user:pass@localhost:5432/app_local\"\n\n# Testing: Use local tools for speed  \ntest-target:\n  snapshot:\n    src_url: \"postgres://user:pass@test-db:5432/app\"\n    dst_url: \"postgres://user:pass@localhost:5432/app_test\"\n\n# Production: Use local tools with monitoring\nprod-target:\n  cpus: 8\n  snapshot:\n    src_url: \"postgres://readonly:pass@prod:5432/app\"\n    dst_url: \"postgres://backup:pass@backup:5432/app\"\n</code></pre>"}, {"location": "database-engines/engine-selection/#team-collaboration", "title": "Team Collaboration", "text": "<p>Standardize on Docker engines for team consistency: <pre><code># Team standard configuration\ndefaults:\n  # Docker engines ensure everyone uses same tool versions\n  postgres_engine: \"pgdocker://\"\n  mysql_engine: \"mydocker://\"\n\ntargets:\n  team-standard:\n    snapshot:\n      src_url: \"pgdocker://user:pass@shared-db:5432/app\"\n      dst_url: \"mydocker://user:pass@dev-db:3306/app\"\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#production-operations", "title": "Production Operations", "text": "<p>Use local engines with proper resource allocation: <pre><code>production-operations:\n  cpus: 16  # Maximize performance\n\n  snapshot:\n    src_url: \"postgres://readonly:pass@prod:5432/app\"\n    dst_url: \"postgres://backup:pass@backup:5432/app\"\n\n    schema_config:\n      exclude_schemas: [\"temp_data\", \"audit_logs\"]\n</code></pre></p>"}, {"location": "database-engines/engine-selection/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>PostgreSQL Local Engine - Local PostgreSQL configuration</li> <li>PostgreSQL Docker Engine - Docker PostgreSQL setup</li> <li>MySQL Local Engine - Local MySQL configuration</li> <li>MySQL Docker Engine - Docker MySQL setup</li> <li>Docker Integration - Docker system overview</li> <li>Database Engines Introduction - Engine architecture</li> </ul>"}, {"location": "database-engines/ephemeral-databases/", "title": "Ephemeral Databases", "text": ""}, {"location": "database-engines/ephemeral-databases/#overview", "title": "Overview", "text": "<p>Ephemeral databases are temporary database containers that DBSnapper creates automatically for specific operations like sanitization and data transformation. These short-lived containers provide isolated environments for processing sensitive data without affecting your production or development databases.</p>"}, {"location": "database-engines/ephemeral-databases/#how-ephemeral-databases-work", "title": "How Ephemeral Databases Work", "text": ""}, {"location": "database-engines/ephemeral-databases/#automatic-creation", "title": "Automatic Creation", "text": "<p>DBSnapper automatically creates ephemeral databases when needed:</p> <pre><code>targets:\n  sanitize-example:\n    snapshot:\n      src_url: \"postgres://user:pass@production:5432/app\"\n      dst_url: \"postgres://user:pass@localhost:5432/app_dev\"\n\n    sanitize:\n      override_query: |\n        UPDATE users SET email = 'user' || id || '@example.com';\n</code></pre> <p>Behind the scenes: 1. DBSnapper creates a temporary database container 2. Loads your snapshot data into the ephemeral database 3. Runs sanitization queries in the isolated environment 4. Exports sanitized data to your destination 5. Automatically destroys the temporary container</p>"}, {"location": "database-engines/ephemeral-databases/#container-lifecycle", "title": "Container Lifecycle", "text": "<p>Creation Phase: - Unique container name generated automatically - Isolated Docker network created - Database credentials configured securely</p> <p>Operation Phase: - Data loaded from source snapshot - Sanitization or transformation queries executed - Processed data prepared for export</p> <p>Cleanup Phase: - Container stopped and removed - Network resources cleaned up - No persistent data remains</p>"}, {"location": "database-engines/ephemeral-databases/#use-cases", "title": "Use Cases", "text": ""}, {"location": "database-engines/ephemeral-databases/#data-sanitization", "title": "Data Sanitization", "text": "<p>The most common use case for ephemeral databases:</p> <pre><code>targets:\n  customer-data-sanitization:\n    snapshot:\n      src_url: \"postgres://readonly:pass@prod:5432/customers\"\n      dst_url: \"postgres://dev:pass@dev:5432/customers_sanitized\"\n\n    sanitize:\n      override_query: |\n        -- Remove sensitive customer data\n        UPDATE customers SET \n          email = 'customer' || id || '@example.com',\n          phone = '555-0000',\n          ssn = NULL,\n          credit_card = NULL;\n\n        -- Anonymize addresses\n        UPDATE addresses SET \n          street = 'Test Street',\n          city = 'Test City',\n          postal_code = '00000';\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#data-transformation", "title": "Data Transformation", "text": "<p>Transform data structure during the copy process:</p> <pre><code>targets:\n  legacy-migration:\n    snapshot:\n      src_url: \"mysql://user:pass@legacy:3306/old_app\"\n      dst_url: \"postgres://user:pass@new:5432/modernized_app\"\n\n    sanitize:\n      override_query: |\n        -- Transform legacy data formats\n        UPDATE user_profiles SET \n          full_name = CONCAT(first_name, ' ', last_name),\n          created_date = DATE(created_timestamp);\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#testing-data-preparation", "title": "Testing Data Preparation", "text": "<p>Create specialized datasets for testing:</p> <pre><code>targets:\n  test-data-prep:\n    snapshot:\n      src_url: \"postgres://user:pass@staging:5432/app\"\n      dst_url: \"postgres://test:pass@test:5432/app_testing\"\n\n    sanitize:\n      override_query: |\n        -- Create predictable test data\n        UPDATE users SET \n          email = 'test' || id || '@testcompany.com',\n          status = 'active'\n        WHERE id &lt;= 100;\n\n        -- Remove non-test data\n        DELETE FROM users WHERE id &gt; 100;\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#database-engine-support", "title": "Database Engine Support", "text": ""}, {"location": "database-engines/ephemeral-databases/#postgresql-ephemeral-databases", "title": "PostgreSQL Ephemeral Databases", "text": "<p>Uses PostgreSQL Docker containers with automatic configuration:</p> <pre><code>targets:\n  postgres-ephemeral:\n    snapshot:\n      src_url: \"pgdocker://user:pass@source:5432/app\"\n      dst_url: \"postgres://user:pass@dest:5432/app_clean\"\n\n    # Ephemeral PostgreSQL container created automatically\n    sanitize:\n      override_query: |\n        UPDATE sensitive_table SET data = 'sanitized';\n</code></pre> <p>Features: - Full PostgreSQL feature support - Schema filtering compatibility - Parallel processing capabilities</p>"}, {"location": "database-engines/ephemeral-databases/#mysql-ephemeral-databases", "title": "MySQL Ephemeral Databases", "text": "<p>Uses MySQL Docker containers with root access for operations:</p> <pre><code>targets:\n  mysql-ephemeral:\n    snapshot:\n      src_url: \"mydocker://user:pass@source:3306/app\"\n      dst_url: \"mysql://user:pass@dest:3306/app_clean\"\n\n    # Ephemeral MySQL container created automatically  \n    sanitize:\n      override_query: |\n        UPDATE users SET email = CONCAT('user', id, '@example.com');\n</code></pre> <p>Features: - MySQL 8.0 feature support - All storage engines supported - Automatic constraint handling</p>"}, {"location": "database-engines/ephemeral-databases/#configuration-options", "title": "Configuration Options", "text": ""}, {"location": "database-engines/ephemeral-databases/#network-isolation", "title": "Network Isolation", "text": "<p>Ephemeral databases run in isolated Docker networks by default:</p> <pre><code>targets:\n  isolated-processing:\n    # Network automatically managed for security\n    snapshot:\n      src_url: \"postgres://user:pass@production:5432/sensitive_db\"\n      dst_url: \"postgres://user:pass@dev:5432/sanitized_db\"\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#resource-allocation", "title": "Resource Allocation", "text": "<p>Control resources for ephemeral database containers:</p> <pre><code>targets:\n  resource-controlled:\n    cpus: 4  # Affects ephemeral container resources\n\n    snapshot:\n      src_url: \"postgres://user:pass@large-db:5432/warehouse\"\n      dst_url: \"postgres://user:pass@dest:5432/warehouse_processed\"\n\n    sanitize:\n      override_query: |\n        -- Resource-intensive sanitization\n        UPDATE large_table SET sensitive_data = 'redacted';\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#security-benefits", "title": "Security Benefits", "text": ""}, {"location": "database-engines/ephemeral-databases/#data-isolation", "title": "Data Isolation", "text": "<p>Complete Isolation: - Ephemeral databases run in separate containers - No network access to production systems - Temporary credentials with limited scope</p> <p>No Persistent State: - Containers destroyed after operations - No data left on disk after processing - Automatic cleanup of all resources</p>"}, {"location": "database-engines/ephemeral-databases/#credential-management", "title": "Credential Management", "text": "<pre><code>targets:\n  secure-ephemeral:\n    snapshot:\n      # Production uses read-only credentials\n      src_url: \"postgres://readonly:limited@prod:5432/app\"\n      # Destination uses dev credentials  \n      dst_url: \"postgres://dev:dev_pass@dev:5432/app\"\n\n    # Ephemeral database uses auto-generated credentials\n    sanitize:\n      override_query: |\n        UPDATE users SET email = 'safe' || id || '@example.com';\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#performance-considerations", "title": "Performance Considerations", "text": ""}, {"location": "database-engines/ephemeral-databases/#optimization-strategies", "title": "Optimization Strategies", "text": "<p>For Large Datasets: <pre><code>targets:\n  large-ephemeral:\n    cpus: 8  # More resources for ephemeral processing\n\n    snapshot:\n      src_url: \"postgres://user:pass@warehouse:5432/analytics\"\n      dst_url: \"postgres://user:pass@dev:5432/analytics_sample\"\n\n    sanitize:\n      override_query: |\n        -- Process in batches for better performance\n        UPDATE large_table SET \n          sensitive_col = 'redacted'\n        WHERE id BETWEEN 1 AND 10000;\n</code></pre></p> <p>Memory Management: - Ephemeral containers automatically sized based on data volume - Streaming processing for large datasets - Automatic cleanup prevents resource leaks</p>"}, {"location": "database-engines/ephemeral-databases/#processing-time", "title": "Processing Time", "text": "<p>Factors Affecting Performance: - Source data size - Complexity of sanitization queries - Available CPU and memory resources - Network bandwidth for data transfer</p> <p>Typical Processing Times: - Small databases (&lt; 1GB): 1-5 minutes - Medium databases (1-10GB): 5-30 minutes - Large databases (&gt; 10GB): 30+ minutes</p>"}, {"location": "database-engines/ephemeral-databases/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "database-engines/ephemeral-databases/#common-issues", "title": "Common Issues", "text": "<p>Container Creation Failures: <pre><code># Verify Docker is working\ndocker version\ndocker ps\n\n# Check available resources\ndocker system df\ndocker system info\n</code></pre></p> <p>Memory Issues: <pre><code>targets:\n  memory-optimized:\n    # Reduce resource usage for large datasets\n    cpus: 2\n\n    sanitize:\n      override_query: |\n        -- Process in smaller batches\n        UPDATE users SET email = 'user' || id || '@example.com' \n        WHERE id &lt;= 1000;\n</code></pre></p> <p>Network Connectivity: <pre><code># Check Docker networks\ndocker network ls\n\n# Verify container networking\ndocker run --rm --network bridge postgres:16-alpine ping -c 1 google.com\n</code></pre></p>"}, {"location": "database-engines/ephemeral-databases/#debug-commands", "title": "Debug Commands", "text": "<pre><code># Monitor ephemeral container creation\ndbsnapper sanitize target-name --verbose\n\n# Check Docker container logs (while running)\ndocker logs $(docker ps -l -q)\n\n# Monitor resource usage\ndocker stats $(docker ps -l -q)\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#performance-issues", "title": "Performance Issues", "text": "<p>Slow Processing: - Increase <code>cpus</code> setting for more resources - Simplify sanitization queries - Process data in smaller batches - Check available system memory</p> <p>Container Startup Delays: - Pre-pull required Docker images - Ensure adequate disk space - Optimize Docker daemon settings</p>"}, {"location": "database-engines/ephemeral-databases/#best-practices", "title": "Best Practices", "text": ""}, {"location": "database-engines/ephemeral-databases/#query-optimization", "title": "Query Optimization", "text": "<pre><code>targets:\n  optimized-sanitization:\n    sanitize:\n      override_query: |\n        -- Use efficient updates with indexes\n        UPDATE users SET \n          email = 'user' || id || '@example.com'\n        WHERE email IS NOT NULL;\n\n        -- Batch process large tables\n        UPDATE large_table SET \n          sensitive_data = NULL \n        WHERE created_date &lt; '2023-01-01'\n        LIMIT 10000;\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#resource-management", "title": "Resource Management", "text": "<pre><code>targets:\n  resource-aware:\n    # Match resources to data size\n    cpus: 4\n\n    sanitize:\n      override_query: |\n        -- Monitor progress with row counts\n        UPDATE users SET email = 'test' || id || '@example.com';\n        -- Returns: UPDATE 15000 (example)\n</code></pre>"}, {"location": "database-engines/ephemeral-databases/#security-practices", "title": "Security Practices", "text": "<ul> <li>Use read-only source credentials when possible</li> <li>Limit sanitization queries to necessary operations only</li> <li>Verify sanitization results before deploying to development</li> <li>Monitor ephemeral container resource usage to detect issues</li> </ul>"}, {"location": "database-engines/ephemeral-databases/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>Database Engines Introduction - Engine architecture overview</li> <li>PostgreSQL Docker Engine - PostgreSQL containerization</li> <li>MySQL Docker Engine - MySQL containerization</li> </ul>"}, {"location": "database-engines/introduction/", "title": "Database Engines Architecture", "text": "<p>DBSnapper uses a sophisticated engine architecture that provides unified database operations across different vendors and deployment models. Each database engine implements standardized interfaces while handling vendor-specific operations and optimizations.</p>"}, {"location": "database-engines/introduction/#engine-architecture-overview", "title": "Engine Architecture Overview", "text": ""}, {"location": "database-engines/introduction/#interface-based-design", "title": "Interface-Based Design", "text": "<p>All database engines implement two core interfaces:</p> <ul> <li>Database Interface: Connection management, query execution, schema operations</li> <li>DatabaseTools Interface: Dump, restore, and utility operations using command-line tools</li> </ul> <p>This design ensures consistent behavior across all supported database types while allowing for vendor-specific optimizations.</p>"}, {"location": "database-engines/introduction/#engine-factory-pattern", "title": "Engine Factory Pattern", "text": "<p>DBSnapper uses URL schemes to automatically select the appropriate engine:</p> <pre><code># URL Scheme \u2192 Engine Selection\npostgres://     \u2192 PostgreSQL Local\npgdocker://     \u2192 PostgreSQL Docker  \nmysql://        \u2192 MySQL Local\nmydocker://     \u2192 MySQL Docker\n</code></pre> <p>The engine factory examines the connection string scheme and instantiates the correct engine implementation automatically.</p>"}, {"location": "database-engines/introduction/#supported-database-engines", "title": "Supported Database Engines", "text": ""}, {"location": "database-engines/introduction/#postgresql-engines", "title": "PostgreSQL Engines", "text": "<ul> <li>PostgreSQL Local (<code>postgres://</code>, <code>postgresql://</code>, <code>pg://</code>, <code>pglocal://</code>, <code>pgl://</code>)</li> <li>Uses locally installed <code>pg_dump</code>, <code>pg_restore</code>, and <code>psql</code> tools</li> <li>Supports schema filtering and parallel operations</li> <li> <p>Best performance for local development</p> </li> <li> <p>PostgreSQL Docker (<code>pgdocker://</code>, <code>pgd://</code>)</p> </li> <li>Uses <code>postgres:16-alpine</code> Docker image (configurable)</li> <li>Automatic container management and networking</li> <li>Ideal when PostgreSQL tools aren't locally installed</li> </ul>"}, {"location": "database-engines/introduction/#mysql-engines", "title": "MySQL Engines", "text": "<ul> <li>MySQL Local (<code>mysql://</code>, <code>mylocal://</code>, <code>myl://</code>)</li> <li>Uses locally installed <code>mysqldump</code> and <code>mysql</code> tools</li> <li>Direct connection to MySQL servers</li> <li> <p>Optimal performance for local MySQL instances</p> </li> <li> <p>MySQL Docker (<code>mydocker://</code>, <code>myd://</code>)</p> </li> <li>Uses <code>mysql:8-oracle</code> Docker image with <code>mysqlsh</code></li> <li>Container-based tool execution</li> <li>Perfect for environments without MySQL client tools</li> </ul>"}, {"location": "database-engines/introduction/#key-engine-features", "title": "Key Engine Features", "text": ""}, {"location": "database-engines/introduction/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>PostgreSQL engines support advanced schema filtering:</p> <pre><code>targets:\n  my-postgres:\n    snapshot:\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n        exclude_schemas: [\"temp_schema\", \"backup_schema\"]\n</code></pre>"}, {"location": "database-engines/introduction/#cpu-configuration", "title": "CPU Configuration", "text": "<p>Configure parallel operations for improved performance:</p> <pre><code>targets:\n  my-target:\n    cpus: 4  # Parallel dump/restore operations\n</code></pre>"}, {"location": "database-engines/introduction/#ephemeral-databases", "title": "Ephemeral Databases", "text": "<p>All Docker engines can create temporary database containers for operations like sanitization and testing:</p> <ul> <li>Automatic container lifecycle management</li> <li>Isolated network environments</li> <li>Automatic cleanup after operations</li> </ul>"}, {"location": "database-engines/introduction/#engine-selection-guidelines", "title": "Engine Selection Guidelines", "text": ""}, {"location": "database-engines/introduction/#use-local-engines-when", "title": "Use Local Engines When:", "text": "<ul> <li>Database tools are installed locally (<code>pg_dump</code>, <code>mysqldump</code>)</li> <li>Maximum performance is required</li> <li>Direct network connectivity to databases</li> <li>Minimal Docker overhead desired</li> </ul>"}, {"location": "database-engines/introduction/#use-docker-engines-when", "title": "Use Docker Engines When:", "text": "<ul> <li>Database tools aren't installed locally</li> <li>Running in containerized environments</li> <li>Consistent tool versions across environments</li> <li>Isolation from host system required</li> </ul>"}, {"location": "database-engines/introduction/#docker-integration", "title": "Docker Integration", "text": ""}, {"location": "database-engines/introduction/#docker-in-docker-support", "title": "Docker-in-Docker Support", "text": "<p>When DBSnapper runs inside a container, engines automatically detect this and adjust their behavior: - Volume mounting strategies adapt for container environments - Network connectivity uses container networking - Tool execution switches between local and Docker modes</p>"}, {"location": "database-engines/introduction/#network-management", "title": "Network Management", "text": "<p>Docker engines create isolated networks for database containers: - Bridge networks for container-to-container communication - Port mapping for external connectivity - Automatic network cleanup</p>"}, {"location": "database-engines/introduction/#url-scheme-reference", "title": "URL Scheme Reference", "text": "Scheme Engine Type Description <code>postgres://</code>, <code>postgresql://</code>, <code>pg://</code> PostgreSQL Local Local PostgreSQL tools <code>pglocal://</code>, <code>pgl://</code> PostgreSQL Local Explicit local designation <code>pgdocker://</code>, <code>pgd://</code> PostgreSQL Docker Containerized PostgreSQL tools <code>mysql://</code> MySQL Local Local MySQL tools <code>mylocal://</code>, <code>myl://</code> MySQL Local Explicit local designation <code>mydocker://</code>, <code>myd://</code> MySQL Docker Containerized MySQL tools"}, {"location": "database-engines/introduction/#advanced-capabilities", "title": "Advanced Capabilities", "text": ""}, {"location": "database-engines/introduction/#connection-string-parsing", "title": "Connection String Parsing", "text": "<p>Engines automatically parse connection strings and extract: - Database credentials and connection parameters - SSL settings and connection options - Database names for operations</p>"}, {"location": "database-engines/introduction/#error-handling", "title": "Error Handling", "text": "<p>Standardized error handling across all engines: - Connection validation and retry logic - Detailed error messages with troubleshooting hints - Graceful handling of network interruptions</p>"}, {"location": "database-engines/introduction/#performance-optimization", "title": "Performance Optimization", "text": "<ul> <li>Connection pooling for database operations</li> <li>Parallel processing where supported</li> <li>Efficient memory usage during large data transfers</li> </ul>"}, {"location": "database-engines/introduction/#next-steps", "title": "Next Steps", "text": "<ul> <li>PostgreSQL Local Engine - Local PostgreSQL tool configuration</li> <li>PostgreSQL Docker Engine - Containerized PostgreSQL setup</li> <li>MySQL Local Engine - Local MySQL tool configuration  </li> <li>MySQL Docker Engine - Containerized MySQL setup</li> </ul>"}, {"location": "database-engines/mysql-docker/", "title": "MySQL Docker (<code>mydocker://</code>)", "text": ""}, {"location": "database-engines/mysql-docker/#overview", "title": "Overview", "text": "<p>The MySQL Docker engine provides containerized MySQL operations using Docker images. This engine automatically manages MySQL tool containers, making it perfect for environments where MySQL client tools aren't locally installed or when you need consistent tool versions across different systems.</p>"}, {"location": "database-engines/mysql-docker/#docker-image-configuration", "title": "Docker Image Configuration", "text": ""}, {"location": "database-engines/mysql-docker/#default-image", "title": "Default Image", "text": "<p>Image: <code>mysql:8-oracle</code> - Official Oracle MySQL 8.0 image - Includes <code>mysqlsh</code> (MySQL Shell) for advanced operations - Optimized for production workloads - Full MySQL 8.0 feature set</p>"}, {"location": "database-engines/mysql-docker/#supported-images", "title": "Supported Images", "text": "<p>The engine works with official MySQL Docker images:</p> <ul> <li><code>mysql:8-oracle</code> (recommended)</li> <li><code>mysql:8.0</code></li> <li><code>mysql:latest</code></li> <li>Custom MySQL images with <code>mysqlsh</code> included</li> </ul>"}, {"location": "database-engines/mysql-docker/#custom-image-configuration", "title": "Custom Image Configuration", "text": "<pre><code># In dbsnapper configuration\ndocker:\n  images:\n    mysql: \"mysql:8-oracle\"  # Custom image\n</code></pre>"}, {"location": "database-engines/mysql-docker/#url-schemes-and-usage", "title": "URL Schemes and Usage", "text": ""}, {"location": "database-engines/mysql-docker/#supported-url-schemes", "title": "Supported URL Schemes", "text": "Scheme Description CLI Abbreviation <code>mydocker://</code> MySQL Docker engine <code>myd</code> <code>myd://</code> Short form <code>myd</code>"}, {"location": "database-engines/mysql-docker/#connection-string-format", "title": "Connection String Format", "text": "<pre><code>mydocker://username:password@host:port/database?parameters\n</code></pre> <p>Example Configurations:</p> <pre><code>targets:\n  docker-mysql:\n    snapshot:\n      src_url: \"mydocker://app:pass@prod.mysql.com:3306/production\"\n      dst_url: \"mydocker://dev:pass@localhost:3306/development\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#container-management", "title": "Container Management", "text": ""}, {"location": "database-engines/mysql-docker/#automatic-operations", "title": "Automatic Operations", "text": "<p>The MySQL Docker engine handles all container operations automatically:</p> <ol> <li>Container Creation: Temporary containers for each operation</li> <li>Tool Execution: MySQL commands run inside containers</li> <li>File Management: Snapshot files transferred via volume mounts</li> <li>Cleanup: Containers removed after operations complete</li> </ol>"}, {"location": "database-engines/mysql-docker/#network-configuration", "title": "Network Configuration", "text": "<pre><code>targets:\n  networked-mysql:\n    # Network automatically managed\n    snapshot:\n      src_url: \"mydocker://user:pass@mysql-server:3306/app\"\n      dst_url: \"mydocker://user:pass@dev-mysql:3306/app_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#target-configuration-examples", "title": "Target Configuration Examples", "text": ""}, {"location": "database-engines/mysql-docker/#basic-docker-setup", "title": "Basic Docker Setup", "text": "<pre><code>targets:\n  docker-basic:\n    snapshot:\n      src_url: \"mydocker://root:password@db.company.com:3306/production\"\n      dst_url: \"mydocker://root:password@localhost:3306/development\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#production-to-development-copy", "title": "Production to Development Copy", "text": "<pre><code>targets:\n  prod-to-dev:\n    snapshot:\n      src_url: \"mydocker://readonly:secret@prod.mysql.company.com:3306/application\"\n      dst_url: \"mydocker://dev:dev_pass@dev.mysql.company.com:3306/application_copy\"\n\n    sanitize:\n      override_query: |\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          phone = '555-0000',\n          ssn = NULL;\n</code></pre>"}, {"location": "database-engines/mysql-docker/#multi-database-environment", "title": "Multi-Database Environment", "text": "<pre><code>targets:\n  customer-docker:\n    snapshot:\n      src_url: \"mydocker://app:prod_pass@prod-mysql:3306/customers\"\n      dst_url: \"mydocker://app:dev_pass@dev-mysql:3306/customers\"\n\n  analytics-docker:\n    snapshot:\n      src_url: \"mydocker://analytics:prod_pass@prod-mysql:3306/analytics\"\n      dst_url: \"mydocker://analytics:dev_pass@dev-mysql:3306/analytics_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#high-performance-configuration", "title": "High-Performance Configuration", "text": "<pre><code>targets:\n  performance-docker:\n    cpus: 4  # Affects container resource allocation\n\n    snapshot:\n      src_url: \"mydocker://user:pass@source:3306/large_database\"\n      dst_url: \"mydocker://user:pass@dest:3306/large_database_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#ssl-and-security-configuration", "title": "SSL and Security Configuration", "text": ""}, {"location": "database-engines/mysql-docker/#ssl-connections", "title": "SSL Connections", "text": "<pre><code>targets:\n  secure-docker:\n    snapshot:\n      src_url: \"mydocker://user:pass@host:3306/db?tls=preferred\"\n      dst_url: \"mydocker://user:pass@localhost:3306/db\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#security-best-practices", "title": "Security Best Practices", "text": "<pre><code>targets:\n  secure-setup:\n    snapshot:\n      # Use read-only credentials for source\n      src_url: \"mydocker://readonly:readonly_pass@prod:3306/app?tls=true\"\n\n      # Use limited credentials for destination\n      dst_url: \"mydocker://dev:dev_pass@dev:3306/app\"\n</code></pre> <p>Security Features: - Isolated container environments - Temporary containers with no persistent state - Automatic cleanup after operations - Network isolation between operations</p>"}, {"location": "database-engines/mysql-docker/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "database-engines/mysql-docker/#container-resource-management", "title": "Container Resource Management", "text": "<pre><code>targets:\n  optimized-docker:\n    cpus: 6  # Container CPU allocation\n\n    snapshot:\n      src_url: \"mydocker://user:pass@source:3306/large_db\"\n      dst_url: \"mydocker://user:pass@dest:3306/large_db\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#large-database-handling", "title": "Large Database Handling", "text": "<pre><code>targets:\n  large-database:\n    cpus: 8\n\n    snapshot:\n      src_url: \"mydocker://user:pass@source:3306/warehouse?timeout=300s\"\n      dst_url: \"mydocker://user:pass@dest:3306/warehouse_copy\"\n\n    sanitize:\n      override_query: |\n        -- Efficient updates for large datasets\n        UPDATE users SET email = CONCAT('user', id, '@example.com') \n        WHERE email IS NOT NULL LIMIT 10000;\n</code></pre>"}, {"location": "database-engines/mysql-docker/#mysql-specific-features", "title": "MySQL-Specific Features", "text": ""}, {"location": "database-engines/mysql-docker/#storage-engine-support", "title": "Storage Engine Support", "text": "<p>Automatically handles all MySQL storage engines: - InnoDB (default) - MyISAM - Memory - Archive - Custom storage engines</p>"}, {"location": "database-engines/mysql-docker/#character-set-handling", "title": "Character Set Handling", "text": "<pre><code>targets:\n  utf8-docker:\n    snapshot:\n      src_url: \"mydocker://user:pass@source:3306/app?charset=utf8mb4\"\n      dst_url: \"mydocker://user:pass@dest:3306/app?charset=utf8mb4\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#mysql-80-features", "title": "MySQL 8.0 Features", "text": "<p>The Docker engine supports modern MySQL features: - JSON data types - Window functions - Common table expressions (CTEs) - Roles and dynamic privileges</p>"}, {"location": "database-engines/mysql-docker/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "database-engines/mysql-docker/#common-issues", "title": "Common Issues", "text": "<p>Docker Not Available: <pre><code># Verify Docker is running\ndocker version\ndocker ps\n</code></pre></p> <p>MySQL Image Issues: <pre><code># Pull the MySQL image\ndocker pull mysql:8-oracle\n\n# Test MySQL tools in container\ndocker run --rm mysql:8-oracle mysql --version\ndocker run --rm mysql:8-oracle mysqldump --version\n</code></pre></p> <p>Connection Problems: <pre><code>targets:\n  debug-connection:\n    snapshot:\n      # Test with extended timeout\n      src_url: \"mydocker://user:pass@host:3306/db?timeout=30s&amp;tls=false\"\n</code></pre></p> <p>Container Creation Failures: <pre><code># Check Docker permissions\ndocker run --rm hello-world\n\n# Verify available resources\ndocker system info\n</code></pre></p>"}, {"location": "database-engines/mysql-docker/#debug-commands", "title": "Debug Commands", "text": "<pre><code># Test Docker engine connectivity\ndbsnapper build mysql-docker-target --dry-run --verbose\n\n# Check MySQL connectivity manually\ndocker run --rm -it mysql:8-oracle mysql -h hostname -P 3306 -u username -p\n\n# Verify mysqldump works\ndocker run --rm mysql:8-oracle mysqldump --version\n\n# Test container networking\ndocker network ls\n</code></pre>"}, {"location": "database-engines/mysql-docker/#performance-issues", "title": "Performance Issues", "text": "<p>Slow Operations: - Check available system resources - Increase <code>cpus</code> setting - Consider network latency to database servers - Monitor disk I/O performance</p> <p>Container Startup Delays: - Pre-pull Docker image: <code>docker pull mysql:8-oracle</code> - Use faster storage for Docker volumes - Optimize Docker daemon settings</p>"}, {"location": "database-engines/mysql-docker/#limitations", "title": "Limitations", "text": ""}, {"location": "database-engines/mysql-docker/#schema-filtering-not-available", "title": "Schema Filtering Not Available", "text": "<p>Like MySQL Local engine, MySQL Docker does not support schema filtering:</p> <pre><code>targets:\n  mysql-docker-no-filtering:\n    # This is NOT supported for MySQL engines\n    # snapshot:\n    #   schema_config:\n    #     include_schemas: [\"schema1\"]  # Not available\n\n    snapshot:\n      src_url: \"mydocker://user:pass@host:3306/database\"\n      dst_url: \"mydocker://user:pass@host:3306/database_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#database-level-operations", "title": "Database-Level Operations", "text": "<p>MySQL Docker engine operates at the database level: - Processes entire databases, not individual schemas - Cannot selectively copy specific tables - All tables within a database are included</p>"}, {"location": "database-engines/mysql-docker/#docker-requirements", "title": "Docker Requirements", "text": ""}, {"location": "database-engines/mysql-docker/#system-requirements", "title": "System Requirements", "text": "<ul> <li>Docker Engine: 20.10+ recommended</li> <li>System Resources: 2GB+ RAM for MySQL containers</li> <li>Network: Bridge network support</li> <li>Storage: Adequate space for temporary containers and data</li> </ul>"}, {"location": "database-engines/mysql-docker/#installation-verification", "title": "Installation Verification", "text": "<pre><code># Verify Docker installation\ndocker --version\ndocker info\n\n# Test MySQL image\ndocker run --rm mysql:8-oracle mysql --version\ndocker run --rm mysql:8-oracle mysqldump --version\n\n# Test container creation\ndocker run --rm --name test-mysql -e MYSQL_ROOT_PASSWORD=test mysql:8-oracle echo \"MySQL container works\"\n</code></pre>"}, {"location": "database-engines/mysql-docker/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>MySQL Local Engine - Local tool alternative</li> <li>PostgreSQL Docker Engine - PostgreSQL containerized comparison</li> <li>Database Engines Introduction - Engine architecture overview</li> </ul>"}, {"location": "database-engines/mysql-local/", "title": "MySQL Local (<code>mysql://</code>)", "text": ""}, {"location": "database-engines/mysql-local/#overview", "title": "Overview", "text": "<p>The MySQL Local engine provides direct connections to MySQL databases using locally installed command-line tools. This engine offers optimal performance for local development and environments where MySQL client tools are available, providing fast and efficient database operations without containerization overhead.</p>"}, {"location": "database-engines/mysql-local/#architecture", "title": "Architecture", "text": "<p>The MySQL Local engine implements the Database and DatabaseTools interfaces using:</p> <ul> <li>Connection Management: Direct MySQL connections via <code>sqlx</code> and MySQL drivers</li> <li>Data Operations: Native SQL execution with MySQL-specific optimizations</li> <li>Tool Operations: Local <code>mysqldump</code> and <code>mysql</code> command execution</li> <li>Constraint Management: Automated foreign key and unique constraint handling</li> </ul>"}, {"location": "database-engines/mysql-local/#requirements", "title": "Requirements", "text": ""}, {"location": "database-engines/mysql-local/#mysql-client-tools", "title": "MySQL Client Tools", "text": "<p>The engine requires these MySQL tools in your system PATH:</p> <ul> <li><code>mysqldump</code> - Database backup creation and export</li> <li><code>mysql</code> - Database restoration and SQL execution</li> <li><code>mysqlshow</code> - Database and table inspection (optional but recommended)</li> </ul>"}, {"location": "database-engines/mysql-local/#supported-versions", "title": "Supported Versions", "text": "<ul> <li>Tested: MySQL 8.3 and MySQL 8.0 series</li> <li>Recommended: MySQL 8.0+ for optimal performance and feature support</li> <li>Compatible: MySQL 5.7+ (with reduced feature availability)</li> </ul>"}, {"location": "database-engines/mysql-local/#installation-options", "title": "Installation Options", "text": "<ul> <li>MySQL Official Installer - Complete MySQL installation with all tools</li> <li>Package Managers: </li> <li>macOS: <code>brew install mysql</code></li> <li>Ubuntu/Debian: <code>apt-get install mysql-client</code></li> <li>RHEL/CentOS: <code>yum install mysql</code></li> <li>Docker: Client tools via <code>mysql:latest</code> image</li> <li>Standalone Client: MySQL client-only packages</li> </ul>"}, {"location": "database-engines/mysql-local/#url-schemes-and-usage", "title": "URL Schemes and Usage", "text": ""}, {"location": "database-engines/mysql-local/#supported-url-schemes", "title": "Supported URL Schemes", "text": "<p>The MySQL Local engine responds to multiple URL schemes:</p> Scheme Description CLI Abbreviation <code>mysql://</code> Standard MySQL scheme <code>my</code> <code>mylocal://</code> Explicit local designation <code>myl</code> <code>myl://</code> Shortest local form <code>myl</code>"}, {"location": "database-engines/mysql-local/#connection-string-format", "title": "Connection String Format", "text": "<pre><code>mysql://username:password@host:port/database?parameters\n</code></pre> <p>Example Configurations:</p> <pre><code>targets:\n  production-mysql:\n    snapshot:\n      src_url: \"mysql://app_user:secret@db.company.com:3306/production\"\n      dst_url: \"mysql://dev_user:dev_pass@localhost:3306/development\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#ssl-and-security-configuration", "title": "SSL and Security Configuration", "text": "<pre><code>targets:\n  secure-mysql:\n    snapshot:\n      src_url: \"mysql://user:pass@host:3306/db?tls=preferred&amp;serverPubKey=server-public.pem\"\n</code></pre> <p>Common MySQL Parameters: - <code>tls=true|false|preferred</code> - SSL/TLS connection mode - <code>charset=utf8mb4</code> - Character set specification - <code>parseTime=true</code> - Enable time parsing - <code>timeout=30s</code> - Connection timeout</p>"}, {"location": "database-engines/mysql-local/#key-differences-from-postgresql", "title": "Key Differences from PostgreSQL", "text": ""}, {"location": "database-engines/mysql-local/#schema-vs-database-structure", "title": "Schema vs Database Structure", "text": "<p>MySQL uses a different schema model than PostgreSQL:</p> <pre><code>targets:\n  mysql-structure:\n    # MySQL: database contains tables directly\n    # No schema filtering support (PostgreSQL-only feature)\n    snapshot:\n      src_url: \"mysql://user:pass@host/database_name\"\n      dst_url: \"mysql://user:pass@localhost/copy_database\"\n</code></pre> <p>Important Notes: - No Schema Filtering: MySQL engines do not support <code>schema_config</code> with <code>include_schemas</code>/<code>exclude_schemas</code> - Database-Level Operations: MySQL operates at the database level, not schema level - Different Permissions Model: MySQL uses database-level privileges</p>"}, {"location": "database-engines/mysql-local/#constraint-handling", "title": "Constraint Handling", "text": "<p>MySQL Local engine automatically manages constraints during operations:</p> <pre><code>-- Automatically executed during operations\nSET UNIQUE_CHECKS=0, FOREIGN_KEY_CHECKS=0, SQL_MODE='';\n</code></pre> <p>This allows for: - Circumventing foreign key constraints during data loading - Handling circular dependencies in table relationships - Faster bulk data operations</p>"}, {"location": "database-engines/mysql-local/#target-configuration-examples", "title": "Target Configuration Examples", "text": ""}, {"location": "database-engines/mysql-local/#basic-local-development", "title": "Basic Local Development", "text": "<pre><code>targets:\n  local-mysql:\n    snapshot:\n      src_url: \"mysql://root:password@localhost:3306/myapp_production\"\n      dst_url: \"mysql://root:password@localhost:3306/myapp_development\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#production-to-development-copy", "title": "Production to Development Copy", "text": "<pre><code>targets:\n  prod-to-dev:\n    snapshot:\n      src_url: \"mysql://readonly:secret@prod.mysql.company.com:3306/application\"\n      dst_url: \"mysql://dev:dev_pass@dev.mysql.company.com:3306/application_copy\"\n\n    sanitize:\n      override_query: |\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          phone = '555-0000',\n          ssn = NULL,\n          created_ip = '0.0.0.0';\n</code></pre>"}, {"location": "database-engines/mysql-local/#multi-database-environment", "title": "Multi-Database Environment", "text": "<pre><code>targets:\n  customer-db:\n    snapshot:\n      src_url: \"mysql://app:prod_pass@prod-mysql:3306/customer_data\"\n      dst_url: \"mysql://app:dev_pass@dev-mysql:3306/customer_data\"\n\n  analytics-db:\n    snapshot:\n      src_url: \"mysql://analytics:prod_pass@prod-mysql:3306/analytics\"\n      dst_url: \"mysql://analytics:dev_pass@dev-mysql:3306/analytics_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#high-performance-configuration", "title": "High-Performance Configuration", "text": "<pre><code>targets:\n  performance-mysql:\n    # CPU configuration affects tool execution parallelism\n    cpus: 4\n\n    snapshot:\n      src_url: \"mysql://user:pass@source:3306/large_database?timeout=60s\"\n      dst_url: \"mysql://user:pass@dest:3306/large_database_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#permissions-and-security", "title": "Permissions and Security", "text": ""}, {"location": "database-engines/mysql-local/#required-database-permissions", "title": "Required Database Permissions", "text": "<p>For Source Database (Building Snapshots): <pre><code>-- Create dedicated read-only user\nCREATE USER 'dbsnapper_readonly'@'%' IDENTIFIED BY 'readonly_password';\nGRANT SELECT ON production.* TO 'dbsnapper_readonly'@'%';\nGRANT LOCK TABLES ON production.* TO 'dbsnapper_readonly'@'%';\nGRANT SHOW VIEW ON production.* TO 'dbsnapper_readonly'@'%';\nFLUSH PRIVILEGES;\n</code></pre></p> <p>For Destination Database (Loading Snapshots): <pre><code>-- Create development user with full database privileges\nCREATE USER 'dbsnapper_dev'@'%' IDENTIFIED BY 'dev_password';\nGRANT ALL PRIVILEGES ON development.* TO 'dbsnapper_dev'@'%';\nGRANT CREATE ON *.* TO 'dbsnapper_dev'@'%';  -- For database creation\nFLUSH PRIVILEGES;\n</code></pre></p>"}, {"location": "database-engines/mysql-local/#security-best-practices", "title": "Security Best Practices", "text": "<pre><code>targets:\n  secure-mysql:\n    snapshot:\n      # Use dedicated users with minimal required privileges\n      src_url: \"mysql://readonly:readonly_pass@prod:3306/app?tls=true\"\n      dst_url: \"mysql://dev:dev_pass@dev:3306/app\"\n</code></pre> <p>Security Recommendations: - Create dedicated users for DBSnapper operations - Use SSL/TLS for production connections - Implement IP-based access restrictions - Regular password rotation - Monitor database access logs</p>"}, {"location": "database-engines/mysql-local/#mysql-specific-features", "title": "MySQL-Specific Features", "text": ""}, {"location": "database-engines/mysql-local/#storage-engine-handling", "title": "Storage Engine Handling", "text": "<p>The engine automatically handles different MySQL storage engines:</p> <pre><code>targets:\n  mixed-engines:\n    # Automatically handles InnoDB, MyISAM, and other engines\n    snapshot:\n      src_url: \"mysql://user:pass@source:3306/mixed_database\"\n      dst_url: \"mysql://user:pass@dest:3306/mixed_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#character-set-management", "title": "Character Set Management", "text": "<pre><code>targets:\n  utf8-database:\n    snapshot:\n      src_url: \"mysql://user:pass@source:3306/app?charset=utf8mb4\"\n      dst_url: \"mysql://user:pass@dest:3306/app?charset=utf8mb4\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#trigger-and-view-handling", "title": "Trigger and View Handling", "text": "<p>MySQL Local engine preserves: - Database triggers - Views and their dependencies - Stored procedures and functions - Event schedules (when permissions allow)</p>"}, {"location": "database-engines/mysql-local/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "database-engines/mysql-local/#large-database-strategies", "title": "Large Database Strategies", "text": "<pre><code>targets:\n  large-mysql:\n    cpus: 8  # Parallel tool execution\n\n    snapshot:\n      src_url: \"mysql://user:pass@source:3306/large_db?timeout=300s\"\n      dst_url: \"mysql://user:pass@dest:3306/large_db\"\n\n    # Optimize for large datasets\n    sanitize:\n      override_query: |\n        -- Use efficient UPDATE statements\n        UPDATE users SET email = CONCAT('user', id, '@example.com') WHERE email IS NOT NULL;\n</code></pre>"}, {"location": "database-engines/mysql-local/#network-optimization", "title": "Network Optimization", "text": "<pre><code>targets:\n  network-optimized:\n    snapshot:\n      # Connection optimization for remote databases\n      src_url: \"mysql://user:pass@remote:3306/db?timeout=60s&amp;readTimeout=45s&amp;writeTimeout=45s\"\n      dst_url: \"mysql://user:pass@local:3306/db\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#memory-management", "title": "Memory Management", "text": "<p>MySQL Local engine optimizes memory usage: - Streaming data processing for large tables - Efficient bulk insert operations - Connection pooling for multiple operations</p>"}, {"location": "database-engines/mysql-local/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "database-engines/mysql-local/#common-issues", "title": "Common Issues", "text": "<p>MySQL Tools Not Found: <pre><code># Verify MySQL tools are installed\nwhich mysqldump\nwhich mysql\nmysql --version\n</code></pre></p> <p>Connection Failures: <pre><code>targets:\n  debug-connection:\n    snapshot:\n      # Test with extended timeout and error details\n      src_url: \"mysql://user:pass@host:3306/db?timeout=30s&amp;tls=false\"\n</code></pre></p> <p>Permission Errors: <pre><code>-- Check user privileges\nSHOW GRANTS FOR 'username'@'hostname';\n\n-- Verify database access\nUSE database_name;\nSHOW TABLES;\n</code></pre></p> <p>Character Set Issues: <pre><code>-- Check database character set\nSHOW CREATE DATABASE database_name;\n\n-- Verify table character sets\nSELECT table_name, table_collation \nFROM information_schema.tables \nWHERE table_schema = 'database_name';\n</code></pre></p>"}, {"location": "database-engines/mysql-local/#debug-commands", "title": "Debug Commands", "text": "<pre><code># Test MySQL connectivity\nmysql -h hostname -P 3306 -u username -p database_name\n\n# Verify mysqldump functionality\nmysqldump --version\nmysqldump -h hostname -P 3306 -u username -p --single-transaction database_name --no-data\n\n# Test DBSnapper connection\ndbsnapper build mysql-target --dry-run --verbose\n</code></pre>"}, {"location": "database-engines/mysql-local/#performance-issues", "title": "Performance Issues", "text": "<p>Slow Dump Operations: - Check source database server performance - Use connection pooling for better throughput - Consider network bandwidth limitations - Monitor disk I/O on both source and destination</p> <p>Large Table Handling: <pre><code>targets:\n  large-table-handling:\n    # Optimize for tables with millions of rows\n    snapshot:\n      src_url: \"mysql://user:pass@source:3306/big_db?timeout=600s\"\n      dst_url: \"mysql://user:pass@dest:3306/big_db\"\n</code></pre></p>"}, {"location": "database-engines/mysql-local/#limitations", "title": "Limitations", "text": ""}, {"location": "database-engines/mysql-local/#schema-filtering-not-available", "title": "Schema Filtering Not Available", "text": "<p>Unlike PostgreSQL engines, MySQL engines do not support schema filtering:</p> <pre><code>targets:\n  mysql-no-filtering:\n    # This is NOT supported for MySQL engines\n    # snapshot:\n    #   schema_config:\n    #     include_schemas: [\"schema1\", \"schema2\"]  # Not available for MySQL\n\n    snapshot:\n      src_url: \"mysql://user:pass@host:3306/database\"\n      dst_url: \"mysql://user:pass@host:3306/database_copy\"\n</code></pre>"}, {"location": "database-engines/mysql-local/#database-level-operations-only", "title": "Database-Level Operations Only", "text": "<p>MySQL engines operate at the database level: - Cannot filter individual schemas within a database - All tables within a database are processed together - No selective table copying (entire database or nothing)</p>"}, {"location": "database-engines/mysql-local/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>MySQL Docker Engine - Container-based alternative</li> <li>PostgreSQL Local Engine - PostgreSQL comparison</li> <li>Database Engines Introduction - Engine architecture overview</li> </ul>"}, {"location": "database-engines/postgresql-docker/", "title": "PostgreSQL Docker (<code>pgdocker://</code>)", "text": ""}, {"location": "database-engines/postgresql-docker/#overview", "title": "Overview", "text": "<p>The PostgreSQL Docker engine provides containerized PostgreSQL operations using Docker images. This engine automatically manages PostgreSQL tool containers, making it ideal for environments where PostgreSQL client tools aren't locally installed or when consistent tool versions are required across different systems.</p>"}, {"location": "database-engines/postgresql-docker/#architecture", "title": "Architecture", "text": "<p>The PostgreSQL Docker engine implements the Database and DatabaseTools interfaces using:</p> <ul> <li>Container Management: Automatic Docker container creation and lifecycle management</li> <li>Tool Execution: PostgreSQL tools (<code>pg_dump</code>, <code>pg_restore</code>, <code>psql</code>) executed within containers</li> <li>Network Isolation: Dedicated Docker networks for secure database communication</li> <li>Volume Management: Automatic file system mounting for data transfer</li> </ul>"}, {"location": "database-engines/postgresql-docker/#docker-image-configuration", "title": "Docker Image Configuration", "text": ""}, {"location": "database-engines/postgresql-docker/#default-image", "title": "Default Image", "text": "<p>Image: <code>postgres:16-alpine</code> - Lightweight Alpine Linux base - PostgreSQL 16 with all standard tools - Optimized for container environments - Minimal attack surface</p>"}, {"location": "database-engines/postgresql-docker/#supported-images", "title": "Supported Images", "text": "<p>The engine is compatible with official PostgreSQL Docker images:</p> <ul> <li><code>postgres:16-alpine</code> (recommended)</li> <li><code>postgres:15-alpine</code></li> <li><code>postgres:14-alpine</code></li> <li><code>postgres:13-alpine</code></li> <li><code>postgres:16</code>, <code>postgres:15</code>, <code>postgres:14</code> (full Debian-based images)</li> </ul>"}, {"location": "database-engines/postgresql-docker/#custom-image-configuration", "title": "Custom Image Configuration", "text": "<pre><code># In dbsnapper configuration\ndocker:\n  images:\n    postgres: \"postgres:16-alpine\"  # Custom image\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#url-schemes-and-usage", "title": "URL Schemes and Usage", "text": ""}, {"location": "database-engines/postgresql-docker/#supported-url-schemes", "title": "Supported URL Schemes", "text": "Scheme Description CLI Abbreviation <code>pgdocker://</code> PostgreSQL Docker engine <code>pgd</code> <code>pgd://</code> Short form <code>pgd</code>"}, {"location": "database-engines/postgresql-docker/#connection-string-format", "title": "Connection String Format", "text": "<pre><code>pgdocker://username:password@host:port/database?parameters\n</code></pre> <p>Example Configurations:</p> <pre><code>targets:\n  docker-postgres:\n    snapshot:\n      src_url: \"pgdocker://user:pass@prod.db.com:5432/production?sslmode=require\"\n      dst_url: \"pgdocker://user:pass@localhost:5432/development\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#container-lifecycle-management", "title": "Container Lifecycle Management", "text": ""}, {"location": "database-engines/postgresql-docker/#automatic-container-operations", "title": "Automatic Container Operations", "text": "<ol> <li>Container Creation: Temporary containers created for each operation</li> <li>Network Setup: Isolated Docker networks for secure communication</li> <li>Tool Execution: PostgreSQL commands executed within containers</li> <li>File Transfer: Snapshot files managed via volume mounts</li> <li>Cleanup: Automatic container and network removal after operations</li> </ol>"}, {"location": "database-engines/postgresql-docker/#network-configuration", "title": "Network Configuration", "text": "<p>The engine creates isolated Docker networks:</p> <pre><code>targets:\n  networked-postgres:\n    # Network automatically managed\n    snapshot:\n      src_url: \"pgdocker://user:pass@database:5432/app\"\n      dst_url: \"pgdocker://user:pass@dev-db:5432/app_copy\"\n</code></pre> <p>Network Features: - Bridge networks for container-to-container communication - DNS resolution between containers - Port mapping for external connectivity - Automatic network cleanup</p>"}, {"location": "database-engines/postgresql-docker/#docker-in-docker-support", "title": "Docker-in-Docker Support", "text": ""}, {"location": "database-engines/postgresql-docker/#detection-and-adaptation", "title": "Detection and Adaptation", "text": "<p>When DBSnapper runs inside a Docker container, the PostgreSQL Docker engine automatically detects this environment and adapts its behavior:</p> <pre><code>targets:\n  dind-aware:\n    # Engine automatically detects Docker-in-Docker\n    # and adjusts volume mounting strategies\n    snapshot:\n      src_url: \"pgdocker://user:pass@host/source_db\"\n      dst_url: \"pgdocker://user:pass@host/dest_db\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#volume-mounting-strategies", "title": "Volume Mounting Strategies", "text": "<p>Host Environment: - Direct host path mounting - Full file system access - Standard Docker volume behavior</p> <p>Docker-in-Docker Environment: - Shared volume mounting between containers - Network-based file transfer when needed - Container-to-container communication</p>"}, {"location": "database-engines/postgresql-docker/#advanced-features", "title": "Advanced Features", "text": ""}, {"location": "database-engines/postgresql-docker/#schema-filtering-support", "title": "Schema Filtering Support", "text": "<p>PostgreSQL Docker engine inherits full schema filtering capabilities:</p> <pre><code>targets:\n  filtered-docker:\n    snapshot:\n      src_url: \"pgdocker://user:pass@source/production\"\n      dst_url: \"pgdocker://user:pass@dest/filtered_copy\"\n\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n        exclude_schemas: [\"temp_schema\", \"audit_logs\"]\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#cpu-configuration", "title": "CPU Configuration", "text": "<p>Parallel operations supported within Docker containers:</p> <pre><code>targets:\n  parallel-docker:\n    cpus: 4  # Parallel pg_dump operations\n\n    snapshot:\n      src_url: \"pgdocker://user:pass@source/large_db\"\n      dst_url: \"pgdocker://user:pass@dest/large_db_copy\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#ephemeral-database-integration", "title": "Ephemeral Database Integration", "text": "<p>The PostgreSQL Docker engine seamlessly integrates with ephemeral databases:</p> <pre><code>targets:\n  with-ephemeral:\n    # Ephemeral database automatically created for sanitization\n    snapshot:\n      src_url: \"pgdocker://user:pass@production/source\"\n      dst_url: \"pgdocker://user:pass@localhost/development\"\n\n    sanitize:\n      override_query: |\n        UPDATE users SET email = 'user' || id || '@example.com';\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#target-configuration-examples", "title": "Target Configuration Examples", "text": ""}, {"location": "database-engines/postgresql-docker/#basic-docker-setup", "title": "Basic Docker Setup", "text": "<pre><code>targets:\n  docker-basic:\n    snapshot:\n      src_url: \"pgdocker://postgres:password@db.company.com:5432/production\"\n      dst_url: \"pgdocker://postgres:password@localhost:5432/development\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#high-performance-docker-configuration", "title": "High-Performance Docker Configuration", "text": "<pre><code>targets:\n  docker-performance:\n    cpus: 8\n\n    snapshot:\n      src_url: \"pgdocker://readonly:secret@prod:5432/app?sslmode=require\"\n      dst_url: \"pgdocker://dev:secret@dev:5432/app_copy\"\n\n      schema_config:\n        exclude_schemas: [\"audit_logs\", \"temp_schema\"]\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#multi-environment-docker-setup", "title": "Multi-Environment Docker Setup", "text": "<pre><code>targets:\n  staging-docker:\n    snapshot:\n      src_url: \"pgdocker://app:prod_pass@production.db:5432/app\"\n      dst_url: \"pgdocker://app:staging_pass@staging.db:5432/app\"\n\n  development-docker:\n    snapshot:\n      src_url: \"pgdocker://app:staging_pass@staging.db:5432/app\"\n      dst_url: \"pgdocker://app:dev_pass@localhost:5432/app_dev\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#container-security", "title": "Container Security", "text": ""}, {"location": "database-engines/postgresql-docker/#security-features", "title": "Security Features", "text": "<ul> <li>Isolated Networks: Each operation uses dedicated Docker networks</li> <li>Temporary Containers: Containers removed after operations</li> <li>No Persistent State: No data stored in containers between runs</li> <li>Limited Privileges: Containers run with minimal required permissions</li> </ul>"}, {"location": "database-engines/postgresql-docker/#security-best-practices", "title": "Security Best Practices", "text": "<pre><code>targets:\n  secure-docker:\n    snapshot:\n      # Use read-only credentials for source\n      src_url: \"pgdocker://readonly:readonly_pass@prod:5432/app?sslmode=require\"\n\n      # Use limited credentials for destination\n      dst_url: \"pgdocker://dev:dev_pass@dev:5432/app?sslmode=disable\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "database-engines/postgresql-docker/#container-resource-management", "title": "Container Resource Management", "text": "<pre><code>targets:\n  optimized-docker:\n    cpus: 6  # Utilize available CPU cores\n\n    # Large database optimization\n    snapshot:\n      src_url: \"pgdocker://user:pass@source/large_db\"\n      dst_url: \"pgdocker://user:pass@dest/large_db\"\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#network-performance", "title": "Network Performance", "text": "<ul> <li>Local Networks: Containers communicate via bridge networks</li> <li>External Connections: Optimized for remote database access</li> <li>Parallel Processing: Multiple containers for concurrent operations</li> </ul>"}, {"location": "database-engines/postgresql-docker/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "database-engines/postgresql-docker/#common-issues", "title": "Common Issues", "text": "<p>Docker Not Available: <pre><code># Verify Docker is running\ndocker version\ndocker ps\n</code></pre></p> <p>Container Creation Failures: <pre><code># Check Docker permissions\ndocker run --rm hello-world\n\n# Verify image availability\ndocker pull postgres:16-alpine\n</code></pre></p> <p>Network Connectivity Issues: <pre><code>targets:\n  debug-network:\n    snapshot:\n      # Test with simplified connection\n      src_url: \"pgdocker://user:pass@host:5432/db?connect_timeout=30\"\n</code></pre></p> <p>Volume Mounting Problems: - Ensure Docker daemon has access to file paths - Check Docker-in-Docker volume sharing - Verify file permissions on host system</p>"}, {"location": "database-engines/postgresql-docker/#debug-commands", "title": "Debug Commands", "text": "<pre><code># Test Docker engine connectivity\ndbsnapper build docker-target --dry-run --verbose\n\n# Check container logs\ndocker logs $(docker ps -l -q)\n\n# Verify network creation\ndocker network ls | grep dbsnapper\n\n# Test image availability\ndocker images | grep postgres\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#performance-issues", "title": "Performance Issues", "text": "<p>Slow Operations: - Check available system resources - Increase <code>cpus</code> setting for parallel operations - Use schema filtering to reduce data volume - Consider network latency to database servers</p> <p>Container Startup Delays: - Pre-pull Docker images: <code>docker pull postgres:16-alpine</code> - Use faster storage for Docker volumes - Optimize Docker daemon configuration</p>"}, {"location": "database-engines/postgresql-docker/#docker-requirements", "title": "Docker Requirements", "text": ""}, {"location": "database-engines/postgresql-docker/#system-requirements", "title": "System Requirements", "text": "<ul> <li>Docker Engine: 20.10+ recommended</li> <li>Docker Compose: 2.0+ (for complex deployments)</li> <li>System Resources: 2GB+ RAM, adequate CPU for parallel operations</li> <li>Network: Bridge network support</li> </ul>"}, {"location": "database-engines/postgresql-docker/#installation-verification", "title": "Installation Verification", "text": "<pre><code># Verify Docker installation\ndocker --version\ndocker info\n\n# Test PostgreSQL image\ndocker run --rm postgres:16-alpine pg_dump --version\n\n# Check network capabilities\ndocker network create test-network\ndocker network rm test-network\n</code></pre>"}, {"location": "database-engines/postgresql-docker/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>PostgreSQL Local Engine - Local tool alternative</li> <li>MySQL Docker Engine - MySQL containerized alternative</li> <li>Database Engines Introduction - Engine architecture overview</li> </ul>"}, {"location": "database-engines/postgresql-local/", "title": "PostgreSQL Local (<code>postgres://</code>)", "text": ""}, {"location": "database-engines/postgresql-local/#overview", "title": "Overview", "text": "<p>The PostgreSQL Local engine provides direct connections to PostgreSQL databases using locally installed command-line tools. This engine offers the best performance for local development and environments where PostgreSQL client tools are readily available.</p>"}, {"location": "database-engines/postgresql-local/#architecture", "title": "Architecture", "text": "<p>The PostgreSQL Local engine implements the Database and DatabaseTools interfaces using:</p> <ul> <li>Connection Management: Direct database connections via <code>sqlx</code> and <code>pgx</code> drivers</li> <li>Data Operations: Native SQL execution for queries and schema operations  </li> <li>Tool Operations: Local <code>pg_dump</code>, <code>pg_restore</code>, and <code>psql</code> command execution</li> <li>Schema Filtering: Advanced include/exclude patterns for selective exports</li> </ul>"}, {"location": "database-engines/postgresql-local/#requirements", "title": "Requirements", "text": ""}, {"location": "database-engines/postgresql-local/#postgresql-client-tools", "title": "PostgreSQL Client Tools", "text": "<p>The engine requires these PostgreSQL tools in your system PATH:</p> <ul> <li><code>pg_dump</code> - Database backup creation</li> <li><code>pg_restore</code> - Database restoration from custom format dumps</li> <li><code>psql</code> - SQL execution and plain text restore operations</li> </ul>"}, {"location": "database-engines/postgresql-local/#supported-versions", "title": "Supported Versions", "text": "<ul> <li>Tested: PostgreSQL 9.2 through 16.x</li> <li>Recommended: PostgreSQL 12+ for optimal performance</li> <li>Compatible: Any PostgreSQL version with client tools</li> </ul>"}, {"location": "database-engines/postgresql-local/#installation-options", "title": "Installation Options", "text": "<ul> <li>Postgres.app (macOS) - Includes all required tools</li> <li>PostgreSQL Official Installer - Full PostgreSQL installation</li> <li>Package Managers: <code>brew install postgresql</code>, <code>apt-get install postgresql-client</code></li> <li>Docker: Client tools only via <code>postgres:alpine</code> image</li> </ul>"}, {"location": "database-engines/postgresql-local/#url-schemes-and-usage", "title": "URL Schemes and Usage", "text": ""}, {"location": "database-engines/postgresql-local/#supported-url-schemes", "title": "Supported URL Schemes", "text": "<p>The PostgreSQL Local engine responds to multiple URL schemes:</p> Scheme Description CLI Abbreviation <code>postgres://</code> Standard PostgreSQL scheme <code>pg</code> <code>postgresql://</code> Alternative standard scheme <code>pg</code> <code>pg://</code> Short form <code>pg</code> <code>pglocal://</code> Explicit local designation <code>pgl</code> <code>pgl://</code> Shortest local form <code>pgl</code>"}, {"location": "database-engines/postgresql-local/#connection-string-format", "title": "Connection String Format", "text": "<pre><code>postgres://username:password@host:port/database?parameters\n</code></pre> <p>Example Configurations:</p> <pre><code>targets:\n  production-db:\n    snapshot:\n      src_url: \"postgres://app_user:secret@db.company.com:5432/production?sslmode=require\"\n      dst_url: \"postgres://dev_user:dev_pass@localhost:5432/development?sslmode=disable\"\n</code></pre>"}, {"location": "database-engines/postgresql-local/#ssl-configuration", "title": "SSL Configuration", "text": "<pre><code>targets:\n  secure-db:\n    snapshot:\n      src_url: \"postgres://user:pass@host/db?sslmode=require&amp;sslcert=client.crt&amp;sslkey=client.key\"\n</code></pre>"}, {"location": "database-engines/postgresql-local/#advanced-features", "title": "Advanced Features", "text": ""}, {"location": "database-engines/postgresql-local/#schema-filtering-postgresql-exclusive", "title": "Schema Filtering (PostgreSQL Exclusive)", "text": "<p>PostgreSQL Local engine supports sophisticated schema filtering unavailable in other engines:</p> <pre><code>targets:\n  filtered-db:\n    snapshot:\n      src_url: \"postgres://user:pass@host/production\"\n      dst_url: \"postgres://user:pass@localhost/filtered_copy\"\n\n      schema_config:\n        # Include specific schemas\n        include_schemas:\n          - \"public\"          # Exact match\n          - \"app_data\"        # Application data schema\n          - \"tenant_001\"      # Specific tenant schema\n\n        # Exclude patterns (processed after include)\n        exclude_schemas:\n          - \"temp_schema\"     # Temporary schema\n          - \"backup_schema\"   # Backup schema\n          - \"audit_logs\"      # Audit schema\n</code></pre> <p>Schema Filtering Behavior: 1. If <code>include_schemas</code> is specified, only those schemas are processed 2. <code>exclude_schemas</code> patterns are applied after include filtering 3. Empty <code>include_schemas</code> means all schemas (before exclude filtering) 4. Schema names must match exactly (no wildcard patterns)</p>"}, {"location": "database-engines/postgresql-local/#cpu-configuration-and-parallel-operations", "title": "CPU Configuration and Parallel Operations", "text": "<p>PostgreSQL Local engine supports parallel operations for improved performance:</p> <pre><code>targets:\n  parallel-db:\n    cpus: 4  # Use 4 parallel workers\n\n    snapshot:\n      src_url: \"postgres://user:pass@source/large_db\"\n      dst_url: \"postgres://user:pass@dest/large_db_copy\"\n</code></pre> <p>Parallel Operation Benefits: - Faster <code>pg_dump</code> operations on multi-core systems - Concurrent table processing during export - Reduced overall backup time for large databases - Automatically adjusts based on available CPU cores</p>"}, {"location": "database-engines/postgresql-local/#connection-pool-management", "title": "Connection Pool Management", "text": "<p>The engine automatically manages connection pools:</p> <pre><code>targets:\n  pooled-db:\n    # Connection pooling handled automatically\n    # Optimal for high-frequency operations\n    snapshot:\n      src_url: \"postgres://user:pass@host/db?pool_max_conns=10\"\n</code></pre>"}, {"location": "database-engines/postgresql-local/#target-configuration-examples", "title": "Target Configuration Examples", "text": ""}, {"location": "database-engines/postgresql-local/#basic-local-development", "title": "Basic Local Development", "text": "<pre><code>targets:\n  local-dev:\n    snapshot:\n      src_url: \"postgres://postgres:password@localhost:5432/myapp_production\"\n      dst_url: \"postgres://postgres:password@localhost:5432/myapp_development\"\n</code></pre>"}, {"location": "database-engines/postgresql-local/#production-to-staging-with-schema-filtering", "title": "Production to Staging with Schema Filtering", "text": "<pre><code>targets:\n  prod-to-staging:\n    cpus: 8\n\n    snapshot:\n      src_url: \"postgres://readonly:secret@prod.db.company.com:5432/production?sslmode=require\"\n      dst_url: \"postgres://staging:secret@staging.db.company.com:5432/staging?sslmode=require\"\n\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n        exclude_schemas: [\"temp_schema\", \"audit_logs\"]\n\n    sanitize:\n      override_query: |\n        UPDATE users SET \n          email = 'user' || id || '@staging.company.com',\n          phone = '555-0000',\n          ssn = NULL;\n</code></pre>"}, {"location": "database-engines/postgresql-local/#high-performance-large-database", "title": "High-Performance Large Database", "text": "<pre><code>targets:\n  large-db:\n    cpus: 16  # Maximum parallel workers\n\n    snapshot:\n      src_url: \"postgres://dbuser:strongpass@warehouse.company.com:5432/analytics\"\n      dst_url: \"postgres://dbuser:devpass@dev-warehouse:5432/analytics_copy\"\n</code></pre>"}, {"location": "database-engines/postgresql-local/#permissions-and-security", "title": "Permissions and Security", "text": ""}, {"location": "database-engines/postgresql-local/#required-database-permissions", "title": "Required Database Permissions", "text": "<p>For Source Database (Building Snapshots): - <code>CONNECT</code> on database - <code>SELECT</code> on all tables to backup - <code>USAGE</code> on schemas to include - <code>pg_read_all_data</code> role (PostgreSQL 14+) or individual table permissions</p> <p>For Destination Database (Loading Snapshots): - <code>CONNECT</code> on database - <code>CREATE</code> privilege to create tables - <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> on destination tables - Schema creation permissions if needed</p>"}, {"location": "database-engines/postgresql-local/#security-best-practices", "title": "Security Best Practices", "text": "<pre><code>targets:\n  secure-setup:\n    snapshot:\n      # Use dedicated read-only user for source\n      src_url: \"postgres://dbsnapper_readonly:readonly_pass@source/db?sslmode=require\"\n\n      # Use dedicated user with limited privileges for destination\n      dst_url: \"postgres://dbsnapper_dev:dev_pass@dest/db?sslmode=disable\"\n</code></pre> <p>Recommended User Setup:</p> <pre><code>-- Source database - read-only user\nCREATE USER dbsnapper_readonly WITH PASSWORD 'readonly_password';\nGRANT CONNECT ON DATABASE production TO dbsnapper_readonly;\nGRANT USAGE ON SCHEMA public TO dbsnapper_readonly;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO dbsnapper_readonly;\n\n-- Destination database - development user\nCREATE USER dbsnapper_dev WITH PASSWORD 'dev_password';\nGRANT CONNECT ON DATABASE development TO dbsnapper_dev;\nGRANT CREATE ON DATABASE development TO dbsnapper_dev;\nGRANT ALL PRIVILEGES ON SCHEMA public TO dbsnapper_dev;\n</code></pre>"}, {"location": "database-engines/postgresql-local/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "database-engines/postgresql-local/#large-database-strategies", "title": "Large Database Strategies", "text": "<pre><code>targets:\n  optimized-large-db:\n    cpus: 8\n\n    # Use custom format for better compression and parallel restore\n    snapshot:\n      src_url: \"postgres://user:pass@source/large_db\"\n      dst_url: \"postgres://user:pass@dest/large_db\"\n\n      # Schema filtering reduces data volume\n      schema_config:\n        exclude_schemas: [\"audit_logs\", \"temp_schema\", \"backup_schema\"]\n</code></pre>"}, {"location": "database-engines/postgresql-local/#network-optimization", "title": "Network Optimization", "text": "<pre><code>targets:\n  network-optimized:\n    snapshot:\n      # Use connection pooling\n      src_url: \"postgres://user:pass@remote/db?pool_max_conns=5&amp;pool_min_conns=1\"\n\n      # Optimize for network latency\n      dst_url: \"postgres://user:pass@local/db?connect_timeout=30\"\n</code></pre>"}, {"location": "database-engines/postgresql-local/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "database-engines/postgresql-local/#common-issues", "title": "Common Issues", "text": "<p>Tool Not Found Errors: <pre><code># Verify PostgreSQL tools are installed and in PATH\nwhich pg_dump\nwhich pg_restore  \nwhich psql\n</code></pre></p> <p>Connection Failures: <pre><code># Test connection manually\ntargets:\n  test-connection:\n    snapshot:\n      src_url: \"postgres://user:pass@host/db?connect_timeout=10&amp;sslmode=disable\"\n</code></pre></p> <p>Schema Filtering Not Working: - Ensure schema names match exactly (case-sensitive) - Verify wildcard patterns are correct - Check that user has permissions on filtered schemas</p> <p>Performance Issues: - Increase <code>cpus</code> setting for parallel operations - Use schema filtering to reduce data volume - Consider network latency for remote connections</p>"}, {"location": "database-engines/postgresql-local/#validation-commands", "title": "Validation Commands", "text": "<pre><code># Test basic connectivity\ndbsnapper build test-target --dry-run\n\n# Validate schema filtering\ndbsnapper build filtered-target --dry-run --verbose\n\n# Check parallel performance\ndbsnapper build large-target --cpus 8 --verbose\n</code></pre>"}, {"location": "database-engines/postgresql-local/#related-documentation", "title": "Related Documentation", "text": "<ul> <li>PostgreSQL Docker Engine - Container-based alternative</li> <li>MySQL Local Engine - MySQL comparison</li> <li>Database Engines Introduction - Engine architecture overview</li> </ul>"}, {"location": "dbsnapper-cloud/introduction/", "title": "DBSnapper Cloud: Team Collaboration Platform", "text": "<p>When individual developers need realistic data, the local DBSnapper Agent is perfect. But when your team needs to collaborate on database snapshots, share sanitized data securely, and integrate with enterprise workflows\u2014that's where DBSnapper Cloud transforms your organization's approach to development data.</p>"}, {"location": "dbsnapper-cloud/introduction/#the-team-development-data-challenge", "title": "The Team Development Data Challenge", "text": "<p>As development teams grow, the database snapshot problem multiplies exponentially:</p>"}, {"location": "dbsnapper-cloud/introduction/#coordination-chaos", "title": "\ud83d\udd04 Coordination Chaos", "text": "<ul> <li>Different team members create inconsistent snapshots </li> <li>QA teams can't reproduce bugs due to data variations</li> <li>New developers spend days setting up realistic databases</li> <li>Database snapshots become tribal knowledge</li> </ul>"}, {"location": "dbsnapper-cloud/introduction/#security-compliance-at-scale", "title": "\ud83d\udd10 Security &amp; Compliance at Scale", "text": "<ul> <li>Sharing database snapshots via Slack or email exposes sensitive data</li> <li>No audit trail of who accessed what data when</li> <li>Individual sanitization approaches create compliance gaps</li> <li>Production access requests create security bottlenecks</li> </ul>"}, {"location": "dbsnapper-cloud/introduction/#integration-bottlenecks", "title": "\u26a1 Integration Bottlenecks", "text": "<ul> <li>CI/CD pipelines need consistent, up-to-date test data</li> <li>Manual snapshot management doesn't scale to multiple environments</li> <li>Different cloud storage approaches across teams create silos</li> <li>No centralized way to manage database credentials and access</li> </ul>"}, {"location": "dbsnapper-cloud/introduction/#dbsnapper-cloud-your-teams-database-snapshot-hub", "title": "DBSnapper Cloud: Your Team's Database Snapshot Hub", "text": "<p>DBSnapper Cloud extends the security-by-design approach of the DBSnapper Agent to enable seamless team collaboration while maintaining your organization's data sovereignty.</p>"}, {"location": "dbsnapper-cloud/introduction/#security-first-team-collaboration", "title": "\ud83d\udee1\ufe0f Security-First Team Collaboration", "text": "<p>Your Data, Your Infrastructure, Your Control: <pre><code># Cloud-managed targets use your own storage\nstorage_profiles:\n  team_s3:\n    provider: s3\n    bucket: your-company-snapshots  # Your S3 bucket\n    awscli_profile: production      # Your AWS credentials\n\ntargets:\n  production_api:\n    snapshot:\n      src_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@your-db:5432/api\"\n    storage_profile: team_s3        # Snapshots stored in YOUR infrastructure\n    sanitize:\n      override_query: \"UPDATE users SET email = CONCAT('user', id, '@example.com')\"\n    sso_groups: [\"developers\", \"qa-team\"]  # Access controlled by YOUR SSO\n</code></pre></p> <p>What This Means: - Your databases stay in your infrastructure - Your snapshots are stored in your cloud storage accounts - Your SSO controls who accesses what data - DBSnapper Cloud only orchestrates\u2014never stores your production data</p>"}, {"location": "dbsnapper-cloud/introduction/#v30-performance-at-scale", "title": "\u26a1 V3.0 Performance at Scale", "text": "<p>Multi-Core Operations with Team Coordination: <pre><code># Configure optimal performance for your infrastructure\ndefaults:\n  cpus: 4  # Global setting for all team operations\n\ntargets:\n  large_production_db:\n    cpus: 8  # Use more cores for your biggest database\n    snapshot:\n      src_url: \"postgresql://prod-cluster:5432/large_app\"\n    # Team members inherit these performance optimizations\n</code></pre></p> <p>Benefits: - Faster snapshots for everyone using multi-core PostgreSQL operations - Consistent performance across team members and CI/CD systems - Container-native operations scale perfectly in Kubernetes environments</p>"}, {"location": "dbsnapper-cloud/introduction/#enterprise-grade-team-features", "title": "\ud83d\udc65 Enterprise-Grade Team Features", "text": "\ud83d\udd10 SSO Integration &amp; Access Control\ud83d\udcca Centralized Management\ud83e\udd16 Modern Development Integration <p>Connect DBSnapper to your existing identity infrastructure:</p> <pre><code>sso:\n  okta:\n    provider_url: https://your-org.okta.com\n    client_id: your-client-id\n\n# Team-based access control\ntargets:\n  user_database:\n    sanitize:\n      override_query: \"UPDATE users SET ssn = NULL, email = CONCAT('user', id, '@example.com')\"\n    sso_groups: [\"developers\", \"qa-team\"]  # Only these groups can access sanitized snapshots\n\n  financial_reports:\n    sso_groups: [\"senior-developers\", \"data-team\"]  # More restricted access\n</code></pre> <p>Cloud-Based Target Management: - Define database targets once, use everywhere - Centralized sanitization rules ensure compliance consistency - Team members get automatic access to authorized targets - Audit logs track all snapshot access and creation</p> <p>Seamless Workflow Integration: <pre><code># GitHub Actions with cloud-managed targets\n- uses: dbsnapper/install-dbsnapper-agent-action@v1\n- run: |\n    # Use cloud-configured target with team storage\n    dbsnapper build production-api --sanitize\n    dbsnapper load production-api latest\n</code></pre></p> <p>Available Integrations: - VS Code Extension - Access cloud targets directly from your editor - GitHub Actions - Automated snapshots with team-shared storage - Terraform Provider - Infrastructure as Code for database snapshot management - MCP Server - AI assistant integration with cloud targets</p>"}, {"location": "dbsnapper-cloud/introduction/#real-world-team-workflows", "title": "Real-World Team Workflows", "text": ""}, {"location": "dbsnapper-cloud/introduction/#development-team-workflow", "title": "Development Team Workflow", "text": "<pre><code>graph LR\n    A[DevOps Engineer] --&gt;|Configures Target| B[DBSnapper Cloud]\n    B --&gt;|Sanitized Snapshots| C[Developer A]\n    B --&gt;|Sanitized Snapshots| D[Developer B]\n    B --&gt;|Sanitized Snapshots| E[QA Team]\n    B --&gt;|Sanitized Snapshots| F[CI/CD Pipeline]\n\n    C --&gt;|dbsnapper load api-db| G[Local Dev Environment]\n    D --&gt;|dbsnapper load api-db| H[Local Dev Environment]\n    E --&gt;|dbsnapper load api-db| I[Testing Environment]\n    F --&gt;|Automated Testing| J[Staging Environment]</code></pre> <ol> <li>DevOps Engineer creates production target with sanitization rules</li> <li>Developers instantly access latest sanitized snapshots: <code>dbsnapper load production-api</code></li> <li>QA Team gets consistent test data for reliable bug reproduction</li> <li>CI/CD Pipelines automatically use up-to-date snapshots for integration tests</li> </ol>"}, {"location": "dbsnapper-cloud/introduction/#enterprise-security-workflow", "title": "Enterprise Security Workflow", "text": "<ul> <li>Production data never leaves your approved infrastructure</li> <li>Sanitized snapshots are automatically shared with authorized team members</li> <li>SSO groups control access (developers get basic access, senior developers get additional targets)</li> <li>Audit logs provide compliance evidence for security reviews</li> </ul>"}, {"location": "dbsnapper-cloud/introduction/#cloud-vs-local-choose-your-approach", "title": "Cloud vs. Local: Choose Your Approach", "text": "Capability Local Agent Only + DBSnapper Cloud Individual Development Perfect \u2705 Perfect \u2705 Team Snapshot Sharing Manual process Automatic \u2705 Consistent Sanitization Per-developer setup Centralized rules \u2705 SSO Integration Not available Full integration \u2705 Audit &amp; Compliance Local logs only Enterprise audit trails \u2705 CI/CD Integration Manual configuration Seamless automation \u2705 Cross-Team Access Not available Group-based permissions \u2705"}, {"location": "dbsnapper-cloud/introduction/#getting-started-with-your-team", "title": "Getting Started with Your Team", "text": ""}, {"location": "dbsnapper-cloud/introduction/#quick-setup-path", "title": "Quick Setup Path", "text": "<pre><code># 1. Individual developers already using DBSnapper Agent\ndbsnapper config init\n\n# 2. DevOps engineer configures team targets in DBSnapper Cloud\n#    (Web interface - no additional CLI installation required)\n\n# 3. Team members automatically get access to cloud-managed targets\ndbsnapper targets  # Now shows both local AND cloud targets\ndbsnapper build production-api --sanitize  # Uses team storage and sanitization rules\n</code></pre>"}, {"location": "dbsnapper-cloud/introduction/#implementation-steps", "title": "Implementation Steps", "text": "<ol> <li>Sign up for DBSnapper Cloud - Free trial available</li> <li>Configure Storage Profiles - Connect your existing cloud storage  </li> <li>Set up Database Targets - Define your production databases with sanitization</li> <li>Enable SSO Integration - Connect your identity provider (optional but recommended)</li> <li>Implement Team Workflows - End-to-end workflows for DevOps, developers, QA, and compliance teams</li> <li>Team members continue using existing DBSnapper Agent - Cloud targets appear automatically</li> </ol>"}, {"location": "dbsnapper-cloud/introduction/#your-data-your-infrastructure-our-orchestration", "title": "Your Data, Your Infrastructure, Our Orchestration", "text": "<p>DBSnapper Cloud Approach: - Processing: Happens in your infrastructure (via DBSnapper Agent) - Storage: Uses your cloud storage accounts (S3, R2, etc.) - Access Control: Integrates with your SSO (Okta, Auth0, etc.) - Data Security: Your production data never leaves your environment</p> <p>What DBSnapper Cloud Provides: - Target configuration management and team sharing - SSO integration and group-based access control - Audit logging and compliance reporting - Seamless integration with development tools</p> <p>Ready to scale your database snapshot workflow across your entire team? Configure your first storage profile to get started.</p>"}, {"location": "dbsnapper-cloud/storage_profiles/", "title": "Storage Profiles: Secure Cloud Storage for Database Snapshots", "text": "<p>Storage Profiles define where and how your database snapshots are stored in cloud storage providers. They're essential for team collaboration, backup strategies, and implementing proper security boundaries between raw and sanitized data.</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#why-cloud-storage-for-database-snapshots", "title": "Why Cloud Storage for Database Snapshots?", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#team-collaboration", "title": "\ud83d\udd17 Team Collaboration", "text": "<p>Enable seamless snapshot sharing across your organization: - Developers can access the latest sanitized snapshots instantly - QA teams get consistent test data for reliable bug reproduction - CI/CD pipelines automatically use up-to-date snapshots</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#security-boundaries", "title": "\ud83d\udee1\ufe0f Security Boundaries", "text": "<p>Implement proper data access controls: - Raw snapshots \u2192 Restricted storage (production team access only) - Sanitized snapshots \u2192 Shared storage (broader team access) - Audit trails \u2192 Track all snapshot access and modifications</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#cost-efficiency", "title": "\ud83d\udcb0 Cost Efficiency", "text": "<p>Optimize storage costs and performance: - Compressed snapshots typically use 10-50% of original database size - Lifecycle policies automatically manage snapshot retention - Multi-region storage for disaster recovery scenarios</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#supported-cloud-storage-providers", "title": "Supported Cloud Storage Providers", "text": "<p>DBSnapper follows a \"Bring Your Own Cloud Storage\" approach\u2014use your existing cloud storage accounts with your security policies and access controls.</p> \ud83c\udf10 Amazon S3\u26a1 Cloudflare R2\ud83c\udfe0 MinIO (Self-Hosted)\ud83c\udf0a DigitalOcean Spaces <p>Full S3 ecosystem support: <pre><code>storage_profiles:\n  production_s3:\n    provider: s3\n    region: us-west-2\n    bucket: dbsnapper-prod-snapshots\n    prefix: raw-snapshots\n    awscli_profile: production  # Use AWS CLI credentials\n\n  sanitized_s3:\n    provider: s3\n    region: us-west-2\n    bucket: dbsnapper-team-snapshots\n    prefix: sanitized-snapshots  \n    awscli_profile: team-access  # Different credentials for team access\n</code></pre></p> <p>Benefits: - Integrated with your existing AWS IAM policies - Server-side encryption (SSE-S3, SSE-KMS) - Intelligent tiering for cost optimization - Cross-region replication for disaster recovery</p> <p>Zero egress fees with global performance: <pre><code>storage_profiles:\n  r2_sanitized:\n    provider: r2\n    bucket: dbsnapper-r2-sanitized\n    prefix: team-snapshots\n    account_id: your-cloudflare-account-id\n    awscli_profile: r2_profile  # Configure AWS CLI for R2\n</code></pre></p> <p>Benefits: - Zero egress charges (significant cost savings) - Global edge network for fast access - S3-compatible API (works with existing tools) - Built-in DDoS protection</p> <p>Complete control with on-premises or private cloud: <pre><code>storage_profiles:\n  minio_local:\n    provider: minio\n    endpoint: https://minio.yourcompany.com\n    bucket: dbsnapper-snapshots\n    prefix: production\n    access_key: your-minio-access-key\n    secret_key: your-minio-secret-key\n</code></pre></p> <p>Benefits: - Complete data sovereignty - No vendor lock-in - Kubernetes-native deployments - Full control over security and compliance</p> <p>Simple, cost-effective object storage: <pre><code>storage_profiles:\n  do_spaces:\n    provider: dospaces\n    endpoint: https://nyc3.digitaloceanspaces.com\n    bucket: dbsnapper-do-snapshots\n    prefix: team-data\n    access_key: your-spaces-key\n    secret_key: your-spaces-secret\n</code></pre></p>"}, {"location": "dbsnapper-cloud/storage_profiles/#creating-storage-profiles", "title": "Creating Storage Profiles", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#option-1-dbsnapper-cloud-web-interface", "title": "Option 1: DBSnapper Cloud Web Interface", "text": "<p>Visual Configuration (Recommended for most users):</p> <ol> <li>Navigate to Storage Profiles in DBSnapper Cloud</li> <li>Click \"New Storage Profile\" to create a profile</li> <li>Configure Provider Settings using the forms below</li> <li>Test connectivity before saving</li> </ol> <p></p>"}, {"location": "dbsnapper-cloud/storage_profiles/#option-2-local-configuration-file", "title": "Option 2: Local Configuration File", "text": "<p>Advanced users can define storage profiles in their local configuration:</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code># Define storage profiles locally (synced to cloud when authtoken present)\nstorage_profiles:\n  prod_s3:\n    provider: s3\n    region: us-west-2\n    bucket: mycompany-db-snapshots\n    prefix: production\n    awscli_profile: dbsnapper-prod\n\n  team_r2:\n    provider: r2\n    bucket: team-sanitized-data\n    prefix: shared\n    account_id: your-account-id\n    awscli_profile: r2_team\n\n# Use in targets\ntargets:\n  production_api:\n    storage_profile: prod_s3        # Raw snapshots go here\n    sanitize:\n      storage_profile: team_r2      # Sanitized snapshots shared here\n</code></pre>"}, {"location": "dbsnapper-cloud/storage_profiles/#storage-profile-configuration", "title": "Storage Profile Configuration", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#basic-configuration", "title": "Basic Configuration", "text": "Profile Name Unique identifier for this storage profile (used in target configuration) Provider Cloud storage provider (s3, r2, minio, dospaces) Bucket Storage bucket/container name where snapshots will be stored Prefix (Optional) Folder/path prefix for organizing snapshots (e.g., <code>production/</code>, <code>team-snapshots/</code>)"}, {"location": "dbsnapper-cloud/storage_profiles/#authentication-methods", "title": "Authentication Methods", "text": "\ud83d\udd11 AWS CLI Profiles (Recommended)\ud83d\udd10 Direct Credentials\ud83c\udfad IAM Roles (AWS) <p>Most secure approach using your existing AWS CLI configuration:</p> <pre><code># Configure AWS CLI profile for DBSnapper\naws configure --profile dbsnapper-prod\n\n# Configure additional profile for R2\naws configure --profile r2_team \\\n  --endpoint-url https://your-account-id.r2.cloudflarestorage.com\n</code></pre> <pre><code>storage_profiles:\n  secure_s3:\n    provider: s3\n    bucket: secure-snapshots\n    awscli_profile: dbsnapper-prod  # Uses existing CLI credentials\n</code></pre> <p>Benefits: - Credentials stored in standard AWS CLI location - Supports MFA and role assumption - Works with AWS SSO and temporary credentials - No credentials in DBSnapper configuration files</p> <p>For environments where AWS CLI profiles aren't available:</p> <pre><code>storage_profiles:\n  direct_s3:\n    provider: s3\n    region: us-east-1\n    bucket: dbsnapper-direct\n    access_key: AKIA...\n    secret_key: xyz...\n</code></pre> <p>Security Consideration</p> <p>Store sensitive credentials as environment variables rather than in configuration files: <pre><code>storage_profiles:\n  secure_direct:\n    provider: s3\n    bucket: dbsnapper-secure\n    access_key: \"{{`AWS_ACCESS_KEY_ID` | env}}\"\n    secret_key: \"{{`AWS_SECRET_ACCESS_KEY` | env}}\"\n</code></pre></p> <p>For EC2, ECS, Lambda, and other AWS services:</p> <pre><code># No credentials needed - uses instance/service IAM role\nstorage_profiles:\n  iam_role_s3:\n    provider: s3\n    region: us-west-2\n    bucket: dbsnapper-role-based\n    # Credentials automatically provided by AWS SDK\n</code></pre> <p>Required IAM Policy: <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::dbsnapper-role-based/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"arn:aws:s3:::dbsnapper-role-based\"\n    }\n  ]\n}\n</code></pre></p>"}, {"location": "dbsnapper-cloud/storage_profiles/#security-best-practices", "title": "Security Best Practices", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#implement-storage-separation", "title": "\ud83d\udcca Implement Storage Separation", "text": "<p>Use different storage profiles for different security boundaries:</p> <pre><code>targets:\n  production_database:\n    # Raw production data - highly restricted access\n    storage_profile: raw_prod_s3\n\n    sanitize:\n      # Sanitized data - broader team access  \n      storage_profile: sanitized_team_s3\n      override_query: \"UPDATE users SET email = CONCAT('user', id, '@example.com')\"\n\n    # Share only sanitized snapshots with these groups\n    sso_groups: [\"developers\", \"qa-team\"]\n</code></pre>"}, {"location": "dbsnapper-cloud/storage_profiles/#encryption-configuration", "title": "\ud83d\udd12 Encryption Configuration", "text": "<p>Enable encryption at rest for sensitive snapshots:</p> Amazon S3Cloudflare R2 <pre><code># Configure encryption in your S3 bucket settings\n# DBSnapper supports:\n# - SSE-S3 (Amazon S3 managed keys)\n# - SSE-KMS (AWS Key Management Service)\n# - SSE-C (Customer-provided keys)\n</code></pre> <p>S3 Bucket Policy Example: <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenyInsecureConnections\",\n      \"Effect\": \"Deny\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:*\",\n      \"Resource\": [\n        \"arn:aws:s3:::dbsnapper-secure/*\",\n        \"arn:aws:s3:::dbsnapper-secure\"\n      ],\n      \"Condition\": {\n        \"Bool\": {\n          \"aws:SecureTransport\": \"false\"\n        }\n      }\n    }\n  ]\n}\n</code></pre></p> <pre><code># R2 provides encryption at rest by default\n# Configure additional security via Cloudflare dashboard:\n# - Access tokens with limited permissions\n# - API token expiration policies  \n# - IP address restrictions\n</code></pre>"}, {"location": "dbsnapper-cloud/storage_profiles/#access-control-monitoring", "title": "\ud83d\udd0d Access Control &amp; Monitoring", "text": "<p>Implement comprehensive access controls:</p> <pre><code># Production raw data - restricted access\nstorage_profiles:\n  production_raw:\n    provider: s3\n    bucket: dbsnapper-production-raw\n    awscli_profile: production_restricted\n    # Only DevOps team has access to this profile\n\n  # Team sanitized data - broader access\n  team_sanitized:  \n    provider: s3\n    bucket: dbsnapper-team-sanitized\n    awscli_profile: team_shared\n    # All developers can access sanitized snapshots\n</code></pre> <p>Monitoring and Audit: - Enable S3/R2 access logging to track snapshot usage - Set up CloudTrail (AWS) or equivalent monitoring for API calls - Configure alerts for unusual access patterns - Regular review of storage costs and usage patterns</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#performance-optimization", "title": "Performance Optimization", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#uploaddownload-performance", "title": "\ud83d\udcc8 Upload/Download Performance", "text": "<p>Optimize for large database snapshots:</p> <pre><code># Use regions close to your database infrastructure\nstorage_profiles:\n  optimized_s3:\n    provider: s3\n    region: us-west-2  # Same region as your RDS/Aurora instances\n    bucket: dbsnapper-optimized\n    prefix: snapshots\n</code></pre> <p>Performance Tips: - Region Selection: Choose storage regions close to your databases - Multipart Uploads: Automatically handled for large snapshots - Compression: DBSnapper automatically compresses snapshots (typically 10-50% size reduction) - Parallel Operations: V3.0 multi-core operations improve upload speeds</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#cost-optimization", "title": "\ud83d\udcb0 Cost Optimization", "text": "<p>Implement intelligent storage lifecycle policies:</p> S3 Lifecycle ManagementR2 Cost Benefits <pre><code>{\n  \"Rules\": [\n    {\n      \"ID\": \"DBSnapper-Lifecycle\",\n      \"Status\": \"Enabled\",\n      \"Filter\": {\n        \"Prefix\": \"snapshots/\"\n      },\n      \"Transitions\": [\n        {\n          \"Days\": 30,\n          \"StorageClass\": \"STANDARD_IA\"\n        },\n        {\n          \"Days\": 90,  \n          \"StorageClass\": \"GLACIER\"\n        }\n      ],\n      \"Expiration\": {\n        \"Days\": 365\n      }\n    }\n  ]\n}\n</code></pre> <p>Cloudflare R2 advantages: - Zero egress fees (major cost savings) - Competitive storage pricing - No charges for API requests - Global edge network for performance</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#common-issues-solutions", "title": "\ud83d\udd27 Common Issues &amp; Solutions", "text": "<p>Connection Test Failures:</p> <pre><code># Test storage profile connectivity\ndbsnapper config check --storage-profile production_s3\n\n# Test with debug information\nDBSNAPPER_DEBUG=true dbsnapper config check --storage-profile production_s3\n</code></pre> <p>Common Problems:</p> AWS Credentials IssuesBucket Permission IssuesNetwork Connectivity Issues <pre><code>Error: failed to list objects in bucket: NoCredentialProviders\n</code></pre> <p>Solutions: 1. Verify AWS CLI profile exists: <code>aws configure list --profile dbsnapper-prod</code> 2. Test profile access: <code>aws s3 ls s3://your-bucket --profile dbsnapper-prod</code> 3. Check IAM permissions include <code>s3:ListBucket</code> 4. Ensure profile name matches storage profile configuration</p> <pre><code>Error: Access Denied (Service: Amazon S3; Status Code: 403)\n</code></pre> <p>Solutions: 1. Verify bucket exists and is accessible 2. Check IAM policy includes required S3 permissions 3. Confirm bucket region matches profile configuration 4. Test with minimal IAM policy first, then expand</p> <pre><code>Error: timeout connecting to storage provider\n</code></pre> <p>Solutions: 1. Check firewall rules allow HTTPS (443) outbound 2. Verify DNS resolution for storage endpoints 3. Test with different regions/endpoints 4. Check corporate proxy settings</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#validation-and-testing", "title": "\ud83d\udd0d Validation and Testing", "text": "<p>Validate storage profile configuration:</p> <pre><code># Check all storage profiles\ndbsnapper config validate --storage-profiles\n\n# Test specific profile with verbose output\ndbsnapper config check --storage-profile team_s3 --verbose\n\n# Test upload/download cycle\ndbsnapper build test-target --dry-run  # Verify configuration without creating snapshot\n</code></pre>"}, {"location": "dbsnapper-cloud/storage_profiles/#migration-and-management", "title": "Migration and Management", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#migrating-between-storage-providers", "title": "\ud83d\ude80 Migrating Between Storage Providers", "text": "<p>Example: Migrating from S3 to R2:</p> <pre><code># 1. Create new R2 storage profile\nstorage_profiles:\n  new_r2:\n    provider: r2\n    bucket: migrated-snapshots\n    account_id: your-r2-account\n    awscli_profile: r2_profile\n\n# 2. Update target to use new profile\ntargets:\n  production_api:\n    storage_profile: new_r2  # Changed from old S3 profile\n</code></pre> <p>Migration Process: 1. Create new storage profile with target provider 2. Test connectivity and permissions 3. Update targets to use new storage profile 4. Create new snapshots with new provider 5. Optionally migrate existing snapshots using cloud provider tools</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#storage-profile-management", "title": "\ud83d\udccb Storage Profile Management", "text": "<p>Best practices for ongoing management:</p> <ul> <li>Naming Convention: Use descriptive names (<code>prod-raw-s3</code>, <code>team-sanitized-r2</code>)</li> <li>Documentation: Document which profiles are used by which teams</li> <li>Regular Audits: Review storage costs and access patterns monthly</li> <li>Credential Rotation: Regular rotation of access keys and secrets</li> <li>Backup Strategy: Consider multiple storage providers for critical snapshots</li> </ul>"}, {"location": "dbsnapper-cloud/storage_profiles/#next-steps", "title": "Next Steps", "text": "<p>Now that you've configured storage profiles:</p> <ol> <li>Create Database Targets - Define your databases and link them to storage profiles</li> <li>Set up Team Access - Configure SSO for secure team collaboration</li> <li>Test Your Setup - Create your first cloud-stored snapshot</li> <li>GitHub Actions Integration - Connect with CI/CD and development tools</li> </ol> <p>Need help with storage configuration? Check our Configuration Settings guide for comprehensive examples, or explore Cloud Storage detailed documentation for advanced scenarios.</p> <p>Ready to implement complete team workflows? Our Team Workflows guide shows how storage profiles integrate with targets, SSO, and daily development processes for enterprise-scale database snapshot collaboration.</p>"}, {"location": "dbsnapper-cloud/targets/", "title": "Database Targets: Your Team's Snapshot Workflows", "text": "<p>Database targets are where DBSnapper Cloud transforms individual database snapshots into team-wide collaborative workflows. A target defines everything needed to safely snapshot production data, sanitize it for security compliance, and share it with the right team members\u2014all while maintaining your organization's data sovereignty.</p>"}, {"location": "dbsnapper-cloud/targets/#why-team-managed-database-targets", "title": "Why Team-Managed Database Targets?", "text": ""}, {"location": "dbsnapper-cloud/targets/#from-individual-chaos-to-team-coordination", "title": "\ud83c\udfaf From Individual Chaos to Team Coordination", "text": "<p>Before DBSnapper Cloud Targets: - Each developer creates their own snapshots with different sanitization approaches - QA teams can't reproduce bugs due to inconsistent test data - Production database access becomes a security and compliance nightmare - New team members spend days figuring out how to get realistic development data</p> <p>With Centralized Target Management: - DevOps engineers define production targets once with proper sanitization rules - All team members automatically get access to consistent, sanitized snapshots - Security teams have centralized control over data access and audit trails - Compliance requirements are built into the snapshot creation process</p>"}, {"location": "dbsnapper-cloud/targets/#v30-performance-at-enterprise-scale", "title": "\ud83d\ude80 V3.0 Performance at Enterprise Scale", "text": "<p>DBSnapper Cloud targets leverage v3.0's multi-core architecture to deliver enterprise-grade performance:</p> <pre><code>targets:\n  large_production_db:\n    cpus: 8  # Use 8 cores for this high-volume database\n    snapshot:\n      src_url: \"postgresql://prod-cluster:5432/large_app\"\n      schema_config:\n        include_schemas: [\"public\", \"analytics\"]  # Only snapshot relevant schemas\n    storage_profile: prod_s3\n    sanitize:\n      storage_profile: team_s3  # Sanitized snapshots shared with team\n      override_query: |\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          ssn = NULL,\n          phone = '555-0000';\n    sso_groups: [\"developers\", \"qa-team\"]\n</code></pre> <p>What This Achieves: - Faster snapshots using multi-core PostgreSQL operations - Targeted data capture with schema filtering (PostgreSQL) - Automatic sanitization with centralized compliance rules - Secure sharing through SSO group-based access control</p>"}, {"location": "dbsnapper-cloud/targets/#team-target-management", "title": "Team Target Management", "text": ""}, {"location": "dbsnapper-cloud/targets/#centralized-target-dashboard", "title": "\ud83c\udf9b\ufe0f Centralized Target Dashboard", "text": "<p>The DBSnapper Cloud targets dashboard gives you complete visibility into your organization's database snapshot ecosystem:</p> <p></p> <p>What You See: - Your Targets - Database targets you've created and manage - Shared Team Targets - Targets shared via SSO groups that you have access to - Connection Status - Real-time connectivity and health monitoring - Recent Activity - Latest snapshots, sanitization runs, and team access - Performance Metrics - Snapshot creation times and storage usage</p>"}, {"location": "dbsnapper-cloud/targets/#target-configuration-workflow", "title": "\u2699\ufe0f Target Configuration Workflow", "text": "\ud83c\udf10 Web Interface (Recommended)\u26a1 Local Configuration Sync\ud83e\udd16 Infrastructure as Code <p>Visual target configuration for most users:</p> <ol> <li>Navigate to Targets in DBSnapper Cloud</li> <li>Click \"Add Target\" to create new or edit existing</li> <li>Configure all settings using the intuitive form interface</li> <li>Test connectivity before saving</li> <li>Team members immediately gain access based on SSO group membership</li> </ol> <p></p> <p>Advanced users can define targets locally that sync to the cloud:</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code># Define targets locally - automatically synced when authtoken present\nauthtoken: your-dbsnapper-cloud-token\n\ntargets:\n  production_api:\n    cpus: 6  # v3.0 multi-core configuration\n    snapshot:\n      src_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@prod:5432/api\"\n      dst_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@dev:5432/api_dev\"\n      schema_config:\n        include_schemas: [\"public\", \"api_data\"]\n    storage_profile: prod_s3\n    sanitize:\n      storage_profile: sanitized_s3\n      override_query: |\n        -- Comprehensive sanitization for GDPR compliance\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          first_name = 'User',\n          last_name = CAST(id AS VARCHAR(10)),\n          phone = '555-0000',\n          ssn = NULL;\n\n        DELETE FROM audit_logs WHERE created_at &lt; NOW() - INTERVAL '90 days';\n    sso_groups: [\"developers\", \"qa-team\"]\n</code></pre> <p>Terraform provider for enterprise infrastructure management:</p> <pre><code>resource \"dbsnapper_target\" \"production_api\" {\n  name = \"production-api\"\n\n  snapshot_config {\n    src_url = \"postgresql://${var.db_user}:${var.db_password}@prod:5432/api\"\n    dst_url = \"postgresql://${var.db_user}:${var.db_password}@dev:5432/api_dev\"\n    cpus = 6\n    schema_config {\n      include_schemas = [\"public\", \"api_data\"]\n    }\n  }\n\n  storage_profile = dbsnapper_storage_profile.prod_s3.name\n\n  sanitization {\n    storage_profile = dbsnapper_storage_profile.sanitized_s3.name\n    override_query = file(\"${path.module}/sanitization/api_sanitize.sql\")\n  }\n\n  sso_groups = [\"developers\", \"qa-team\"]\n}\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#complete-target-configuration-guide", "title": "Complete Target Configuration Guide", "text": ""}, {"location": "dbsnapper-cloud/targets/#core-target-settings", "title": "\ud83d\udccb Core Target Settings", "text": "\ud83c\udff7\ufe0f Target Identity &amp; Performance\ud83d\udd17 Database Connections\ud83d\uddc4\ufe0f Schema Configuration (PostgreSQL Only) Target Name Unique identifier used across all DBSnapper tools and integrations <pre><code># Naming best practices:\nproduction_users_api     # \u2705 Clear, descriptive\nprod-analytics-db        # \u2705 Environment and purpose\nstaging_orders          # \u2705 Environment specific\n\n# Avoid:\ndb1, test, mydb         # \u274c Not descriptive\n</code></pre> CPU Configuration (New in v3.0) Optimize snapshot performance for your infrastructure <pre><code>targets:\n  small_db:\n    cpus: 2  # Lightweight databases\n\n  large_production:\n    cpus: 8  # High-volume production databases\n\n  # If not specified, uses global default from 'defaults.cpus'\n</code></pre> Source Database URL (Required) Production database to snapshot from <pre><code># PostgreSQL with SSL and environment variables\nsrc_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@prod.db.company.com:5432/users_api?sslmode=require\"\n\n# MySQL with connection parameters  \nsrc_url: \"mysql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@mysql-prod:3306/ecommerce\"\n</code></pre> Destination Database URL (Optional) Development/testing database where snapshots are loaded <pre><code># Will be created if it doesn't exist, or completely replaced if it does\ndst_url: \"postgresql://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@dev.db.company.com:5432/users_api_dev\"\n</code></pre> <p>Destructive Operation</p> <p>Loading snapshots completely replaces the destination database. Never use production databases as destinations.</p> <p>Advanced PostgreSQL schema filtering for targeted snapshots: <pre><code>targets:\n  large_production_app:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/large_app\"\n      schema_config:\n        # Option 1: Include only specific schemas\n        include_schemas: [\"public\", \"analytics\", \"reporting\"]\n\n        # Option 2: Exclude specific schemas (alternative to include_schemas)\n        # exclude_schemas: [\"temp_data\", \"logs\", \"debug\"]\n\n        # Option 3: Include all schemas (override default behavior)\n        # use_default_schema: false\n</code></pre></p> <p>Schema Configuration Benefits: - Faster snapshots by excluding unnecessary schemas - Smaller storage usage with targeted data capture - Cleaner development environments without production logs/temp data - Compliance-focused by excluding sensitive operational schemas</p>"}, {"location": "dbsnapper-cloud/targets/#storage-security-configuration", "title": "\u2601\ufe0f Storage &amp; Security Configuration", "text": "\ud83d\udce6 Storage Profiles\ud83e\uddfc Data Sanitization\ud83d\udc65 Team Access Control <p>Implement proper data access boundaries: <pre><code>targets:\n  production_users:\n    # Raw production data - restricted access\n    storage_profile: production_restricted_s3\n\n    sanitize:\n      # Sanitized data - broader team access\n      storage_profile: team_shared_s3\n      override_query: |\n        -- Remove PII while maintaining data utility\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          phone = REGEXP_REPLACE(phone, '\\\\d', '5', 'g'),\n          ssn = NULL,\n          address = 'Test Address';\n</code></pre></p> <p>Three-tier sanitization system for maximum flexibility:</p> <p>Priority 1: Target-Level Override (Highest Priority) <pre><code>targets:\n  users_api:\n    sanitize:\n      override_query: |\n        -- Target-specific sanitization rules\n        UPDATE users SET email = CONCAT('user', id, '@example.com');\n        UPDATE orders SET customer_notes = 'Test order notes';\n</code></pre></p> <p>Priority 2: Global Override (Medium Priority) <pre><code># Applied to all targets unless overridden\noverride:\n  san_query: |\n    -- Organization-wide sanitization standards\n    UPDATE users SET ssn = NULL, phone = '555-0000';\n</code></pre></p> <p>Priority 3: Query File (Lowest Priority) <pre><code>targets:\n  legacy_system:\n    sanitize:\n      query_file: \"sanitization/legacy_sanitize.sql\"  # File in working directory\n</code></pre></p> <p>SSO group-based sharing for secure collaboration: <pre><code>targets:\n  # Basic sharing\n  users_database:\n    sso_groups: [\"developers\", \"qa-team\"]\n\n  # Restricted sharing for sensitive data\n  financial_reports:\n    sso_groups: [\"senior-developers\", \"finance-team\", \"compliance-auditors\"]\n\n  # Public sharing for reference data\n  product_catalog:\n    sso_groups: [\"all-engineering\", \"marketing\", \"customer-support\"]\n</code></pre></p> <p>Access Control Benefits: - Automatic provisioning - New team members get access based on SSO groups - Centralized management - IT manages access through your existing identity provider - Audit compliance - Complete trail of who accessed what data when - Granular permissions - Different data sensitivity levels for different teams</p>"}, {"location": "dbsnapper-cloud/targets/#v30-performance-container-optimization", "title": "\u26a1 V3.0 Performance &amp; Container Optimization", "text": "\ud83d\ude80 Multi-Core Performance\ud83d\udc33 Container-Native Configuration <p>Optimize snapshot operations for your infrastructure: <pre><code># Global performance defaults\ndefaults:\n  cpus: 4  # All targets use 4 cores unless overridden\n\ntargets:\n  # Small database - use fewer resources\n  reference_data:\n    cpus: 1\n\n  # Large production database - maximize performance  \n  user_analytics:\n    cpus: 12  # Use all available cores for large datasets\n    snapshot:\n      schema_config:\n        exclude_schemas: [\"logs\", \"temp_data\"]  # Reduce data volume\n</code></pre></p> <p>Perfect for Kubernetes and Docker environments: <pre><code># Container resource alignment\ntargets:\n  containerized_app:\n    cpus: 4  # Matches Kubernetes resource limits\n    snapshot:\n      src_url: \"postgresql://{{`DB_HOST` | env}}:5432/{{`DB_NAME` | env}}\"\n    storage_profile: container_s3\n\n    # Container-optimized sanitization\n    sanitize:\n      # Uses ephemeral Docker containers (no permanent database needed)\n      override_query: |\n        UPDATE users SET email = 'user@example.com';\n</code></pre></p>"}, {"location": "dbsnapper-cloud/targets/#real-world-team-workflows", "title": "Real-World Team Workflows", "text": ""}, {"location": "dbsnapper-cloud/targets/#complete-development-lifecycle", "title": "\ud83d\udd04 Complete Development Lifecycle", "text": "<pre><code>graph TB\n    A[DevOps Engineer] --&gt;|Creates Target| B[DBSnapper Cloud]\n    B --&gt;|Configures| C[Production DB]\n    B --&gt;|Sanitizes &amp; Stores| D[Team Storage]\n\n    D --&gt; E[Developer Laptop]\n    D --&gt; F[QA Environment] \n    D --&gt; G[CI/CD Pipeline]\n    D --&gt; H[Staging Environment]\n\n    E --&gt;|dbsnapper load api-prod| E1[Local Dev DB]\n    F --&gt;|Automated Load| F1[QA Testing DB]  \n    G --&gt;|Integration Tests| G1[Test DB]\n    H --&gt;|Pre-Production| H1[Staging DB]</code></pre>"}, {"location": "dbsnapper-cloud/targets/#role-based-workflows", "title": "\ud83d\udc68\u200d\ud83d\udcbc Role-Based Workflows", "text": "DevOps Engineer: Target CreationDeveloper: Daily UsageQA Engineer: Testing WorkflowsCI/CD Pipeline: Automated Testing <pre><code># 1. Create target with comprehensive configuration\ntargets:\n  production_users_api:\n    cpus: 8\n    snapshot:\n      src_url: \"postgresql://readonly_user:{{`RO_PASS` | env}}@prod-cluster:5432/users_api\"\n      schema_config:\n        exclude_schemas: [\"audit_logs\", \"temp_data\"]\n    storage_profile: prod_secure_s3\n    sanitize:\n      storage_profile: team_shared_s3\n      override_query: |\n        -- GDPR-compliant sanitization\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          first_name = 'Test',\n          last_name = 'User' || id,\n          phone = '555-0000',\n          ssn = NULL,\n          date_of_birth = '1990-01-01';\n\n        DELETE FROM user_sessions;\n        DELETE FROM audit_logs WHERE created_at &lt; NOW() - INTERVAL '30 days';\n    sso_groups: [\"backend-developers\", \"qa-engineers\", \"data-analysts\"]\n</code></pre> <pre><code># Check available targets (shows both local and cloud targets)\ndbsnapper targets\n\n# Load latest sanitized snapshot for development\ndbsnapper load production_users_api\n\n# Or load specific snapshot\ndbsnapper load production_users_api 2  # Load 2nd most recent\n\n# Check target details and available snapshots\ndbsnapper target production_users_api\n</code></pre> <pre><code># Load consistent snapshot for bug reproduction\ndbsnapper load production_users_api latest\n\n# Create test-specific snapshot\ndbsnapper build qa_test_target --sanitize\n\n# Share snapshot with development team for bug investigation\ndbsnapper pull production_users_api  # Download to local storage\n</code></pre> <pre><code># GitHub Actions workflow\nname: Integration Tests\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: dbsnapper/install-dbsnapper-agent-action@v1\n      - name: Load test database\n        run: |\n          # Cloud target automatically provides latest sanitized snapshot\n          dbsnapper load production_users_api\n\n      - name: Run integration tests\n        run: |\n          # Tests run against realistic production-like data\n          npm run test:integration\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#enterprise-security-compliance", "title": "\ud83d\udd12 Enterprise Security &amp; Compliance", "text": "Multi-Environment Access ControlAudit &amp; Compliance Workflow <pre><code>targets:\n  # Production - highly restricted\n  prod_users_api:\n    sso_groups: [\"devops-team\", \"senior-engineers\"]\n    sanitize:\n      storage_profile: compliance_s3\n\n  # Staging - broader access  \n  staging_users_api:\n    sso_groups: [\"backend-developers\", \"qa-engineers\", \"product-managers\"]\n\n  # Reference data - open access\n  product_catalog:\n    sso_groups: [\"all-engineering\", \"marketing\", \"customer-support\"]\n</code></pre> <pre><code># All target access is logged in DBSnapper Cloud\n# Compliance team can review:\n# - Who accessed which targets when\n# - What sanitization rules were applied\n# - Where snapshots were stored and retrieved\n# - Performance and usage metrics\n\n# Example audit query capabilities:\n# \"Show all access to production_users_api target in the last 30 days\"\n# \"List all users who downloaded sanitized financial_reports snapshots\"\n# \"Verify PII sanitization was applied to all shared snapshots\"\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#production-best-practices", "title": "Production Best Practices", "text": ""}, {"location": "dbsnapper-cloud/targets/#target-architecture-patterns", "title": "\ud83c\udfd7\ufe0f Target Architecture Patterns", "text": "Environment-Based TargetsService-Based Targets <pre><code># Separate targets for different environments\ntargets:\n  prod_api_database:\n    snapshot:\n      src_url: \"postgresql://readonly@prod-cluster:5432/api\"\n    sso_groups: [\"devops\", \"senior-developers\"]\n\n  staging_api_database:  \n    snapshot:\n      src_url: \"postgresql://user@staging-db:5432/api\"\n    sso_groups: [\"developers\", \"qa-team\"]\n\n  dev_api_database:\n    snapshot:\n      src_url: \"postgresql://user@dev-db:5432/api\" \n    sso_groups: [\"all-developers\"]\n</code></pre> <pre><code># Separate targets for different microservices\ntargets:\n  users_service_prod:\n    cpus: 6\n    snapshot:\n      src_url: \"postgresql://readonly@users-db:5432/users\"\n      schema_config:\n        include_schemas: [\"public\", \"user_data\"]\n\n  orders_service_prod:\n    cpus: 4\n    snapshot:\n      src_url: \"postgresql://readonly@orders-db:5432/orders\"\n      schema_config:\n        include_schemas: [\"public\", \"order_data\"]\n\n  analytics_service_prod:\n    cpus: 12  # Large analytics database\n    snapshot:\n      src_url: \"postgresql://readonly@analytics-cluster:5432/analytics\"\n      schema_config:\n        exclude_schemas: [\"temp_calculations\", \"debug_logs\"]\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#security-implementation", "title": "\ud83d\udd10 Security Implementation", "text": "<p>Connection Security: <pre><code># Use dedicated read-only users for snapshots\ntargets:\n  secure_production:\n    snapshot:\n      # Read-only user with minimal permissions\n      src_url: \"postgresql://dbsnapper_readonly:{{`READONLY_PASS` | env}}@prod:5432/app?sslmode=require\"\n\n      # Development database (can be overwritten)\n      dst_url: \"postgresql://dev_user:{{`DEV_PASS` | env}}@dev:5432/app_dev?sslmode=require\"\n</code></pre></p> <p>Network Security: - Use SSL/TLS for all database connections - Implement database firewall rules for DBSnapper agent IPs - Use VPC peering or private networks when possible - Enable connection pooling for high-frequency operations</p> <p>Credential Management: - Store sensitive credentials as environment variables - Use AWS IAM roles or equivalent cloud identity systems - Implement regular credential rotation - Avoid hardcoding credentials in configuration files</p>"}, {"location": "dbsnapper-cloud/targets/#performance-optimization", "title": "\ud83d\udcca Performance Optimization", "text": "<pre><code># Optimize for your infrastructure and data size\ntargets:\n  large_analytical_db:\n    cpus: 16  # Use all available cores\n    snapshot:\n      src_url: \"postgresql://user@analytics-cluster:5432/warehouse\"\n      schema_config:\n        # Only capture relevant schemas to reduce size/time\n        include_schemas: [\"marts\", \"dimensions\", \"facts\"]\n        exclude_schemas: [\"staging\", \"temp\", \"logs\"]\n\n    # Use high-performance storage for large databases\n    storage_profile: nvme_optimized_s3\n\n    sanitize:\n      # Sanitized version can use standard storage\n      storage_profile: standard_team_s3\n      override_query: |\n        -- Minimal sanitization to preserve analytical utility\n        UPDATE customers SET \n          name = 'Customer ' || id,\n          email = CONCAT('customer', id, '@example.com');\n</code></pre> <p>Performance Best Practices: - Use CPU configuration to match your infrastructure capabilities - Implement schema filtering to reduce snapshot size and time - Choose storage regions close to your database infrastructure - Monitor snapshot creation times and optimize accordingly - Use separate storage profiles for different performance requirements</p>"}, {"location": "dbsnapper-cloud/targets/#troubleshooting-validation", "title": "Troubleshooting &amp; Validation", "text": ""}, {"location": "dbsnapper-cloud/targets/#target-testing-validation", "title": "\ud83d\udd27 Target Testing &amp; Validation", "text": "<pre><code># Test target connectivity and configuration\ndbsnapper config check --target production_api\n\n# Validate storage profile connectivity\ndbsnapper config check --storage-profile team_s3\n\n# Dry run to verify configuration without creating snapshot\ndbsnapper build production_api --dry-run\n\n# Test complete workflow\ndbsnapper build test_target &amp;&amp; dbsnapper load test_target\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#common-issues-solutions", "title": "\ud83d\udea8 Common Issues &amp; Solutions", "text": "Connection IssuesPermission IssuesStorage Issues <pre><code>Error: failed to connect to source database\n</code></pre> <p>Solutions: - Verify database credentials and connection string format - Check network connectivity and firewall rules - Ensure database user has required permissions (SELECT for source, CREATE/DROP for destination) - Test connection manually: <code>psql \"postgresql://user:pass@host:5432/db\"</code></p> <pre><code>Error: permission denied for schema public\n</code></pre> <p>Solutions: - Grant appropriate database permissions to DBSnapper user - Use dedicated read-only user for source database connections - Verify SSL/TLS configuration if required - Check that destination database user has CREATE/DROP privileges</p> <pre><code>Error: failed to upload snapshot to cloud storage\n</code></pre> <p>Solutions: - Verify storage profile configuration and credentials - Test cloud storage access independently - Check network connectivity to storage provider - Review IAM permissions for storage operations</p>"}, {"location": "dbsnapper-cloud/targets/#next-steps-integration", "title": "Next Steps &amp; Integration", "text": ""}, {"location": "dbsnapper-cloud/targets/#recommended-next-actions", "title": "\ud83c\udfaf Recommended Next Actions", "text": "<ol> <li>Complete Storage Setup - Configure secure cloud storage for your snapshots</li> <li>Enable SSO Integration - Set up team authentication and access control</li> <li>Explore Advanced Sanitization - Learn comprehensive data privacy techniques</li> <li>Set up CI/CD Integration - Automate snapshots in your development pipeline</li> </ol>"}, {"location": "dbsnapper-cloud/targets/#modern-development-integration", "title": "\ud83d\udd17 Modern Development Integration", "text": "<p>Available Integrations: - VS Code Extension - Manage targets directly from your editor - GitHub Actions - Automated snapshots in CI/CD pipelines - Terraform Provider - Infrastructure as Code management - MCP Server - AI assistant integration for database operations</p> <p>Ready to create your first team target? Start by configuring storage profiles for secure cloud snapshot storage, then return here to create targets that your entire team can access.</p> <p>Need complete implementation guidance? Check out our Team Workflows guide for end-to-end workflows covering DevOps setup, daily developer usage, QA testing, and compliance processes.</p>"}, {"location": "dbsnapper-cloud/team-workflows/", "title": "Team Workflows: End-to-End Database Snapshot Collaboration", "text": "<p>Transform your team's approach to development data with proven workflows that scale from startup teams to enterprise organizations. These workflows integrate all DBSnapper Cloud capabilities\u2014targets, storage, SSO, and sanitization\u2014into practical, day-to-day development processes.</p>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-overview-from-setup-to-daily-usage", "title": "Workflow Overview: From Setup to Daily Usage", "text": "<pre><code>graph TB\n    A[DevOps Setup] --&gt; B[Team Onboarding]\n    B --&gt; C[Daily Development]\n    C --&gt; D[QA &amp; Testing]\n    D --&gt; E[Production Integration]\n    E --&gt; F[Compliance &amp; Audit]\n\n    A1[Configure Targets] --&gt; A\n    A2[Setup Storage] --&gt; A\n    A3[Configure SSO] --&gt; A\n\n    C1[Load Snapshots] --&gt; C\n    C2[Local Development] --&gt; C\n    C3[Feature Testing] --&gt; C\n\n    D1[Consistent Test Data] --&gt; D\n    D2[Bug Reproduction] --&gt; D\n    D3[Integration Testing] --&gt; D\n\n    F1[Access Reviews] --&gt; F\n    F2[Audit Reports] --&gt; F\n    F3[Compliance Evidence] --&gt; F</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-1-devops-team-initial-setup-configuration", "title": "Workflow 1: DevOps Team - Initial Setup &amp; Configuration", "text": ""}, {"location": "dbsnapper-cloud/team-workflows/#infrastructure-setup", "title": "\ud83c\udfd7\ufe0f Infrastructure Setup", "text": "<p>Goal: Establish secure, scalable database snapshot infrastructure for the entire organization.</p> Step 1: Storage InfrastructureStep 2: SSO IntegrationStep 3: Production Targets <pre><code># Configure storage profiles for different security levels\nstorage_profiles:\n  # Production raw data - highly restricted\n  prod_secure_s3:\n    provider: s3\n    region: us-west-2\n    bucket: company-prod-db-snapshots\n    awscli_profile: dbsnapper_prod_readonly\n    # Only DevOps team has access\n\n  # Team sanitized data - broader access\n  team_shared_s3:\n    provider: s3  \n    region: us-west-2\n    bucket: company-team-db-snapshots\n    awscli_profile: dbsnapper_team_shared\n    # Developers and QA have access\n\n  # Analytics data - specialized access\n  analytics_r2:\n    provider: r2\n    bucket: company-analytics-snapshots\n    account_id: your-r2-account\n    awscli_profile: r2_analytics\n    # Data team and analysts have access\n</code></pre> <p>Configure enterprise authentication:</p> <ol> <li>Set up Okta integration following the complete guide</li> <li>Create organizational groups:</li> </ol> <pre><code>Okta Group Structure:\n\u251c\u2500\u2500 dbsnapper-devops           # Full administrative access\n\u251c\u2500\u2500 dbsnapper-senior-devs      # Production database access\n\u251c\u2500\u2500 dbsnapper-developers       # Development and staging access\n\u251c\u2500\u2500 dbsnapper-qa-engineers     # Testing and sanitized data access\n\u251c\u2500\u2500 dbsnapper-data-team        # Analytics and reporting access\n\u2514\u2500\u2500 dbsnapper-contractors      # Limited, time-bound access\n</code></pre> <p>Configure production database targets with comprehensive security:</p> <pre><code>targets:\n  # User API - Core production database\n  prod_users_api:\n    cpus: 8  # High performance for large database\n    snapshot:\n      # Read-only user with minimal permissions\n      src_url: \"postgresql://dbsnapper_readonly:{{`PROD_DB_PASS` | env}}@prod-users-cluster:5432/users_api?sslmode=require\"\n      # Development target database\n      dst_url: \"postgresql://dev_user:{{`DEV_DB_PASS` | env}}@dev-cluster:5432/users_api_dev?sslmode=require\"\n      schema_config:\n        # Only capture relevant schemas\n        include_schemas: [\"public\", \"user_data\", \"api_metadata\"]\n\n    # Raw production data storage\n    storage_profile: prod_secure_s3\n\n    sanitize:\n      # Sanitized data for team sharing\n      storage_profile: team_shared_s3\n      override_query: |\n        -- GDPR/CCPA compliant sanitization\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          first_name = 'Test',\n          last_name = 'User' || id,\n          phone = '555-0000',\n          ssn = NULL,\n          date_of_birth = '1990-01-01',\n          address = '123 Test Street',\n          city = 'Test City',\n          postal_code = '12345';\n\n        -- Remove sensitive session and audit data  \n        DELETE FROM user_sessions;\n        DELETE FROM password_reset_tokens;\n        DELETE FROM audit_logs WHERE created_at &lt; NOW() - INTERVAL '30 days';\n\n        -- Sanitize payment information\n        UPDATE payment_methods SET \n          card_number = '4111111111111111',\n          cardholder_name = 'Test Cardholder',\n          billing_address = '123 Test Street';\n\n    # Group-based access control\n    sso_groups: [\"dbsnapper-developers\", \"dbsnapper-qa-engineers\"]\n\n  # Orders API - E-commerce database  \n  prod_orders_api:\n    cpus: 6\n    snapshot:\n      src_url: \"postgresql://dbsnapper_readonly:{{`PROD_DB_PASS` | env}}@prod-orders-cluster:5432/orders_api?sslmode=require\"\n      dst_url: \"postgresql://dev_user:{{`DEV_DB_PASS` | env}}@dev-cluster:5432/orders_api_dev?sslmode=require\"\n\n    storage_profile: prod_secure_s3\n    sanitize:\n      storage_profile: team_shared_s3\n      override_query: |\n        -- Order data sanitization\n        UPDATE orders SET \n          customer_email = CONCAT('customer', customer_id, '@example.com'),\n          shipping_address = '123 Test Street',\n          billing_address = '123 Test Street',\n          order_notes = 'Test order notes';\n\n        -- Remove financial data\n        UPDATE payments SET \n          payment_method_details = '{\"type\": \"test_card\"}',\n          transaction_id = 'test_' || id;\n\n    sso_groups: [\"dbsnapper-developers\", \"dbsnapper-qa-engineers\"]\n\n  # Analytics warehouse - Business intelligence data\n  prod_analytics_warehouse:\n    cpus: 12  # Large analytical database\n    snapshot:\n      src_url: \"postgresql://analytics_readonly:{{`ANALYTICS_DB_PASS` | env}}@analytics-cluster:5432/warehouse?sslmode=require\"\n      dst_url: \"postgresql://analytics_dev:{{`ANALYTICS_DEV_PASS` | env}}@analytics-dev:5432/warehouse_dev?sslmode=require\"\n      schema_config:\n        # Focus on business data, exclude operational schemas\n        include_schemas: [\"facts\", \"dimensions\", \"marts\"]\n        exclude_schemas: [\"staging\", \"temp\", \"etl_logs\"]\n\n    storage_profile: analytics_r2  # Use R2 for cost savings\n    sanitize:\n      storage_profile: analytics_r2\n      override_query: |\n        -- Preserve analytical utility while removing PII\n        UPDATE dim_customers SET \n          customer_name = 'Customer ' || customer_id,\n          email_domain = 'example.com',\n          phone_area_code = '555';\n\n        -- Keep business metrics, remove personal identifiers\n        UPDATE fact_orders SET \n          customer_ip = '127.0.0.1',\n          user_agent = 'Test Browser';\n\n    sso_groups: [\"dbsnapper-data-team\", \"dbsnapper-senior-devs\"]\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#validation-testing", "title": "\ud83d\udd0d Validation &amp; Testing", "text": "<p>Comprehensive setup validation:</p> <pre><code># Test all storage profiles\ndbsnapper config check --storage-profiles\n\n# Validate all target configurations  \ndbsnapper config validate --targets\n\n# Test SSO integration\ndbsnapper auth whoami\n\n# Create test snapshots\ndbsnapper build prod_users_api --dry-run\ndbsnapper build prod_orders_api --dry-run  \ndbsnapper build prod_analytics_warehouse --dry-run\n\n# Test sanitization\ndbsnapper sanitize prod_users_api --dry-run\n</code></pre> <p>Security Validation Checklist: - [ ] All source databases use read-only users - [ ] SSL/TLS enabled for all database connections - [ ] Storage profiles use appropriate access controls - [ ] Sanitization queries remove all PII - [ ] SSO groups properly restrict access - [ ] Audit logging captures all operations</p>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-2-developer-team-daily-development-workflow", "title": "Workflow 2: Developer Team - Daily Development Workflow", "text": ""}, {"location": "dbsnapper-cloud/team-workflows/#typical-developer-day-with-dbsnapper", "title": "\ud83d\udc69\u200d\ud83d\udcbb Typical Developer Day with DBSnapper", "text": "<p>Goal: Seamlessly integrate realistic database snapshots into daily development workflow.</p> Morning: Start DevelopmentMidday: Feature TestingAfternoon: Bug InvestigationEnd of Day: Clean Environment <pre><code># Check available database targets (filtered by SSO group membership)\ndbsnapper targets\n\n# Output shows targets you have access to:\n# prod_users_api (via dbsnapper-developers group)\n# prod_orders_api (via dbsnapper-developers group)\n\n# Load latest sanitized snapshot for feature development\ndbsnapper load prod_users_api\n\n# Verify database is ready\npsql postgresql://dev_user:pass@localhost:5432/users_api_dev -c \"\\dt\"\n\n# Start development with realistic data\nnpm run dev  # or your development command\n</code></pre> <pre><code># Working on orders feature - need orders database\ndbsnapper load prod_orders_api\n\n# Load specific snapshot if needed (0 = latest, 1 = previous, etc.)\ndbsnapper load prod_orders_api 1\n\n# Check target details and available snapshots\ndbsnapper target prod_orders_api\n\n# Run integration tests with realistic data\nnpm run test:integration\n</code></pre> <pre><code># QA reported bug - need to reproduce with exact data\n# Load the same snapshot QA is using\ndbsnapper load prod_users_api 0  # Latest snapshot\n\n# Reproduce bug with realistic data\nnpm run test:bug-reproduction\n\n# Debug with confidence - data matches production patterns\nnpm run debug:user-flow\n</code></pre> <pre><code># Optional: Pull snapshots locally for offline work\ndbsnapper pull prod_users_api\ndbsnapper pull prod_orders_api\n\n# Snapshots now available locally even without internet\nls ~/.dbsnapper/  # Shows downloaded snapshots\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#development-scenarios", "title": "\ud83d\udd04 Development Scenarios", "text": "New Feature DevelopmentBug Fix WorkflowPerformance Testing <p>Scenario: Building user profile enhancement feature</p> <pre><code># Start with fresh user data\ndbsnapper load prod_users_api\n\n# Develop feature against realistic user profiles\n# - Real email patterns (sanitized to @example.com)\n# - Authentic user behavior data\n# - Production-like data volumes\n\n# Test edge cases that only appear in production data\nnpm run test:edge-cases\n</code></pre> <p>Scenario: Fixing payment processing bug reported in production</p> <pre><code># Load orders database with payment data\ndbsnapper load prod_orders_api\n\n# Bug likely involves specific data patterns\npsql postgresql://dev_user:pass@localhost:5432/orders_api_dev -c \"\n  SELECT payment_status, COUNT(*) \n  FROM orders \n  WHERE created_at &gt; NOW() - INTERVAL '7 days' \n  GROUP BY payment_status;\"\n\n# Fix code with confidence using production-like data\n# Test fix against realistic data patterns\nnpm run test:payment-processing\n</code></pre> <p>Scenario: Optimizing database queries for production load</p> <pre><code># Load large dataset for performance testing\ndbsnapper load prod_analytics_warehouse\n\n# Run performance tests against realistic data volumes\nnpm run test:performance\n\n# Profile queries with production-like data distribution\n# Identify bottlenecks that only appear with real data patterns\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-3-qa-team-testing-quality-assurance", "title": "Workflow 3: QA Team - Testing &amp; Quality Assurance", "text": ""}, {"location": "dbsnapper-cloud/team-workflows/#qa-testing-workflows", "title": "\ud83e\uddea QA Testing Workflows", "text": "<p>Goal: Ensure consistent, reproducible testing with realistic data across all QA scenarios.</p> Test Environment SetupBug Reproduction WorkflowRegression Testing <p>Daily QA environment refresh:</p> <pre><code>#!/bin/bash\n# qa-daily-refresh.sh - Run via cron or CI/CD\n\n# Refresh all test databases with latest sanitized snapshots\ndbsnapper load prod_users_api\ndbsnapper load prod_orders_api\n\n# Verify data integrity\ndbsnapper config check --target prod_users_api\ndbsnapper config check --target prod_orders_api\n\n# Run smoke tests to verify environment\nnpm run test:smoke\n\necho \"QA environment refreshed with latest sanitized data\"\n</code></pre> <p>Scenario: Reproducing production bug report</p> <pre><code># Bug report: \"User profile update fails for users with special characters\"\n\n# Load latest user data\ndbsnapper load prod_users_api\n\n# Find users with special characters (sanitized data preserves patterns)\npsql postgresql://dev_user:pass@localhost:5432/users_api_dev -c \"\n  SELECT id, first_name, last_name \n  FROM users \n  WHERE first_name ~ '[^a-zA-Z ]' \n  OR last_name ~ '[^a-zA-Z ]' \n  LIMIT 10;\"\n\n# Test with specific user data patterns that trigger the bug\nnpm run test:user-profile-update\n\n# Document reproduction steps with realistic test data\n</code></pre> <p>Comprehensive regression test suite:</p> <pre><code># Load consistent snapshot for regression tests\ndbsnapper load prod_users_api 0  # Always use latest\ndbsnapper load prod_orders_api 0\n\n# Run comprehensive test suite\nnpm run test:regression\n\n# Tests run against:\n# - Realistic user behavior patterns\n# - Production-like data volumes  \n# - Authentic data relationships\n# - Real-world edge cases\n\n# Results are reproducible across team members\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#qa-test-scenarios", "title": "\ud83c\udfaf QA Test Scenarios", "text": "User Journey TestingLoad Testing with Realistic DataSecurity Testing <p>End-to-end user flow validation:</p> <pre><code>// Integration test with realistic data\ndescribe('User Registration to Purchase Flow', () =&gt; {\n  beforeEach(async () =&gt; {\n    // Fresh sanitized data for each test\n    await loadSnapshot('prod_users_api');\n    await loadSnapshot('prod_orders_api');\n  });\n\n  it('handles complete user journey', async () =&gt; {\n    // Test against realistic user data patterns\n    // - Authentic email patterns (sanitized)\n    // - Real-world name variations\n    // - Production-like user behavior data\n\n    const result = await testUserJourney();\n    expect(result).toBeSuccessful();\n  });\n});\n</code></pre> <p>Performance testing with production data patterns:</p> <pre><code># Load large dataset for performance testing\ndbsnapper load prod_analytics_warehouse\n\n# Run load tests with realistic query patterns\nnpm run test:load-testing\n\n# Test scenarios:\n# - High user concurrency patterns from production\n# - Complex analytical queries with real data distributions\n# - Edge cases that only appear with production data volumes\n</code></pre> <p>Validate sanitization and security measures:</p> <pre><code># Verify no PII in sanitized snapshots\nnpm run test:pii-detection\n\n# Test access controls\nnpm run test:authorization\n\n# Validate sanitization effectiveness\npsql postgresql://dev_user:pass@localhost:5432/users_api_dev -c \"\n  -- Should find no real email addresses\n  SELECT COUNT(*) FROM users WHERE email NOT LIKE '%@example.com';\n\n  -- Should find no real SSNs\n  SELECT COUNT(*) FROM users WHERE ssn IS NOT NULL;\n\n  -- Should find no real phone numbers except test pattern\n  SELECT COUNT(*) FROM users WHERE phone != '555-0000';\"\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-4-devops-team-production-integration-automation", "title": "Workflow 4: DevOps Team - Production Integration &amp; Automation", "text": ""}, {"location": "dbsnapper-cloud/team-workflows/#cicd-pipeline-integration", "title": "\ud83d\udd27 CI/CD Pipeline Integration", "text": "<p>Goal: Seamlessly integrate database snapshots into automated development pipelines.</p> GitHub Actions IntegrationKubernetes CronJob for Automated RefreshesTerraform Infrastructure Management <pre><code># .github/workflows/integration-tests.yml\nname: Integration Tests with DBSnapper\n\non: [push, pull_request]\n\njobs:\n  integration-tests:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgres:16\n        env:\n          POSTGRES_PASSWORD: postgres\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install DBSnapper Agent\n        uses: dbsnapper/install-dbsnapper-agent-action@v1\n        with:\n          version: latest\n\n      - name: Configure DBSnapper\n        env:\n          DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n          DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n        run: |\n          # Cloud targets automatically available\n          dbsnapper targets\n\n      - name: Load Test Database\n        run: |\n          # Load latest sanitized snapshot\n          dbsnapper load prod_users_api\n\n          # Verify data loaded correctly\n          dbsnapper config check --target prod_users_api\n\n      - name: Run Integration Tests  \n        run: |\n          # Tests run against realistic production data\n          npm run test:integration\n\n      - name: Run Performance Tests\n        if: github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'\n        run: |\n          # Load analytics database for performance testing\n          dbsnapper load prod_analytics_warehouse\n          npm run test:performance\n</code></pre> <pre><code># k8s/dbsnapper-refresh-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: dbsnapper-qa-refresh\nspec:\n  schedule: \"0 6 * * *\"  # Daily at 6 AM\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: dbsnapper-refresh\n            image: ghcr.io/dbsnapper/dbsnapper:latest\n            env:\n            - name: DBSNAPPER_AUTHTOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: dbsnapper-secrets\n                  key: authtoken\n            - name: DBSNAPPER_SECRET_KEY  \n              valueFrom:\n                secretKeyRef:\n                  name: dbsnapper-secrets\n                  key: secret-key\n            command:\n            - /bin/sh\n            - -c\n            - |\n              # Refresh QA environment with latest sanitized data\n              dbsnapper load prod_users_api\n              dbsnapper load prod_orders_api\n\n              # Verify data integrity\n              dbsnapper config check --target prod_users_api\n              dbsnapper config check --target prod_orders_api\n\n              # Notify team of successful refresh\n              echo \"QA databases refreshed successfully\"\n          restartPolicy: OnFailure\n</code></pre> <pre><code># terraform/dbsnapper.tf\nterraform {\n  required_providers {\n    dbsnapper = {\n      source = \"dbsnapper/dbsnapper\"\n      version = \"~&gt; 1.0\"\n    }\n  }\n}\n\n# Storage profiles\nresource \"dbsnapper_storage_profile\" \"prod_s3\" {\n  name     = \"prod-secure-s3\"\n  provider = \"s3\"\n  region   = \"us-west-2\"\n  bucket   = \"company-prod-db-snapshots\"\n  awscli_profile = \"dbsnapper-prod\"\n}\n\nresource \"dbsnapper_storage_profile\" \"team_s3\" {\n  name     = \"team-shared-s3\" \n  provider = \"s3\"\n  region   = \"us-west-2\"\n  bucket   = \"company-team-db-snapshots\"\n  awscli_profile = \"dbsnapper-team\"\n}\n\n# Database targets\nresource \"dbsnapper_target\" \"prod_users_api\" {\n  name = \"prod-users-api\"\n\n  snapshot_config {\n    src_url = var.prod_users_db_url\n    dst_url = var.dev_users_db_url\n    cpus = 8\n    schema_config {\n      include_schemas = [\"public\", \"user_data\"]\n    }\n  }\n\n  storage_profile = dbsnapper_storage_profile.prod_s3.name\n\n  sanitization {\n    storage_profile = dbsnapper_storage_profile.team_s3.name\n    override_query = file(\"${path.module}/sanitization/users_api.sql\")\n  }\n\n  sso_groups = [\"dbsnapper-developers\", \"dbsnapper-qa-engineers\"]\n}\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#monitoring-observability", "title": "\ud83d\udcca Monitoring &amp; Observability", "text": "Snapshot Creation MonitoringUsage Analytics Dashboard <pre><code>#!/bin/bash\n# monitoring/snapshot-health-check.sh\n\n# Monitor snapshot creation success rates\nTARGETS=(\"prod_users_api\" \"prod_orders_api\" \"prod_analytics_warehouse\")\n\nfor target in \"${TARGETS[@]}\"; do\n  echo \"Checking target: $target\"\n\n  # Test target connectivity\n  if ! dbsnapper config check --target \"$target\"; then\n    echo \"ERROR: Target $target connectivity failed\"\n    # Send alert to monitoring system\n    curl -X POST \"$MONITORING_WEBHOOK\" \\\n      -H \"Content-Type: application/json\" \\\n      -d \"{\\\"alert\\\": \\\"DBSnapper target $target connectivity failed\\\"}\"\n  fi\n\n  # Check recent snapshot availability\n  SNAPSHOT_COUNT=$(dbsnapper target \"$target\" --json | jq '.snapshots | length')\n  if [ \"$SNAPSHOT_COUNT\" -lt 1 ]; then\n    echo \"WARNING: No recent snapshots for $target\"\n  fi\ndone\n</code></pre> <pre><code>-- monitoring/usage_analytics.sql\n-- Query DBSnapper Cloud API or audit logs\n\n-- Snapshot creation frequency by target\nSELECT \n  target_name,\n  DATE(created_at) as date,\n  COUNT(*) as snapshots_created,\n  AVG(size_mb) as avg_size_mb,\n  AVG(duration_seconds) as avg_duration\nFROM snapshots \nWHERE created_at &gt;= NOW() - INTERVAL '30 days'\nGROUP BY target_name, DATE(created_at)\nORDER BY date DESC;\n\n-- User access patterns by SSO group\nSELECT \n  sso_group,\n  COUNT(DISTINCT user_id) as unique_users,\n  COUNT(*) as total_accesses,\n  COUNT(DISTINCT target_name) as targets_accessed\nFROM access_logs \nWHERE created_at &gt;= NOW() - INTERVAL '7 days'\nGROUP BY sso_group;\n\n-- Storage utilization by profile\nSELECT \n  storage_profile_name,\n  SUM(size_mb) as total_size_mb,\n  COUNT(*) as total_snapshots,\n  AVG(size_mb) as avg_snapshot_size\nFROM snapshots s\nJOIN targets t ON s.target_id = t.id\nGROUP BY storage_profile_name;\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-5-compliance-team-audit-security", "title": "Workflow 5: Compliance Team - Audit &amp; Security", "text": ""}, {"location": "dbsnapper-cloud/team-workflows/#compliance-audit-workflows", "title": "\ud83d\udd0d Compliance &amp; Audit Workflows", "text": "<p>Goal: Maintain comprehensive audit trails and compliance evidence for database snapshot access and sanitization.</p> Quarterly Access ReviewPII Sanitization ValidationAudit Trail Generation <pre><code>#!/bin/bash\n# compliance/quarterly-access-review.sh\n\necho \"=== DBSnapper Quarterly Access Review ===\"\necho \"Report Period: $(date -d '3 months ago' '+%Y-%m-%d') to $(date '+%Y-%m-%d')\"\necho\n\n# Review all users with database access\necho \"Active Users by SSO Group:\"\ndbsnapper auth groups --json | jq -r '\n  .groups[] | \n  \"Group: \\(.name) - Users: \\(.user_count) - Targets: \\(.target_count)\"'\n\necho\n\n# Review target access patterns\necho \"Target Access Summary:\"\ndbsnapper targets --json | jq -r '\n  .targets[] | \n  \"Target: \\(.name) - SSO Groups: \\(.sso_groups | join(\", \")) - Last Snapshot: \\(.last_snapshot_date)\"'\n\necho\n\n# Check sanitization compliance\necho \"Sanitization Compliance Check:\"\nfor target in $(dbsnapper targets --json | jq -r '.targets[].name'); do\n  echo \"Checking sanitization for: $target\"\n\n  # Verify sanitization query exists and is appropriate\n  if dbsnapper target \"$target\" --json | jq -e '.sanitize.override_query' &gt; /dev/null; then\n    echo \"  \u2705 Sanitization configured\"\n  else\n    echo \"  \u26a0\ufe0f  No sanitization configured - review required\"\n  fi\ndone\n</code></pre> <pre><code>#!/bin/bash\n# compliance/pii-validation.sh\n\n# Validate that sanitization removes all PII\nTARGETS=(\"prod_users_api\" \"prod_orders_api\")\n\nfor target in \"${TARGETS[@]}\"; do\n  echo \"Validating PII sanitization for: $target\"\n\n  # Load sanitized snapshot\n  dbsnapper load \"$target\"\n\n  # Check for PII patterns in sanitized data\n  echo \"  Checking email sanitization...\"\n  EMAIL_COUNT=$(psql \"$DST_DB_URL\" -t -c \"\n    SELECT COUNT(*) FROM users \n    WHERE email NOT LIKE '%@example.com' \n    AND email NOT LIKE '%@test.com';\")\n\n  if [ \"$EMAIL_COUNT\" -gt 0 ]; then\n    echo \"  \u274c Found $EMAIL_COUNT unsanitized emails\"\n  else\n    echo \"  \u2705 All emails properly sanitized\"\n  fi\n\n  echo \"  Checking SSN sanitization...\"\n  SSN_COUNT=$(psql \"$DST_DB_URL\" -t -c \"\n    SELECT COUNT(*) FROM users WHERE ssn IS NOT NULL;\")\n\n  if [ \"$SSN_COUNT\" -gt 0 ]; then\n    echo \"  \u274c Found $SSN_COUNT unsanitized SSNs\"\n  else\n    echo \"  \u2705 All SSNs properly sanitized\"\n  fi\n\n  echo \"  Checking phone number sanitization...\"\n  PHONE_COUNT=$(psql \"$DST_DB_URL\" -t -c \"\n    SELECT COUNT(*) FROM users \n    WHERE phone != '555-0000' \n    AND phone IS NOT NULL;\")\n\n  if [ \"$PHONE_COUNT\" -gt 0 ]; then\n    echo \"  \u274c Found $PHONE_COUNT unsanitized phone numbers\"  \n  else\n    echo \"  \u2705 All phone numbers properly sanitized\"\n  fi\ndone\n</code></pre> <pre><code># compliance/generate_audit_report.py\nimport json\nimport subprocess\nfrom datetime import datetime, timedelta\n\ndef generate_compliance_report():\n    \"\"\"Generate comprehensive compliance audit report\"\"\"\n\n    report = {\n        \"report_date\": datetime.now().isoformat(),\n        \"report_period\": {\n            \"start\": (datetime.now() - timedelta(days=90)).isoformat(),\n            \"end\": datetime.now().isoformat()\n        },\n        \"sections\": {}\n    }\n\n    # User access audit\n    print(\"Generating user access audit...\")\n    users_result = subprocess.run([\n        'dbsnapper', 'auth', 'users', '--json'\n    ], capture_output=True, text=True)\n\n    if users_result.returncode == 0:\n        report[\"sections\"][\"user_access\"] = json.loads(users_result.stdout)\n\n    # Target configuration audit\n    print(\"Generating target configuration audit...\")\n    targets_result = subprocess.run([\n        'dbsnapper', 'targets', '--json'\n    ], capture_output=True, text=True)\n\n    if targets_result.returncode == 0:\n        targets = json.loads(targets_result.stdout)\n        report[\"sections\"][\"targets\"] = targets\n\n        # Analyze sanitization coverage\n        sanitized_targets = []\n        unsanitized_targets = []\n\n        for target in targets.get('targets', []):\n            if target.get('sanitize', {}).get('override_query'):\n                sanitized_targets.append(target['name'])\n            else:\n                unsanitized_targets.append(target['name'])\n\n        report[\"sections\"][\"sanitization_analysis\"] = {\n            \"sanitized_targets\": sanitized_targets,\n            \"unsanitized_targets\": unsanitized_targets,\n            \"coverage_percentage\": len(sanitized_targets) / len(targets['targets']) * 100\n        }\n\n    # SSO configuration audit\n    print(\"Generating SSO configuration audit...\")\n    sso_result = subprocess.run([\n        'dbsnapper', 'auth', 'sso-status', '--json'\n    ], capture_output=True, text=True)\n\n    if sso_result.returncode == 0:\n        report[\"sections\"][\"sso_configuration\"] = json.loads(sso_result.stdout)\n\n    # Save report\n    report_filename = f\"dbsnapper_compliance_report_{datetime.now().strftime('%Y%m%d')}.json\"\n    with open(report_filename, 'w') as f:\n        json.dump(report, f, indent=2)\n\n    print(f\"Compliance report saved to: {report_filename}\")\n    return report\n\nif __name__ == \"__main__\":\n    generate_compliance_report()\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#best-practices-troubleshooting", "title": "Best Practices &amp; Troubleshooting", "text": ""}, {"location": "dbsnapper-cloud/team-workflows/#workflow-optimization", "title": "\ud83c\udfaf Workflow Optimization", "text": "Performance Best PracticesSecurity Best PracticesTeam Communication <p>Optimize snapshot operations for team workflows:</p> <pre><code># High-performance configuration for team environments\ndefaults:\n  cpus: 4  # Baseline performance for all operations\n\ntargets:\n  # Large production databases - use more resources\n  prod_analytics_warehouse:\n    cpus: 12\n    snapshot:\n      schema_config:\n        # Reduce snapshot size with targeted schemas\n        include_schemas: [\"marts\", \"dimensions\"]\n        exclude_schemas: [\"staging\", \"temp\", \"logs\"]\n\n  # Frequent development use - optimize for speed\n  prod_users_api:\n    cpus: 6\n    storage_profile: high_performance_s3  # NVMe storage class\n</code></pre> <p>Maintain security across all workflows:</p> <pre><code># Regular security validation script\n#!/bin/bash\n# security/validate-security.sh\n\necho \"=== DBSnapper Security Validation ===\"\n\n# Check all connections use SSL/TLS\ndbsnapper targets --json | jq -r '.targets[].snapshot.src_url' | while read url; do\n  if [[ ! \"$url\" =~ sslmode=require ]]; then\n    echo \"WARNING: $url does not enforce SSL\"\n  fi\ndone\n\n# Verify read-only users\necho \"Verify using read-only database users...\"\n\n# Check sanitization on all shared targets\necho \"Validating sanitization configuration...\"\ndbsnapper targets --json | jq -r '.targets[] | select(.sso_groups | length &gt; 0) | .name' | while read target; do\n  if ! dbsnapper target \"$target\" --json | jq -e '.sanitize.override_query' &gt; /dev/null; then\n    echo \"ERROR: Shared target $target has no sanitization configured\"\n  fi\ndone\n</code></pre> <p>Effective communication strategies:</p> <pre><code># Team Communication Template\n\n## Weekly DBSnapper Status Update\n\n### \ud83d\udcca Usage Statistics\n- Snapshots created: [X] this week\n- Active developers: [X] \n- QA test runs: [X]\n- Storage usage: [X] GB\n\n### \ud83c\udfaf New Features Available\n- [Feature]: Available for [teams]\n- [Database]: New target available for [use case]\n\n### \ud83d\udccb Reminders\n- Remember to load fresh snapshots for integration testing\n- Report any issues with snapshot quality\n- New team members: Complete SSO setup first\n\n### \ud83d\udd27 Maintenance Windows\n- [Date/Time]: Snapshot refresh (minimal downtime)\n- [Date/Time]: Storage migration (plan accordingly)\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#troubleshooting-common-issues", "title": "\ud83d\udea8 Troubleshooting Common Issues", "text": "Workflow Disruptions <p>Quick resolution for common workflow issues:</p> <pre><code># Troubleshooting toolkit\n#!/bin/bash\n# troubleshooting/workflow-debug.sh\n\necho \"=== DBSnapper Workflow Diagnostics ===\"\n\n# Check authentication\necho \"1. Checking authentication...\"\nif ! dbsnapper auth whoami; then\n  echo \"\u274c Authentication failed - check SSO configuration\"\n  exit 1\nfi\n\n# Check target accessibility\necho \"2. Checking target access...\"\ndbsnapper targets &gt; /dev/null\nif [ $? -eq 0 ]; then\n  echo \"\u2705 Targets accessible\"\nelse\n  echo \"\u274c Cannot access targets - check SSO group membership\"\nfi\n\n# Check storage connectivity\necho \"3. Checking storage connectivity...\"\nif ! dbsnapper config check --storage-profiles; then\n  echo \"\u274c Storage connectivity issues - check AWS credentials\"\nfi\n\n# Check recent snapshot availability\necho \"4. Checking snapshot freshness...\"\nTARGETS=$(dbsnapper targets --json | jq -r '.targets[].name')\nfor target in $TARGETS; do\n  LAST_SNAPSHOT=$(dbsnapper target \"$target\" --json | jq -r '.last_snapshot_date')\n  if [ \"$LAST_SNAPSHOT\" = \"null\" ]; then\n    echo \"\u26a0\ufe0f  No snapshots available for $target\"\n  else\n    echo \"\u2705 $target: Last snapshot $LAST_SNAPSHOT\"\n  fi\ndone\n</code></pre>"}, {"location": "dbsnapper-cloud/team-workflows/#summary-enterprise-ready-database-snapshot-workflows", "title": "Summary: Enterprise-Ready Database Snapshot Workflows", "text": "<p>\ud83c\udf89 You now have complete, production-ready workflows that transform your team's approach to development data:</p>"}, {"location": "dbsnapper-cloud/team-workflows/#workflow-benefits-achieved", "title": "\u2705 Workflow Benefits Achieved", "text": "<ul> <li>\ud83c\udfd7\ufe0f DevOps Efficiency - Infrastructure as Code setup with comprehensive security</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Developer Productivity - Seamless daily workflows with realistic data</li> <li>\ud83e\uddea QA Reliability - Consistent, reproducible testing with production-like data</li> <li>\ud83d\udd27 Automation Integration - CI/CD pipelines with automated database provisioning</li> <li>\ud83d\udd0d Compliance Readiness - Complete audit trails and security validation</li> </ul>"}, {"location": "dbsnapper-cloud/team-workflows/#next-steps", "title": "\ud83d\ude80 Next Steps", "text": "<ol> <li>Choose your starting workflow based on your team's immediate needs</li> <li>Implement incrementally - start with one workflow and expand</li> <li>Monitor and optimize using the provided monitoring scripts</li> <li>Scale gradually as your team adopts the new processes</li> </ol> <p>Questions or need help implementing these workflows?  - Review the targets configuration guide for detailed setup - Check the SSO integration guide for team authentication - Explore storage profiles for cloud storage configuration</p> <p>Your team is now equipped with enterprise-grade database snapshot workflows that scale from individual development to organization-wide collaboration.</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/", "title": "Okta SSO Integration: Complete Setup Guide", "text": "<p>Transform your team's database snapshot access with secure, automated Okta integration. This guide walks you through connecting your Okta organization with DBSnapper Cloud to enable automatic team provisioning, group-based access control, and enterprise-grade audit capabilities.</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#integration-overview", "title": "Integration Overview", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#enterprise-grade-security-features", "title": "\ud83d\udee1\ufe0f Enterprise-Grade Security Features", "text": "<p>Fully Supported Okta Capabilities: - Service Provider (SP) Initiated Authentication - Users authenticate via DBSnapper Cloud - OpenID Connect (OIDC) - Modern, secure authentication protocol - Proof Key for Code Exchange (PKCE) - Enhanced security for authentication flows - Just-in-Time (JIT) Provisioning - Automatic account creation on first login - Group Membership Sync - Real-time access control based on Okta groups - Multi-Factor Authentication (MFA) - Inherited from your Okta security policies</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#what-youll-achieve", "title": "\ud83c\udfaf What You'll Achieve", "text": "<p>After completing this setup: - New team members get database access automatically when added to Okta groups - Existing team members log in with their corporate credentials - Access control is managed through your existing Okta group structure - Departing employees lose access immediately when deactivated in Okta - Audit compliance is automatically maintained across both systems</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#prerequisites-planning", "title": "Prerequisites &amp; Planning", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#before-you-begin", "title": "\ud83d\udccb Before You Begin", "text": "<p>Required Access: - Okta Administrator privileges in your organization - DBSnapper Cloud Administrator access for your organization - IT Security Review (recommended for production deployments)</p> <p>Planning Checklist: - [ ] Identify which Okta groups should access DBSnapper - [ ] Define access levels (developers, QA, senior engineers, etc.) - [ ] Review your organization's SSO security policies - [ ] Plan user communication for the SSO rollout</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#group-mapping-strategy", "title": "\ud83c\udfaf Group Mapping Strategy", "text": "<p>Plan your Okta group to DBSnapper access mapping:</p> <pre><code>graph TB\n    A[Okta Groups] --&gt; B[DBSnapper Target Access]\n\n    A1[backend-developers] --&gt; B1[Staging &amp; Development DBs]\n    A2[senior-engineers] --&gt; B2[Production DB Access]  \n    A3[qa-engineers] --&gt; B3[Test Data &amp; Sanitized Snapshots]\n    A4[data-analysts] --&gt; B4[Analytics &amp; Reporting DBs]</code></pre>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-1-configure-dbsnapper-application-in-okta", "title": "Step 1: Configure DBSnapper Application in Okta", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#11-add-dbsnapper-from-the-okta-integration-network", "title": "1.1 Add DBSnapper from the Okta Integration Network", "text": "<ol> <li>Sign into your Okta Admin Console</li> <li>Navigate to Applications \u2192 Browse App Catalog</li> <li>Search for \"DBSnapper\" in the catalog</li> </ol> <ol> <li>Click \"Add Integration\" for the DBSnapper application</li> </ol>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#12-configure-application-settings", "title": "1.2 Configure Application Settings", "text": "<p>Basic Application Configuration:</p> <ol> <li>Application Name - Leave as \"DBSnapper\" or customize for your organization</li> <li>Application Logo - DBSnapper logo is included automatically</li> <li>Application Visibility - Configure based on your rollout strategy</li> </ol> <p>Important Security Settings:</p> <ul> <li>Sign-On Policy - Configure according to your organization's security requirements</li> <li>MFA Requirements - Inherit from your organization's global MFA policies  </li> <li>Session Policies - Use your existing Okta session management rules</li> </ul>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#13-collect-integration-credentials", "title": "1.3 Collect Integration Credentials", "text": "<p>Record these values for DBSnapper Cloud configuration:</p> \ud83d\udccb Required Information\ud83d\udd12 Security Notes Client ID Public identifier for the DBSnapper application in your Okta org Example: <code>0oag95e2z5BhLZ5AI5d7</code> Client Secret Private secret for secure authentication (click \"Reveal\" to see) Example: <code>abcdef123456...</code> (keep this secure!) Okta Domain Your organization's Okta URL Example: <code>https://yourcompany.okta.com</code> <p>Client Secret Security: - Store the client secret securely (password manager recommended) - Only share with authorized DBSnapper administrators - Rotate regularly according to your security policies - Never commit to source control or share via insecure channels</p> <p>Access Validation: - Test the integration with a small group first - Verify group membership sync works correctly - Confirm MFA requirements are enforced</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-2-configure-dbsnapper-cloud-sso-integration", "title": "Step 2: Configure DBSnapper Cloud SSO Integration", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#21-access-sso-configuration", "title": "2.1 Access SSO Configuration", "text": "<ol> <li>Sign into DBSnapper Cloud with your administrator account</li> <li>Navigate to Settings \u2192 SSO from the main menu</li> <li>Click \"Add SSO Tenant\" to begin configuration</li> </ol>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#22-configure-sso-tenant-settings", "title": "2.2 Configure SSO Tenant Settings", "text": "<p>Enter the Okta integration details you collected from Step 1:</p> <p></p> \ud83d\udd27 Configuration Fields\ud83d\udd0d Validation Steps Provider Select \"Okta - OIDC\" from the dropdown Client ID Enter the Client ID from your Okta DBSnapper application Example: <code>0oag95e2z5BhLZ5AI5d7</code> Client Secret Enter the Client Secret from your Okta DBSnapper application   This will be encrypted and stored securely Issuer URL Your Okta domain URL Example: <code>https://yourcompany.okta.com</code> Organization Domain Your company's email domain (pre-filled) Example: <code>yourcompany.com</code> Must match your users' email domains for automatic user mapping <p>Before clicking Save: - [ ] Client ID matches exactly from Okta - [ ] Client Secret was copied correctly (no extra spaces) - [ ] Issuer URL is your exact Okta domain - [ ] Organization Domain matches your team's email addresses - [ ] All fields are filled in completely</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#23-complete-integration-setup", "title": "2.3 Complete Integration Setup", "text": "<ol> <li>Click \"Save\" to create the SSO tenant</li> <li>Verify the configuration appears correctly</li> </ol> <p>Configuration Complete! Your Okta SSO integration is now active.</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-3-configure-group-based-access-control", "title": "Step 3: Configure Group-Based Access Control", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#31-set-up-okta-groups", "title": "3.1 Set Up Okta Groups", "text": "<p>In your Okta Admin Console:</p> <ol> <li>Navigate to Directory \u2192 Groups</li> <li>Create or identify groups for DBSnapper access:</li> </ol> <pre><code>Recommended Group Structure:\n\u251c\u2500\u2500 dbsnapper-developers        # Basic development database access\n\u251c\u2500\u2500 dbsnapper-senior-engineers  # Production database access  \n\u251c\u2500\u2500 dbsnapper-qa-engineers      # Testing and QA database access\n\u251c\u2500\u2500 dbsnapper-data-analysts     # Analytics and reporting access\n\u2514\u2500\u2500 dbsnapper-admins           # Full administrative access\n</code></pre> <ol> <li>Add users to appropriate groups based on their roles and access needs</li> </ol>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#32-configure-target-access-in-dbsnapper", "title": "3.2 Configure Target Access in DBSnapper", "text": "<p>Map your Okta groups to database targets:</p> <pre><code># Example target configuration with group-based access\ntargets:\n  production_api:\n    snapshot:\n      src_url: \"postgresql://readonly@prod-cluster:5432/api\"\n    storage_profile: prod_s3\n    sanitize:\n      storage_profile: sanitized_s3  \n      override_query: |\n        UPDATE users SET \n          email = CONCAT('user', id, '@example.com'),\n          ssn = NULL;\n    # Only these Okta groups can access sanitized snapshots  \n    sso_groups: [\"dbsnapper-developers\", \"dbsnapper-qa-engineers\"]\n\n  sensitive_financial:\n    # More restrictive access for sensitive data\n    sso_groups: [\"dbsnapper-senior-engineers\", \"dbsnapper-data-analysts\"]\n\n  reference_data:  \n    # Broadly accessible reference data\n    sso_groups: [\"dbsnapper-developers\", \"dbsnapper-qa-engineers\", \"dbsnapper-data-analysts\"]\n</code></pre>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-4-test-validate-integration", "title": "Step 4: Test &amp; Validate Integration", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#41-test-authentication-flow", "title": "4.1 Test Authentication Flow", "text": "<p>Verify the complete authentication process:</p> <ol> <li>Visit DBSnapper Cloud sign-in page: app.dbsnapper.com/sign_in</li> <li>Enter your corporate email address (must match Organization Domain)</li> <li>Click \"Next\" </li> <li>Click \"Sign in with SSO\" button</li> <li>You should be redirected to Okta for authentication</li> <li>Complete Okta authentication (including MFA if required)</li> <li>You should be redirected back to DBSnapper Cloud and automatically logged in</li> </ol>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#42-verify-group-access", "title": "4.2 Verify Group Access", "text": "<p>Test that group membership controls access correctly:</p> \ud83d\udc68\u200d\ud83d\udcbb Developer Test\ud83d\udc68\u200d\ud83d\udcbc Admin Test\ud83d\udeab Access Restriction Test <pre><code># User in \"dbsnapper-developers\" group should see:\ndbsnapper targets\n# Output should show targets with sso_groups: [\"dbsnapper-developers\"]\n\ndbsnapper load production_api\n# Should successfully load sanitized snapshot\n</code></pre> <pre><code># User in \"dbsnapper-admins\" group should see:\ndbsnapper targets  \n# Output should show all targets they have access to\n\n# Should be able to create new targets via DBSnapper Cloud interface\n</code></pre> <pre><code># User NOT in required SSO groups should:\ndbsnapper targets\n# NOT see restricted targets\n\ndbsnapper load sensitive_financial\n# Should receive \"Access Denied\" error\n</code></pre>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#43-monitor-integration-health", "title": "4.3 Monitor Integration Health", "text": "<p>Set up monitoring for ongoing operations:</p> <ul> <li>Okta Admin Console - Monitor DBSnapper application usage and authentication events</li> <li>DBSnapper Cloud Audit Logs - Track user access patterns and snapshot usage</li> <li>Group Membership Changes - Set up alerts for sensitive group modifications</li> <li>Failed Authentication Events - Monitor for potential security issues</li> </ul>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-5-team-rollout-training", "title": "Step 5: Team Rollout &amp; Training", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#51-pilot-deployment", "title": "5.1 Pilot Deployment", "text": "<p>Start with a small group for testing:</p> <ol> <li>Select 3-5 team members for initial testing</li> <li>Add them to appropriate Okta groups </li> <li>Walk them through the SSO login process</li> <li>Verify they can access expected database targets</li> <li>Collect feedback on the user experience</li> </ol>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#52-full-team-rollout", "title": "5.2 Full Team Rollout", "text": "<p>After successful pilot testing:</p> \ud83d\udce2 Communication Plan\ud83c\udfaf Success Metrics <p>Email template for team notification:</p> <pre><code>Subject: Database Snapshots - Now Available via SSO\n\nTeam,\n\nWe've enabled SSO access for database snapshots through DBSnapper Cloud.\n\nWhat's Changed:\n\u2022 Log in using your corporate credentials at app.dbsnapper.com\n\u2022 Access to database snapshots is now controlled by your team membership\n\u2022 No more separate passwords to manage\n\nGetting Started:\n1. Visit app.dbsnapper.com/sign_in\n2. Enter your @yourcompany.com email address  \n3. Click \"Sign in with SSO\"\n4. Use your existing corporate credentials\n\nQuestions? Contact: [your IT team]\n</code></pre> <p>Track rollout success: - [ ] All team members can authenticate successfully - [ ] Users can access expected database targets - [ ] No one has access to unauthorized targets - [ ] MFA requirements are enforced correctly - [ ] Audit logging captures all access events</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#troubleshooting-support", "title": "Troubleshooting &amp; Support", "text": ""}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#common-issues", "title": "\ud83d\udea8 Common Issues", "text": "Authentication FailuresAccess Control IssuesIntegration Health <p>\"Invalid client credentials\" error: - Verify Client ID and Client Secret are correct - Check that Client Secret doesn't have extra spaces - Confirm Okta application is active and assigned to users</p> <p>\"User not found\" error: - Verify Organization Domain matches user email domains - Check that user exists in Okta and has access to DBSnapper app - Confirm user is in at least one DBSnapper-related group</p> <p>User can't see expected targets: - Verify user is in the correct Okta groups - Check that target configuration includes user's groups in <code>sso_groups</code> - Confirm group names match exactly (case sensitive)</p> <p>User sees too many targets: - Review group membership in Okta - Check target configurations for overly broad <code>sso_groups</code> - Verify principle of least privilege is followed</p> <p>Group sync not working: - Check Okta group membership updates are propagating - Verify DBSnapper can read group information from Okta - Contact support if sync appears delayed or broken</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#validation-commands", "title": "\ud83d\udd27 Validation Commands", "text": "<pre><code># Test SSO authentication and group access\ndbsnapper auth whoami  # Shows current user and group memberships\n\n# List available targets (filtered by group membership)\ndbsnapper targets\n\n# Test target access\ndbsnapper config check --target your_target_name\n\n# Debug authentication issues\nDBSNAPPER_DEBUG=true dbsnapper auth whoami\n</code></pre>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#getting-help", "title": "\ud83d\udcde Getting Help", "text": "<p>Enterprise Support: - Email: enterprise@dbsnapper.com - Priority Support: Available for enterprise customers - Documentation: Additional guides at docs.dbsnapper.com</p> <p>Community Support: - GitHub Issues: github.com/dbsnapper/dbsnapper/issues - Community Forum: community.dbsnapper.com</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#summary-youre-ready-for-enterprise-scale-database-snapshots", "title": "Summary: You're Ready for Enterprise-Scale Database Snapshots", "text": "<p>\ud83c\udf89 Congratulations! You've successfully configured enterprise-grade database snapshot access with:</p> <ul> <li>\u2705 Secure SSO Authentication - Team members use corporate credentials</li> <li>\u2705 Automated Access Control - Group membership determines database access  </li> <li>\u2705 Compliance-Ready Auditing - Complete access trails for security reviews</li> <li>\u2705 Scalable Team Management - New hires get automatic access, departing employees lose access immediately</li> </ul> <p>Next Steps: - Configure additional database targets with appropriate group access - Set up cloud storage profiles for team snapshot sharing - Explore advanced sanitization for compliance requirements</p> <p>Your team now has secure, scalable access to realistic database snapshots for development, testing, and analysis\u2014all managed through your existing identity infrastructure.</p>"}, {"location": "sanitize/configuration/", "title": "Sanitize Configuration", "text": "<p>The DBSnapper Agent is configured using a YAML file, which is created when you run <code>dbsnapper config init</code> In this file you can specify multiple target configurations, each target being a set of options for a database you want to sanitize.</p> <p>Referring to our sample configuration file, the highlighted lines show the configuration options for a database sanitization:</p> <p><code>~/.config/dbsnapper/dbnsapper.yml</code> example</p> <pre><code>authtoken: 1234567890abcdef1234567890abcdef....\nworking_directory: /Users/snappy/.dbsnapper\ndocker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:16-alpine\nsecret_key: 1234567890abcdef1234567890abcdef\n# Target configurations\ntargets:\n  sakila:\n    name: sakila\n    # Snapshot configuration\n    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n    # Sanitization configuration\n    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: sakila-sanitize.sql\n    # Subsetting configuration\n    subset:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_subset?tls=false\n      subset_tables:\n        - table: sakila.film\n          where: \"film_id &lt; 20\"\n        - table: sakila.actor\n          percent: 20\n      copy_tables:\n        - sakila.store\n      excluded_tables:\n        - sakila.staff\n      added_relationships:\n        - fk_table: sakila.address\n          fk_columns: city_id\n          ref_table: sakila.city\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: sakila.store\n          ref_table: sakila.staff\n</code></pre>"}, {"location": "sanitize/configuration/#configuration-options", "title": "Configuration options", "text": "<p>Sanitization configuration options consist of a destination url <code>dst_url</code> that will be used to load a snapshot for sanitization and a query file <code>query_file</code> that contains the sanitization queries. When the sanitization is complete, a snapshot of the sanitized database will be created and stored in the working directory.</p> <p>Subset configuration options</p> <code>dst_url</code> <p>Connection string for the database where you will sanitize the snapshot (will be overwritten)</p> <code>query_file</code> <p>Name of the file containing the sanitization queries, located in your working directory (default: <code>~/.dbsnapper</code>)</p> <p>Danger: Destination database <code>dst_url</code> will be DROPPED and RECREATED</p> <p>Any connection string provided in the <code>dst_url</code> attribute will be overwritten when certain commands are used such as <code>load</code> and <code>sanitize</code> </p>"}, {"location": "sanitize/introduction/", "title": "Sanitization Introduction", "text": "<p>One of the main features of DBSnapper is the ability to sanitize a database. This allows you to remove sensitive information from your database, making it safe to use in development, testing, or analytics environments.</p>"}, {"location": "sanitize/introduction/#why-is-it-important", "title": "Why is it Important?", "text": "<ul> <li>Privacy Protection: In today's digital world, protecting personal information is not just ethical but also a legal requirement in many cases. Sanitization helps you respect and protect user privacy.</li> <li>Regulatory Compliance: Laws like GDPR and HIPAA require strict handling of personal data. Sanitization ensures that you stay on the right side of these regulations.</li> <li>Secure Development and Testing: Using real data in development and testing can be risky if it contains sensitive information. Sanitized data allows your team to work with realistic datasets without exposing personal information.</li> </ul>"}, {"location": "sanitize/introduction/#overview", "title": "Overview", "text": "<p>DBSnapper provides the <code>sanitize</code> command to sanitize a database, which applies a sanitization query to a snapshot of the database.</p>"}, {"location": "sanitize/introduction/#sanitize-command-steps", "title": "<code>sanitize</code> command steps", "text": "<ol> <li>Locate the desired snapshot in the working directory or pull it from the cloud (if configured)</li> <li>Load the snapshot into the <code>dst_url</code> database</li> <li>Apply the sanitization query to the database</li> <li>Create a new snapshot of the sanitized database and store it in the working directory and push it to the cloud (if configured)</li> </ol> <p>Note</p> <p>In the near future we will be adding more capabilities to the sanitization process to make it even more powerful and flexible. Stay tuned for updates!</p>"}, {"location": "sanitize/sanitize/", "title": "Sanitize a Database", "text": "<p>Given the example configuration for the <code>sakila</code> target, we can sanitize the datbase with the snapshot we created in the Snapshot Build and Load example.</p> <p>Target: sakila - sanitize configuration</p> <pre><code>    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: \"sakila.san.sql\"\n</code></pre> <p>Here is our snapshot from the prior example:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nTables in target:  sakila\n...\nListing ALL snapshots for target: sakila\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n| INDEX |         CREATED         |       NAME        |       FILENAME        |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n|     0 | 2024-Feb-17 @ 23:14:50Z | 1708236890_sakila | 1708236890_sakila.zip | 981 kB | false      |       |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n</code></pre> <p>Now we're ready to sanitize this snapshot of the <code>sakila</code> target which can be done with the <code>sanitize</code> command:</p> <pre><code>dbsnapper sanitize sakila\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha-dev Build Date: 2024-02-20T07:46:10-07:00\nDBSnapper Cloud: Enabled\n\nSTART: Sanitize Snapshot #0, Name: 1708236890_sakila\n-&gt; LOADING original Snapshot #0: Name: 1708236890_sakila, Snapshot File: 1708236890_sakila.zip, Dest DB URL: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n--&gt; Using engine: mysql-local\n--&gt; Using Target: sakila\n--&gt; Pulling to local file: /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Local snapshot already exists at /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Pulled snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Unzipping snapshot /Users/snappy/.dbsnapper/1708236890_sakila.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-1815162460\n--&gt; Dropping and recreating database myl://localhost:3306/sakila_sanitized\n-&gt; LOADING Snapshot Completed for Target: sakila\n--&gt; Executing sanitization query\n--&gt; Building sanitized snapshot\n--&gt; Zipping snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.san.zip\n--&gt; Sanitized snapshot '1708236890_sakila' created at /Users/snappy/.dbsnapper/1708236890_sakila.san.zip\nFINISHED Sanitizing snapshot\n</code></pre> <p>When this is finished you can list the <code>sakila</code> target snapshot:</p> <pre><code>dbsnapper target sakila\n</code></pre> <p>Output:</p> <pre><code>Listing ALL snapshots for target: sakila\n+-------+-------------------------+-------------------+-----------------------+--------+------------+---------------------------+\n| INDEX |         CREATED         |       NAME        |       FILENAME        |  SIZE  | SANITIZED? |           SANFN           |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+---------------------------+\n|     0 | 2024-Feb-17 @ 23:14:50Z | 1708236890_sakila | 1708236890_sakila.zip | 981 kB | true       | 1708236890_sakila.san.zip |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+---------------------------+\n</code></pre> <p>In this output, we now see that <code>SANITIZED?</code> column is <code>true</code> and the <code>SANFN</code> column shows the name of the sanitized snapshot file that is located in the working directory.</p>"}, {"location": "share/introduction/", "title": "Introduction", "text": "<p>One of the key DBSnapper features is the ability to easily share sanitized database snapshots with your team, developers, testers, and other stakeholders. Sharing snapshots is useful for providing sanitized data for development, testing, and other purposes without exposing sensitive information.</p> <p>By utilizing your own private cloud storage providers, you can securely store your snapshots in your own approved infrastructure and use the DBSnapper Agent and Cloud to facilitate easy sharing.</p>"}, {"location": "share/introduction/#sharing-architecture", "title": "Sharing Architecture", "text": "<p>The DBSnapper sharing architecture is designed to allow you to create snapshots of your database and share the unsanitized and sanitized snapshots in separate locations. You can then share the sanitized cloud storage location with your team and they can use the DBSnapper agent to access and load these snapshots.</p> <p> DBSnapper Architecture Overview </p>"}, {"location": "share/introduction/#getting-started", "title": "Getting Started", "text": "<p>To get started with using a cloud storage engine, you will need to configure a <code>storage_profile</code> in your DBSnapper configuration file. The <code>storage_profile</code> configuration specifies the cloud storage provider, credentials, and bucket information for the storage engine.</p> <p>AWS S3 configuration examples</p> <pre><code>storage_profiles:\n  s3-with-provided-credentials:\n    provider: s3\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    region: &lt;region&gt;\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  s3-from-awscli-shared-profile:\n    provider: s3\n    awscli_profile: dbsnapper_credentials\n    bucket: dbsnapper-test-s3\n    prefix:\n\ntargets:\n  # Share target\n  shared-s3:\n    name: shared-s3\n    share:\n      storage_profile: s3-from-awscli-shared-profile\n      dst_url: postgres://localhost:5432/dbsnapper_test\n</code></pre> <p>In the example above we have two <code>storage_profiles</code> configurations for AWS S3. The first configuration <code>s3-with-provided-credentials</code> explicitly specifies the access key, secret key, region, and bucket for an S3 storage engine. <code>s3-from-awscli-shared-profile</code>, on the other hand, indicates that we want to retrieve the credentials from the <code>dbsnapper_credentials</code> AWS shared configuration profile as specified in the <code>awscli_profile</code> field.</p> <p>We have also defined a <code>share target</code> configuration on line 19 that uses the <code>s3-from-awscli-shared-profile</code> storage profile to access shared snapshots from the <code>dbsnapper-test-s3</code> bucket. These shared snapshots will be loaded into the <code>dbsnapper_test</code> database as specified in the <code>dst_url</code> field on line 23.</p>"}, {"location": "snapshot/build_and_load/", "title": "Build and Load", "text": "<p>Given the example configuration for the <code>sakila</code> target, we can build a snapshot and load it to the destination database.</p> <p>Target: sakila - snapshot configuration</p> <pre><code>    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n</code></pre>"}, {"location": "snapshot/build_and_load/#build-a-snapshot", "title": "Build a snapshot", "text": "<p>Now we're ready to create our first snapshot of the <code>sakila</code> target which can be done with the <code>build</code> command:</p> <pre><code>dbsnapper build sakila\n</code></pre> <p>Output:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nSTART: Build Snapshot for target: sakila with engine: mysql-local\n--&gt; Local target, Local storage, non-localhost DB.\n--&gt; Zipping snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.zip\nFINISH: Building DB Snapshot for target: sakila\n</code></pre> <p>When this is finished you can list all snapshots for the <code>sakila</code> target with:</p> <pre><code>dbsnapper target sakila\n</code></pre> <p>Output:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nTables in target:  sakila\n...\nListing ALL snapshots for target: sakila\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n| INDEX |         CREATED         |       NAME        |       FILENAME        |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n|     0 | 2024-Feb-17 @ 23:14:50Z | 1708236890_sakila | 1708236890_sakila.zip | 981 kB | false      |       |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n</code></pre> <p>And we see our new snapshot <code>1708236890_sakila</code> listed on line 10.</p>"}, {"location": "snapshot/build_and_load/#load-a-snapshot", "title": "Load a snapshot", "text": "<p>If a <code>dst_url</code> is defined in the target snapshot definition, you can load a snapshot to the destination using the index on the snapshot list. The <code>load</code> command will drop and recreate the destination database and restore the snapshot to the destination.</p> <p>Use the index of the snapshot to specify which snapshot to load. The default is snapshot index 0.</p> <pre><code>dbsnapper load sakila 0\n</code></pre> <p>Output:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nSTART: Loading original Snapshot #0: Name: 1708236890_sakila, Snapshot File: 1708236890_sakila.zip, Dest DB URL: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n--&gt; Using engine: mysql-local\n--&gt; Using Target: sakila\n--&gt; Pulling to local file: /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Local snapshot already exists at /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Pulled snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Unzipping snapshot /Users/snappy/.dbsnapper/1708236890_sakila.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-2125850599\n--&gt; Dropping and recreating database myl://localhost:3306/sakila_snap\nFINISH: Loading Snapshot for Target: sakila\n</code></pre> <p>Here we see:</p> <ul> <li>In lines 4-6, we identify which snapshot we're using</li> <li>In lines 7-10, the snapshot is retrieved and unzipped to a temporary directory. Since it is already in the working directory, we don't have to <code>pull</code> it from the cloud</li> <li>In line 11, the destination database is dropped, recreated, and loaded with the snapshot</li> </ul>"}, {"location": "snapshot/configuration/", "title": "Snapshot Configuration", "text": "<p>The DBSnapper Agent is configured using a YAML file, which is created when you run <code>dbsnapper config init</code> In this file you can specify multiple target configurations, each target being a set of options for a database you want to snapshot.</p> <p>Referring to our sample configuration file, the highlighted lines show the configuration options for a snapshot:</p> <p><code>~/.config/dbsnapper/dbnsapper.yml</code> example</p> <pre><code>authtoken: 1234567890abcdef1234567890abcdef....\nworking_directory: /Users/snappy/.dbsnapper\ndocker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:16-alpine\nsecret_key: 1234567890abcdef1234567890abcdef\n# Target configurations\ntargets:\n  sakila:\n    name: sakila\n    # Snapshot configuration\n    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n    # Subsetting configuration\n    subset:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_subset?tls=false\n      subset_tables:\n        - table: sakila.film\n          where: \"film_id &lt; 20\"\n        - table: sakila.actor\n          percent: 20\n      copy_tables:\n        - sakila.store\n      excluded_tables:\n        - sakila.staff\n      added_relationships:\n        - fk_table: sakila.address\n          fk_columns: city_id\n          ref_table: sakila.city\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: sakila.store\n          ref_table: sakila.staff\n    # Sanitization configuration\n    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: sakila-sanitize.sql\n</code></pre>"}, {"location": "snapshot/configuration/#configuration-options", "title": "Configuration options", "text": "<p>As you can see, the configuration options are quite simple, consisting of a database source url <code>src_url</code> and a destination url <code>dst_url</code>.</p> <p>Subset configuration options</p> <code>src_url</code> <p>Connection string for the database you want to snapshot  </p> <code>dst_url</code> <p>Connection string for the database where you want to restore the snapshot.</p> <p>Danger: Destination database <code>dst_url</code> will be DROPPED and RECREATED</p> <p>Any connection string provided in the <code>dst_url</code> attribute will be overwritten when certain commands are used such as <code>load</code> which loads a snapshot to the destination database.</p>"}, {"location": "snapshot/introduction/", "title": "Introduction", "text": "<p>Snapshotting your database is a fundamental feature of DBSnapper, allowing you to create a point-in-time snapshot or backup of your database. Snapshotting takes a full copy of your database, including the schema and data, and stores it in a compressed file. This snapshot can then be used to restore your database to the state it was in when the snapshot was taken.</p>"}, {"location": "snapshot/introduction/#overview", "title": "Overview", "text": "<p>DBSnapper provides the <code>build</code> and <code>load</code> commands to create and restore a snapshot of your database.</p>"}, {"location": "snapshot/introduction/#build-command-steps", "title": "<code>build</code> command steps", "text": "<ol> <li>Connect to database specified by the <code>src_url</code> in the target definition</li> <li>Invoke the database vendor's database dump command to create a snapshot of the database</li> <li>Compress the snapshot file and store it in the working directory</li> </ol>"}, {"location": "snapshot/introduction/#load-command-steps", "title": "<code>load</code> command steps", "text": "<ol> <li>Find the requested snapshot file in the working directory</li> <li>Decompress the snapshot file</li> <li>Connect to the database specified by the <code>dst_url</code> in the target definition</li> <li>Drop and recreate the database</li> <li>Invoke the database vendor's database restore command to restore the snapshot to the database</li> </ol>"}, {"location": "snapshot/introduction/#required-dependencies", "title": "Required Dependencies", "text": "<p>DBSnapper uses database vendor tools to perform snapshot operations.</p> Database Vendor <code>build</code> command <code>load</code> command queries PostgreSQL <code>pg_dump</code> <code>pg_restore</code> <code>psql</code> MySQL <code>mysqldump</code> <code>mysql</code> <code>mysql</code>"}]}