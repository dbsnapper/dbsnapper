{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Welcome", "text": ""}, {"location": "#welcome-to-dbsnapper", "title": "Welcome to DBSnapper", "text": "<p>Latest Updates</p> <ul> <li>VSCode Extension - Available on the Visual Studio Marketplace for in-editor database snapshot management</li> <li>Terraform Provider - Manage DBSnapper resources with Infrastructure as Code</li> <li>GitHub Actions - Automate database snapshots in your CI/CD pipelines</li> </ul> <p> DBSnapper Architecture Overview </p> <p>DBSnapper revolutionizes the way development teams handle database snapshotting, bringing de-identified production data into the heart of development and testing workflows. It stands as a robust alternative to traditional, often cumbersome methods for creating development and test fixtures. With DBSnapper, you get to leverage real, production-grade data, stripped of its sensitive elements, to power your development and testing environments.</p>"}, {"location": "#quick-start", "title": "Quick Start", "text": "<p>Ready to get started? Here's your path to creating your first database snapshot:</p> <ol> <li>Install DBSnapper - Get the CLI tool set up</li> <li>Quick Start Guide - Create your first snapshot in minutes</li> <li>Sign up for DBSnapper Cloud - Share snapshots with your team</li> </ol>"}, {"location": "#sign-up-for-dbsnapper-cloud", "title": "Sign Up for DBSnapper Cloud", "text": "<p>Sign Up for the DBSnapper Cloud and get started with a safer, simpler way to manage your database snapshots.</p>"}, {"location": "#releases-and-integrations", "title": "Releases and Integrations", "text": "<ul> <li> <p>   The DBSnapper Agent interacts with your databases and communicates with the DBSnapper Cloud.</p> </li> <li> <p> - DBSnapper Extension for VSCode, allowing you to load database snapshots directly from your editor.</p> </li> <li> <p>   The DBSnapper Terraform Provider allows you to manage DBSnapper resources using the Terraform platform and Infrastructure as Code.</p> </li> <li> <p>   The DBSnapper GitHub Action allows you to include DBSnapper in your CI/CD pipelines for automated management of database snapshots.</p> </li> <li> <p>   DBSnapper supports Okta OIDC for Single Sign-On (SSO) authentication and group sharing.</p> </li> </ul>"}, {"location": "#key-features", "title": "Key Features", "text": "<p>DBSnapper provides comprehensive database snapshot management with these core capabilities:</p>"}, {"location": "#database-support-storage", "title": "\ud83d\uddc4\ufe0f Database Support &amp; Storage", "text": "<ul> <li>PostgreSQL and MySQL databases with more engines coming soon</li> <li>Bring Your Own Storage - Use Amazon S3, Cloudflare R2, or your preferred cloud storage</li> <li>Presigned URLs for secure upload and download operations</li> </ul>"}, {"location": "#security-compliance", "title": "\ud83d\udd12 Security &amp; Compliance", "text": "<ul> <li>Data Sanitization - Remove or mask sensitive information</li> <li>SSO Integration - Okta OIDC support with group-based sharing</li> <li>Private-cloud First - Your data stays in your infrastructure</li> </ul>"}, {"location": "#developer-experience", "title": "\u2699\ufe0f Developer Experience", "text": "<ul> <li>Zero-Config Operation - Complex operations in a single command</li> <li>Terraform Provider - Infrastructure as Code support</li> <li>GitHub Actions - CI/CD pipeline integration</li> <li>VSCode Extension - In-editor snapshot management</li> <li>Docker-Enabled - Leverages containerization for database tools</li> </ul>"}, {"location": "#advanced-capabilities", "title": "\ud83d\udcca Advanced Capabilities", "text": "<ul> <li>Database Subsetting - Create smaller, relationally-complete snapshots</li> <li>Ephemeral Sanitization - No need for temporary databases</li> <li>Templating Engine - Environment variable support for sensitive data</li> </ul>"}, {"location": "#core-capabilities", "title": "Core Capabilities", "text": ""}, {"location": "#subsetting-smaller-snapshots-relationally-complete", "title": "\ud83d\udce6 Subsetting - Smaller snapshots, relationally complete", "text": "<p>Create smaller, more manageable relationally-complete snapshots of your database. Perfect for development and testing when you only need a subset of production data.</p> <p>Key Benefits:</p> <ul> <li>Efficient Data Management - Work with relevant data only, reducing resource usage</li> <li>Maintains Data Integrity - Preserves relational structure and referential integrity</li> <li>Customizable Criteria - Define specific tables, rows, or data sets to include</li> </ul>"}, {"location": "#snapshotting-simplified-database-backups", "title": "Snapshotting - Simplified database backups", "text": "<p>DBSnapper offers an efficient and powerful solution for snapshotting databases that simplifies the snapshotting process for different database platforms.</p> <p>DBSnapper's snapshotting capability is not just about capturing data; it's a strategic tool that integrates into and enhances the entire software development lifecycle. From creating realistic test environments and aiding in AI model training to providing essential support in CI/CD pipelines, DBSnapper stands as an indispensable asset for any software development team aiming to streamline and improve their database management and utilization in a modern startup environment.</p> <ul> <li> <p>Real-world Test Cases: Utilizing de-identified data snapshots, you can create more effective and realistic test cases. This helps in identifying potential issues in a more accurate production-like environment.</p> </li> <li> <p>Seamless Integration with CI/CD Pipelines: DBSnapper can be easily integrated into CI/CD pipelines such as GitHub Actions, automating the process of generating snapshots for your team and ensuring the team is using the latest and most accurate data for testing.</p> </li> <li> <p>Training AI Models: For AI and machine learning initiatives, having access to diverse, real-world data sets is crucial. DBSnapper's ability to provide de-identified snapshots of real operational data can significantly enhance the training process of AI models, leading to more accurate models.</p> </li> </ul>"}, {"location": "#sanitization-de-identification-and-sensitive-data-removal", "title": "Sanitization - De-identification and sensitive data removal", "text": "<p>DBSnapper enables you to de-identify and sanitize your production data, removing sensitive information such as personal details, financial data, and other confidential information. This is crucial for ensuring compliance with data protection regulations and maintaining the privacy and security of your users.</p> <ul> <li> <p>Data Provenance During Sanitization: The DBSnapper tools are designed to give you full control over your data, ensuring that no sensitive or proprietary data leaves your environment.</p> </li> <li> <p>Adherence to GDPR and Other Regulations: In the era of stringent data protection laws like the GDPR, CCPA, and others, DBSnapper's sanitization feature ensures that your data handling practices are compliant, reducing the risk of legal complications and hefty fines.</p> </li> <li> <p>Maintaining Data Utility Post-Sanitization: Despite the removal of sensitive data, the utility and integrity of the dataset are preserved, making it suitable for development, testing, and analysis without compromising privacy.</p> </li> </ul>"}, {"location": "#share-securely-distribute-snapshots-via-dbsnapper-cloud", "title": "Share - Securely distribute snapshots via DBSnapper Cloud", "text": "<p>The sharing aspect of DBSnapper is made possible through the DBSnapper Cloud, a critical feature for secure storage and distribution of database snapshots. It's designed for seamless collaboration within your team or for integration with automated processes.</p> <ul> <li> <p>SSO-Aware Team Sharing: DBSnapper Cloud supports Single Sign-On (SSO), and is SSO-Group aware, allowing you to easily share snapshots with your team members, using the groups you've already set up in your SSO provider.</p> </li> <li> <p>Flexibility of Storage Choices: With DBSnapper, you have the flexibility to 'Bring Your Own Cloud Storage Provider'. This means you can choose the cloud storage that best aligns with your company\u2019s policies and data management strategies, ensuring that your data remains within your approved PaaS vendor.</p> </li> <li> <p>Easy Access for Team Members: Shared snapshots are easily accessible to authorized team members. This facilitates collaboration, as team members can work with the same datasets in a synchronized manner.</p> </li> <li> <p>Integration with Automated Processes: The DBSnapper Cloud is designed for integration with automated processes, such as CI/CD pipelines, making it simpler to incorporate database snapshots into your development and deployment workflows.</p> </li> </ul>"}, {"location": "configuration/", "title": "Configuration Settings", "text": "<p>The DBSnapper configuration system provides comprehensive control over database snapshot operations, cloud integration, security settings, and workflow automation. This guide covers all configuration options from basic setup to advanced features.</p>"}, {"location": "configuration/#quick-start", "title": "Quick Start", "text": ""}, {"location": "configuration/#default-configuration-location", "title": "Default Configuration Location", "text": "<p>DBSnapper looks for configuration files in this order:</p> <ol> <li><code>./dbsnapper.yml</code> (current directory)</li> <li><code>~/.config/dbsnapper/dbsnapper.yml</code> (user config directory)</li> <li>Environment variables (see Environment Variables)</li> </ol>"}, {"location": "configuration/#minimal-configuration", "title": "Minimal Configuration", "text": "~/.config/dbsnapper/dbsnapper.yml<pre><code># Required for encryption\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\n\n# Define your first target\ntargets:\n  myapp-dev:\n    name: \"My App Development\"\n    snapshot:\n      src_url: \"postgresql://user:pass@localhost:5432/myapp_prod\"\n      dst_url: \"postgresql://user:pass@localhost:5432/myapp_dev\"\n</code></pre>"}, {"location": "configuration/#complete-configuration-reference", "title": "Complete Configuration Reference", "text": "<p>Complete Configuration Example</p> <pre><code># Core Settings\nauthtoken: jREZkinFQpSJb4TWZAKioHuCD7KG2GV3xZqiUwbZfeJVTS7V\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: ~/.dbsnapper\nenv: development\ndebug: true\ntrace: false\n\n\n# Docker Configuration\ndocker:\n  images:\n    mysql: mysql:9\n    postgres: postgres:latest\n\n# Global Overrides\noverride:\n  san_query: RFJPUCBUQUJMRSBJRiBFWElTVFMgZGJzbmFwcGVyX2luZm87...\n  dst_db_url: postgres://postgres:postgres@localhost:15432/override_db\n\n# Default Settings\ndefaults:\n  shared_target_dst_db_url: postgres://postgres:postgres@localhost:15432/shared_default\n\n# SSO Configuration\nsso:\n  okta:\n    provider_url: https://dev-89837244.okta.com\n    client_id: 0oag95e2z5BhLZ5AI5d7\n    redirect_url: http://localhost:8080/callback\n\n# Storage Profiles\nstorage_profiles:\n  aws-s3-prod:\n    provider: s3\n    awscli_profile: default\n    region: us-west-2\n    bucket: dbsnapper-production\n    prefix: snapshots\n\n  cloudflare-r2:\n    provider: r2\n    awscli_profile: r2_profile\n    bucket: dbsnapper-r2\n    prefix: sanitized\n\n# Target Definitions\ntargets:\n  myapp-prod:\n    name: \"Production Database\"\n    snapshot:\n      src_url: postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@prod-host:5432/myapp\n      dst_url: postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@dev-host:5432/myapp_dev\n      schema_config:\n        include_schemas: [\"public\", \"app_data\"]\n        exclude_schemas: [\"temp_logs\"]\n    storage_profile: aws-s3-prod\n\n    sanitize:\n      dst_url: postgres://user:pass@localhost:5432/myapp_sanitized\n      query_file: sanitize.sql\n      override_query: \"UPDATE users SET email = 'user@example.com';\"\n\n    subset:\n      src_url: postgres://user:pass@prod-host:5432/myapp\n      dst_url: postgres://user:pass@localhost:5432/myapp_subset\n      subset_tables:\n        - table: public.users\n          where: \"created_at &gt; '2023-01-01'\"\n        - table: public.orders\n          percent: 20\n      copy_tables:\n        - public.settings\n      excluded_tables:\n        - public.temp_data\n      added_relationships:\n        - fk_table: public.orders\n          fk_columns: user_id\n          ref_table: public.users\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: public.audit_log\n          ref_table: public.users\n\n  shared-team-target:\n    name: \"Team Shared Target\"\n    share:\n      dst_url: postgres://user:pass@localhost:5432/team_shared\n      storage_profile_name: cloudflare-r2\n</code></pre>"}, {"location": "configuration/#configuration-sections", "title": "Configuration Sections", "text": ""}, {"location": "configuration/#global-settings", "title": "Global Settings", "text": ""}, {"location": "configuration/#core-settings", "title": "Core Settings", "text": "<pre><code># Required: Encryption key for sensitive data\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\n\n# Optional: Authentication token for DBSnapper Cloud\nauthtoken: jREZkinFQpSJb4TWZAKioHuCD7KG2GV3...\n\n# Optional: Local storage location (default: ~/.dbsnapper)\nworking_directory: ~/.dbsnapper\n\n# Optional: Environment mode (development/staging/production)\nenv: development\n</code></pre> <p>Core Settings Details:</p> <code>secret_key</code> (required string[32]) 16-byte hexadecimal string (32 characters) used for encrypting sensitive configuration data. Generated automatically with <code>dbsnapper config init</code>. <code>authtoken</code> (optional string) Authentication token for DBSnapper Cloud integration. Enables cloud targets, storage profiles, and team sharing features. <code>working_directory</code> (optional string) Directory for storing local snapshots and temporary files. Defaults to <code>~/.dbsnapper</code>. <code>env</code> (optional string) Environment mode that configures API endpoints automatically: - <code>development</code> \u2192 <code>http://app.dbsnapper.local:3000/api/v3</code> - <code>staging</code> \u2192 <code>https://stg4777.dbsnapper.com/api/v3</code> - <code>production</code> \u2192 <code>https://app.dbsnapper.com/api/v3</code>"}, {"location": "configuration/#docker-configuration", "title": "Docker Configuration", "text": "<p>Configure Docker images for database engines:</p> <pre><code>docker:\n  images:\n    mysql: mysql:9\n    postgres: postgres:latest\n</code></pre> <p>Supported Database Images:</p> <ul> <li><code>mysql</code>: MySQL database engine</li> <li><code>postgres</code>: PostgreSQL database engine</li> </ul>"}, {"location": "configuration/#environment-variables", "title": "Environment Variables", "text": "<p>DBSnapper supports comprehensive environment variable configuration using the <code>DBSNAPPER_</code> prefix. Convert configuration keys by replacing periods with double underscores <code>__</code>.</p>"}, {"location": "configuration/#common-environment-variables", "title": "Common Environment Variables", "text": "<pre><code># Core Settings\nDBSNAPPER_SECRET_KEY=c614a689a559d1b517c28a5e4fcdc059\nDBSNAPPER_AUTHTOKEN=jREZkinFQpSJb4TWZAKioHuCD7KG2GV3...\nDBSNAPPER_WORKING_DIRECTORY=/opt/dbsnapper\nDBSNAPPER_ENV=production\n\n# Automation &amp; CI\nDBSNAPPER_NO_CONFIRM=true          # Skip confirmation prompts (i.e. when loading snapshots)\n\n# Configuration Overrides\nDBSNAPPER_OVERRIDE__SAN_QUERY=RFJPUCBUQUJMRSBJRi...\nDBSNAPPER_OVERRIDE__DST_DB_URL=postgres://user:pass@localhost:5432/override_db\nDBSNAPPER_DEFAULTS__SHARED_TARGET_DST_DB_URL=postgres://user:pass@localhost:5432/shared_default\n\n# Docker Images\nDBSNAPPER_DOCKER__IMAGES__POSTGRES=postgres:16-alpine\nDBSNAPPER_DOCKER__IMAGES__MYSQL=mysql:8-oracle\n</code></pre>"}, {"location": "configuration/#configuration-priority", "title": "Configuration Priority", "text": "<p>Environment variables take precedence over configuration file settings, allowing for deployment-specific overrides without modifying configuration files.</p>"}, {"location": "configuration/#url-templates", "title": "URL Templates", "text": "<p>DBSnapper supports Go template syntax in all connection URLs, enabling dynamic configuration with environment variables:</p> <pre><code>targets:\n  dynamic-target:\n    snapshot:\n      src_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@{{`DB_HOST` | env}}:5432/{{`DB_NAME` | env}}\"\n      dst_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@localhost:5432/{{`DB_NAME` | env}}_snapshot\"\n</code></pre> <p>Template Functions:</p> <ul> <li><code>{{</code>ENV_VAR<code>| env}}</code> - Substitute environment variable value</li> <li><code>{{</code>CONSTANT<code>}}</code> - Use literal string value</li> </ul> <p>Supported URL Fields:</p> <ul> <li><code>snapshot.src_url</code> and <code>snapshot.dst_url</code></li> <li><code>sanitize.dst_url</code></li> <li><code>subset.src_url</code> and <code>subset.dst_url</code></li> <li><code>share.dst_url</code></li> </ul>"}, {"location": "configuration/#targets", "title": "Targets", "text": "<p>Targets define database connections and operations. Each target can support multiple operations: snapshot, sanitize, subset, and share.</p>"}, {"location": "configuration/#basic-target-configuration", "title": "Basic Target Configuration", "text": "<pre><code>targets:\n  myapp-prod:\n    name: \"Production Database\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod-host:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev-host:5432/myapp_dev\" # Will be created if it doesn't exist or overwritten if it does.\n    storage_profile: aws-s3-production\n</code></pre>"}, {"location": "configuration/#schema-configuration", "title": "Schema Configuration", "text": "<p>PostgreSQL Only Feature: Control which database schemas are included in snapshots. Schema filtering is only supported for PostgreSQL databases - MySQL uses standard dump without filtering.</p> <pre><code>targets:\n  postgres-target:\n    name: \"PostgreSQL with Schema Filtering\"\n    snapshot:\n      src_url: \"postgresql://user:pass@host:5432/myapp\"\n      dst_url: \"postgresql://user:pass@host:5432/myapp_snap\"\n      schema_config:\n        # Option 1: Include only specific schemas\n        include_schemas: [\"public\", \"app_data\", \"reports\"]\n\n        # Option 2: Exclude specific schemas (alternative to include_schemas)\n        # exclude_schemas: [\"temp_logs\", \"analytics\", \"debug\"]\n\n        # Option 3: Control default behavior\n        use_default_schema: false # true = default schema only, false = all schemas\n</code></pre> <p>Schema Configuration Behavior:</p> Configuration Build Behavior Load Behavior No <code>schema_config</code> Builds <code>public</code> schema only Loads <code>public</code> schema only <code>use_default_schema: true</code> Builds default schema (<code>public</code>) Loads default schema from snapshot <code>use_default_schema: false</code> Builds all available schemas Loads all schemas present in snapshot <code>include_schemas: [\"public\", \"app\"]</code> Builds only listed schemas that exist Loads only listed schemas from snapshot <code>exclude_schemas: [\"temp\", \"logs\"]</code> Builds all schemas except listed ones Loads all schemas except excluded ones <p>Schema Configuration Options:</p> <code>include_schemas</code> (optional array) Include only specified schemas in build/load operations. Takes precedence over <code>exclude_schemas</code>. Only existing schemas will be processed. <code>exclude_schemas</code> (optional array) Exclude specified schemas from build/load operations. Processes all other available schemas. <code>use_default_schema</code> (optional boolean) Controls default schema behavior: - <code>true</code>: Process only the default schema (<code>public</code> for PostgreSQL) - <code>false</code>: Process all available schemas in the database - Not set: Defaults to <code>public</code> schema only <p>Important Notes:</p> <ul> <li>Dynamic Analysis: Schema filtering analyzes available schemas in the source database at build time</li> <li>Load Consistency: Load operations respect the schemas that were included in the snapshot</li> <li>Non-existent Schemas: Requesting schemas that don't exist will show warnings but won't fail the operation</li> <li>MySQL Limitation: Schema filtering is PostgreSQL-only; MySQL targets ignore <code>schema_config</code></li> <li>Validation: Schemas cannot appear in both <code>include_schemas</code> and <code>exclude_schemas</code></li> </ul> <p>Schema Validation</p> <p>The system prevents schemas from appearing in both <code>include_schemas</code> and <code>exclude_schemas</code> lists. Configuration validation will fail if this occurs.</p> <p>!!! tip \"Best Practices\" - Use <code>include_schemas</code> for explicit control over which schemas to process - Use <code>exclude_schemas</code> when you want most schemas except a few (logs, temp data) - Set <code>use_default_schema: false</code> to capture all schemas in complex applications - Test schema configuration with small databases first to verify expected behavior</p>"}, {"location": "configuration/#sanitization-configuration", "title": "Sanitization Configuration", "text": "<p>Configure data sanitization for security and compliance:</p> <pre><code>targets:\n  sanitized-target:\n    sanitize:\n      dst_url: \"postgresql://user:pass@localhost:5432/myapp_sanitized\"\n      query_file: \"sanitization.sql\"\n      override_query: \"UPDATE users SET email = CONCAT('user', id, '@example.com');\"\n</code></pre> <p>Sanitization Priority System</p> <p>DBSnapper uses a three-level priority system for sanitization queries:</p> <ol> <li>Target-level <code>override_query</code> (Highest Priority)</li> <li>Global <code>override.san_query</code> (Medium Priority)</li> <li>Target <code>query_file</code> (Lowest Priority)</li> </ol> <p>Base64 Encoding Support</p> <p>All sanitization queries support base64 encoding for complex SQL:</p> <pre><code># Base64 encoded query (automatically detected and decoded)\noverride:\n  san_query: \"VVBEQVRFIHVzZXJzIFNFVCBlbWFpbCA9ICd1c2VyQGV4YW1wbGUuY29tJzs=\"\n</code></pre>"}, {"location": "configuration/#subset-configuration", "title": "Subset Configuration", "text": "<p>Create smaller, referentially intact database subsets:</p> <pre><code>targets:\n  subset-target:\n    subset:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_subset\"\n\n      # Tables to subset with specific criteria\n      subset_tables:\n        - table: public.users\n          where: \"created_at &gt; '2023-01-01'\"\n        - table: public.orders\n          percent: 20\n\n      # Tables to copy completely\n      copy_tables:\n        - public.settings\n        - public.configuration\n\n      # Tables to exclude entirely\n      excluded_tables:\n        - public.temp_data\n        - public.audit_logs\n\n      # Add missing foreign key relationships\n      added_relationships:\n        - fk_table: public.orders\n          fk_columns: user_id\n          ref_table: public.users\n          ref_columns: id\n\n      # Exclude problematic relationships (circular dependencies)\n      excluded_relationships:\n        - fk_table: public.audit_log\n          ref_table: public.users\n</code></pre> <p>Subset Configuration Details:</p> <code>subset_tables</code> Initial tables of interest with filtering criteria. Use either <code>where</code> clause or <code>percent</code> (not both). <code>copy_tables</code> Tables copied in their entirety to maintain referential integrity. <code>excluded_tables</code> Tables completely excluded from the subset. <code>added_relationships</code> Foreign key relationships not defined in database schema but needed for referential integrity. <code>excluded_relationships</code> Relationships to ignore (useful for breaking circular dependencies)."}, {"location": "configuration/#share-configuration", "title": "Share Configuration", "text": "<p>Configure targets for team sharing via cloud storage:</p> <pre><code>targets:\n  shared-target:\n    name: \"Team Shared Target\"\n    share:\n      dst_url: \"postgresql://user:pass@localhost:5432/shared_db\"\n      storage_profile_name: \"s3-sanitized-bucket\"\n</code></pre> <p>Share Configuration Details:</p> <code>dst_url</code> Database connection where shared snapshots will be loaded. <code>storage_profile_name</code> Reference to storage profile containing shared snapshots."}, {"location": "configuration/#storage-profiles", "title": "Storage Profiles", "text": "<p>Configure cloud storage for snapshot backup and sharing:</p>"}, {"location": "configuration/#amazon-s3", "title": "Amazon S3", "text": "<pre><code>storage_profiles:\n  s3-production:\n    provider: s3\n    region: us-west-2\n    bucket: dbsnapper-prod-snapshots\n    prefix: team-snapshots\n\n    # Option 1: Use AWS CLI profile\n    awscli_profile: production\n\n    # Option 2: Direct credentials (not recommended for production)\n    access_key: AKIA...\n    secret_key: xyz...\n</code></pre>"}, {"location": "configuration/#cloudflare-r2", "title": "Cloudflare R2", "text": "<pre><code>storage_profiles:\n  r2-sanitized:\n    provider: r2\n    bucket: dbsnapper-r2-sanitized\n    prefix: sanitized\n\n    # Option 1: Use AWS CLI profile configured for R2\n    awscli_profile: r2_profile\n\n    # Option 2: Direct credentials with account ID\n    access_key: your-r2-access-key\n    secret_key: your-r2-secret-key\n    account_id: your-cloudflare-account-id\n</code></pre>"}, {"location": "configuration/#minio", "title": "MinIO", "text": "<pre><code>storage_profiles:\n  minio-local:\n    provider: minio\n    endpoint: http://localhost:9000\n    bucket: dbsnapper-minio\n    prefix: snapshots\n    access_key: minioadmin\n    secret_key: minioadmin\n</code></pre>"}, {"location": "configuration/#digitalocean-spaces", "title": "DigitalOcean Spaces", "text": "<pre><code>storage_profiles:\n  do-spaces:\n    provider: dospaces\n    endpoint: https://nyc3.digitaloceanspaces.com\n    bucket: dbsnapper-do-spaces\n    prefix: production\n    access_key: your-spaces-key\n    secret_key: your-spaces-secret\n</code></pre>"}, {"location": "configuration/#override-system", "title": "Override System", "text": "<p>Global overrides provide system-wide defaults and overrides:</p> <pre><code>override:\n  # Global sanitization query (base64 encoded)\n  san_query: \"VVBEQVRFIHVzZXJzIFNFVCBlbWFpbCA9ICd1c2VyQGV4YW1wbGUuY29tJzs=\"\n\n  # Global destination database override\n  dst_db_url: \"postgresql://postgres:postgres@localhost:5432/global_override\"\n</code></pre> <p>Override Priority:</p> <ul> <li>Override settings apply to all operations unless specifically overridden in target configuration</li> <li>Target-level settings always take precedence over global overrides</li> </ul>"}, {"location": "configuration/#defaults-system", "title": "Defaults System", "text": "<p>Configure default values for shared and team operations:</p> <pre><code>defaults:\n  # Default destination for shared team snapshots\n  shared_target_dst_db_url: \"postgresql://user:pass@localhost:5432/shared_default\"\n</code></pre>"}, {"location": "configuration/#sso-configuration", "title": "SSO Configuration", "text": "<p>Configure Single Sign-On for team sharing and authentication:</p> <pre><code>sso:\n  okta:\n    provider_url: https://dev-89837244.okta.com\n    client_id: 0oag95e2z5BhLZ5AI5d7\n    redirect_url: http://localhost:8080/callback\n    # Tokens are encrypted automatically when stored\n    access_token: $aes$fdff26aae3438c29...\n    refresh_token: $aes$6d42e5fd73fea9de...\n    expires: 1715292878\n</code></pre>"}, {"location": "configuration/#configuration-modes", "title": "Configuration Modes", "text": ""}, {"location": "configuration/#local-mode-standalone", "title": "Local Mode (Standalone)", "text": "<p>Local mode uses only the configuration file without cloud integration:</p> <pre><code># Minimal local configuration\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: ~/.dbsnapper\n\ntargets:\n  local-target:\n    name: \"Local Development\"\n    snapshot:\n      src_url: \"postgresql://user:pass@localhost:5432/myapp\"\n      dst_url: \"postgresql://user:pass@localhost:5432/myapp_dev\"\n</code></pre> <p>Local Mode Features:</p> <ul> <li>Local snapshot storage only</li> <li>No cloud integration</li> <li>No team sharing capabilities</li> <li>Faster operations (no network overhead)</li> </ul>"}, {"location": "configuration/#cloud-mode", "title": "Cloud Mode", "text": "<p>Cloud mode enables full feature set with DBSnapper Cloud integration:</p> <pre><code># Cloud configuration\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nauthtoken: jREZkinFQpSJb4TWZAKioHuCD7KG2GV3...\nenv: production\n\nstorage_profiles:\n  cloud-storage:\n    provider: s3\n    bucket: dbsnapper-cloud\n    awscli_profile: production\n\ntargets:\n  cloud-target:\n    name: \"Production with Cloud\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp\"\n    storage_profile: cloud-storage\n</code></pre> <p>Cloud Mode Features:</p> <ul> <li>Cloud snapshot storage and backup</li> <li>Team sharing with SSO integration</li> <li>Cloud target management</li> <li>Enhanced security with encryption</li> <li>Cross-team collaboration</li> </ul>"}, {"location": "configuration/#security-considerations", "title": "Security Considerations", "text": ""}, {"location": "configuration/#credential-management", "title": "Credential Management", "text": "<p>Best Practices:</p> <ul> <li>Use AWS CLI profiles instead of hardcoded credentials</li> <li>Store sensitive values as environment variables</li> <li>Use DBSnapper's encryption for stored tokens</li> <li>Regularly rotate authentication tokens</li> </ul> <p>Example Secure Configuration:</p> <pre><code># Use environment variables for sensitive data\nauthtoken: # Set via DBSNAPPER_AUTHTOKEN\nsecret_key: # Set via DBSNAPPER_SECRET_KEY\n\ntargets:\n  secure-target:\n    snapshot:\n      # Use templates with environment variables\n      src_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@{{`DB_HOST` | env}}/myapp\"\n      dst_url: \"postgres://{{`DB_USER` | env}}:{{`DB_PASS` | env}}@localhost/myapp_dev\"\n\nstorage_profiles:\n  secure-s3:\n    provider: s3\n    # Use AWS CLI profile instead of credentials\n    awscli_profile: dbsnapper-production\n    bucket: secure-snapshots\n</code></pre>"}, {"location": "configuration/#encryption", "title": "Encryption", "text": "<p>DBSnapper automatically encrypts sensitive configuration values:</p> <ul> <li>SSO tokens (access_token, refresh_token)</li> <li>Optionally other sensitive strings using <code>$aes$</code> prefix</li> <li>All encryption uses AES with the configured <code>secret_key</code></li> </ul>"}, {"location": "configuration/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "configuration/#configuration-validation", "title": "Configuration Validation", "text": "<p>Validate your configuration:</p> <pre><code>dbsnapper config check\n</code></pre>"}, {"location": "configuration/#common-issues", "title": "Common Issues", "text": "<p>Invalid Schema Configuration</p> <pre><code>Error: schema 'analytics' cannot be in both include and exclude lists\n</code></pre> <ul> <li>Remove schema from one of the lists</li> <li>Use either <code>include_schemas</code> OR <code>exclude_schemas</code>, not both for the same schema</li> </ul> <p>Template Parsing Errors</p> <pre><code>Error: template parsing failed for src_url\n</code></pre> <ul> <li>Check template syntax: <code>{{</code>VARIABLE<code>| env}}</code></li> <li>Ensure environment variables exist</li> <li>Verify no typos in variable names</li> </ul> <p>Storage Profile Issues</p> <pre><code>Error: storage profile 'missing-profile' not found\n</code></pre> <ul> <li>Verify storage profile name matches configuration</li> <li>Check storage profile is properly configured</li> <li>Ensure required credentials are provided</li> </ul>"}, {"location": "configuration/#debug-mode", "title": "Debug Mode", "text": "<p>For troubleshooting, you can enable additional logging by adding <code>debug: true</code> to your configuration file or using environment variables as needed.</p>"}, {"location": "configuration/#migration-guide", "title": "Migration Guide", "text": ""}, {"location": "configuration/#upgrading-from-earlier-versions", "title": "Upgrading from Earlier Versions", "text": "<p>v2.6.0+ Override System:</p> <ul> <li>Added <code>override.san_query</code> and <code>override.dst_db_url</code></li> <li>Migrate target-specific overrides to global overrides if desired</li> </ul> <p>v2.7.0+ Defaults System:</p> <ul> <li>Added <code>defaults.shared_target_dst_db_url</code></li> <li>Configure default shared target destination</li> </ul> <p>Schema Configuration:</p> <ul> <li>New <code>schema_config</code> section replaces database-specific schema filtering</li> <li>Update existing schema filtering configurations</li> </ul>"}, {"location": "configuration/#see-also", "title": "See Also", "text": "<ul> <li>Build Command - Creating database snapshots</li> <li>Load Command - Restoring snapshots</li> <li>Cloud Storage Setup</li> <li>Database Engines</li> <li>Sanitization</li> </ul>"}, {"location": "how-it-works/", "title": "How DBSnapper Works", "text": ""}, {"location": "how-it-works/#architecture-overview", "title": "Architecture Overview", "text": "<p> DBSnapper Architecture Overview </p> <p>The DBSnapper architecture is designed to provide a safe, secure, and fast way to create and share database snapshots. The architecture is built around a bring-your-own-infrastructure model, keeping data within your control at all times. Our focus on integrations ensures that DBSnapper can seamlessly fit into your existing workflows and tools, providing automation and ease of use for your team.</p>"}, {"location": "how-it-works/#step-1-snapshot-your-database", "title": "Step 1: Snapshot Your Database", "text": "<p>Begin by using the DBSnapper Agent to create a snapshot of your database. This snapshot captures a point-in-time backup of the current state of your database. Use the DBSnapper cloud to upload the snapshots into your own private storage provider</p>"}, {"location": "how-it-works/#step-2-sanitize-sensitive-data", "title": "Step 2: Sanitize Sensitive Data", "text": "<p>The next crucial step is sanitizing your snapshot. DBSnapper provides tools to remove sensitive data and Personally Identifiable Information (PII) efficiently. This process is essential for maintaining privacy and compliance with data protection regulations. The de-identification feature ensures that your development and testing environments are safe for use without compromising on data integrity.</p> <p>DBSnapper allows you to remove sensitive data and Personally Identifiable Information (PII) from the snapshot, ensuring de-identification is straightforward and efficient.</p>"}, {"location": "how-it-works/#step-3-share-with-your-team", "title": "Step 3: Share with your team", "text": "<p>After sanitization, it's time to share the snapshot with your team. DBSnapper facilitates easy and secure sharing directly from your private cloud storage. This step is instrumental in speeding up the software development process, as your team gets access to realistic, yet secure, user data. Whether it\u2019s for development, testing, or staging, your team can work with data that mirrors the production environment, enhancing the accuracy and efficiency of your workflows.</p>"}, {"location": "how-it-works/#get-started", "title": "Get Started", "text": "<p>Sign up and get running with DBSnapper, add integrations to take advantage of automation, and start sharing database snapshots with your team today.</p>"}, {"location": "installation/", "title": "DBSnapper Agent Installation", "text": "<p>The DBSnapper Agent is available for Mac and Linux with several ways to quickly install the Agent on your system.</p> <p> DBSnapper Agent User Interface </p>"}, {"location": "installation/#releases-page", "title": "Releases Page", "text": "<p>Our releases are available on the DBSnapper Agent Releases Page. You can download the latest release and view release notes from this page.</p>"}, {"location": "installation/#docker-images", "title": "Docker Images", "text": "<p>One of the easiest ways to get started with DBSnapper is to use the Docker image. The DBSnapper Agent is available as a Docker image on the DBSnapper Agent Packages Page. You can pull the latest image using the following command:</p> <pre><code>docker pull ghcr.io/dbsnapper/dbsnapper:latest\n</code></pre> <p>And run an interactive shell with the following command:</p> <pre><code>docker run -it ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n</code></pre>"}, {"location": "installation/#macos-universal-binary", "title": "MacOS Universal Binary", "text": "<p>MacOS users can install the Universal Mac package available on the DBSnapper Agent Releases Page. This package is designed to be compatible with various MacOS versions and hardware architectures, including both Intel and Apple Silicon chips</p>"}, {"location": "installation/#homebrew-tap", "title": "Homebrew Tap", "text": "<p>Get the latest release with Homebrew</p> <pre><code>brew install dbsnapper/tap/dbsnapper\n</code></pre>"}, {"location": "installation/#debian-rpm-and-apk-packages", "title": "Debian, RPM, and APK Packages", "text": "<p>For Linux users, <code>dbsnapper</code> can be installed using the <code>.deb</code>, <code>.rpm</code>, or <code>.apk</code> packages, depending on your Linux distribution and architecture. You can download these packages from the DBSnapper Agent Releases page.</p> <p>Here's an example of how you can install dbsnapper using a Debian package:</p> <ol> <li>Download the Release: Open your terminal and use wget to download the desired release. Replace 1.2.1 with the version number you wish to install:</li> </ol> <p>Replace the following values accordingly:</p> <p><code>TAG</code> with the version number you wish to install,</p> <p><code>ARCH</code> with your system architecture, and</p> <p><code>PKG_MGR_EXT</code> with your package extension.</p> <p>This following example installs the <code>dbsnapper</code> package version 1.2.1 for a Linux system with an AMD64 architecture using the Debian package manager.</p> <pre><code>TAG=2.0.3 &amp;&amp; \\\nARCH=linux_x86_64 &amp;&amp; \\\nPKG_MGR_EXT=deb &amp;&amp; \\\n\nwget https://github.com/dbsnapper/dbsnapper/releases/download/v\"$TAG\"/dbsnapper_\"$ARCH\".\"$PKG_MGR_EXT\"\n</code></pre> <ol> <li>Install with <code>dpkg</code>: Once the download is complete, you can install the package using dpkg:</li> </ol> <pre><code>dpkg -i dbsnapper_\"$ARCH\".\"$PKG_MGR\"\n</code></pre> <ol> <li>Verify Installation: After installation, you can verify that dbsnapper is installed by running the following command:</li> </ol> <pre><code>dbsnapper config check\n</code></pre> <p>Which will output some useful information about your environment and the dbsnapper installation.</p> <pre><code>root@snappy:/# dbsnapper config check\n\nDBSnapper Agent - Version: 2.0.3 (9e8b3abde4e0) Build Date: 2024-03-05T21:33:39Z\nDBSnapper Cloud: Standalone Mode\n\nChecking DBSnapper Configuration\n  \u2705 Config file ( /Users/joescharf/app/dbsnapper/agent/dbsnapper.yml ) found and loaded\n  \ud83d\udd35 Postgres Local Engine (pglocal)\n    \u2705 psql found at /Applications/Postgres.app/Contents/Versions/latest/bin/psql\n    \u2705 pg_dump found at /Applications/Postgres.app/Contents/Versions/latest/bin/pg_dump\n    \u2705 pg_restore found at /Applications/Postgres.app/Contents/Versions/latest/bin/pg_restore\n  \ud83d\udd35 MySQL Local Engine (mylocal)\n    \u2705 mysqldump found at /opt/homebrew/bin/mysqldump\n    \u2705 mysql found at /opt/homebrew/bin/mysql\n  \ud83d\udd35 Postgres Docker Engine (pgdocker)\n    \u2705 Docker client connected\n    \u2705 docker.images set in config file\n    \u2705 docker.images.postgres set in config file\n      \u2705 Found Docker image: postgres:latest\n  \ud83d\udd35 Mysql Docker Engine (mydocker)\n    \u2705 Docker client connected\n    \u2705 docker.images set in config file\n    \u2705 docker.images.mysql set in config file\n      \u2705 Found Docker image: mysql:8.0-oracle\n  \u2705 All supported database engines configured\n  \u26a0\ufe0f  DBSnapper Cloud not configured - get an account at https://dbsnapper.com\n\n  \u2705 Configuration OK\n</code></pre> <p>Continue to the Quick Start guide to configure DBSnapper and create your first snapshot.</p>"}, {"location": "philosophy/", "title": "Philosophy", "text": ""}, {"location": "philosophy/#why-dbsnapper-exists", "title": "Why DBSnapper Exists", "text": "<p>Development teams across the industry face a common challenge: how to work with realistic data for development and testing without compromising security or privacy. Traditional approaches often fall short:</p> <ul> <li>Brittle test fixtures that don't reflect real-world data complexity</li> <li>Manual database copying processes that are error-prone and time-consuming</li> <li>Compliance violations when production data is used without proper sanitization</li> <li>Development bottlenecks when teams can't access quality test data</li> </ul> <p>DBSnapper was created to solve these fundamental problems by providing an automated, secure, and compliant way to create de-identified database snapshots. Our goal is to make realistic development data accessible to every team, regardless of their infrastructure or compliance requirements.</p>"}, {"location": "philosophy/#core-principles", "title": "Core Principles", "text": "<p>DBSnapper is built on fundamental principles that prioritize security, privacy, and user control. These principles guide every architectural decision and feature development.</p>"}, {"location": "philosophy/#security-and-privacy-first", "title": "Security and Privacy First", "text": "<p>Data security and privacy form the foundation of everything we build:</p> <p>Data Sovereignty: Your data remains under your complete control at all times. DBSnapper never requires access to your databases outside your approved infrastructure, and we don't store copies of your sensitive data.</p> <p>Encryption Everywhere: When using DBSnapper Cloud, all sensitive information is encrypted both at rest and in transit. Connection string templating allows you to retrieve credentials from environment variables, avoiding the need to provide sensitive credentials to external services.</p> <p>Compliance Ready: Built-in data sanitization and subsetting features help teams maintain compliance with data protection regulations like GDPR, HIPAA, and SOX.</p>"}, {"location": "philosophy/#bring-your-own-infrastructure", "title": "Bring Your Own Infrastructure", "text": "<p>Rather than locking you into proprietary solutions, DBSnapper embraces a \"bring your own\" philosophy:</p> <p>Your Cloud Storage: Use your preferred cloud storage provider (Amazon S3, Cloudflare R2, or others) to ensure data stays within your approved infrastructure and reduces overall costs.</p> <p>Your Environment: The DBSnapper Agent works equally well in on-premises environments, cloud infrastructure, or local development machines.</p> <p>Your Workflow: Integrate with existing CI/CD pipelines, development tools, and team processes without disruption.</p>"}, {"location": "philosophy/#simplicity-without-compromise", "title": "Simplicity Without Compromise", "text": "<p>Complex problems don't always require complex solutions. DBSnapper prioritizes simplicity while maintaining powerful capabilities:</p> <p>Minimal Dependencies: Built with Go to create statically linked binaries that are easy to distribute and run across different operating systems and platforms.</p> <p>Smart Containerization: Optional Docker-based database engines eliminate the need to install database tools locally while supporting different database versions and platforms.</p> <p>Configuration as Code: YAML-based configuration files make snapshot processes reproducible and version-controllable.</p>"}, {"location": "philosophy/#design-decisions", "title": "Design Decisions", "text": "<p>These principles translate into specific architectural choices that make DBSnapper both powerful and accessible:</p>"}, {"location": "philosophy/#agent-based-architecture", "title": "Agent-Based Architecture", "text": "<p>The DBSnapper Agent runs in your environment, maintaining complete control over your data while providing secure communication with DBSnapper Cloud for orchestration and sharing features.</p>"}, {"location": "philosophy/#database-agnostic-approach", "title": "Database Agnostic Approach", "text": "<p>Support for multiple database engines (PostgreSQL, MySQL) with a pluggable architecture that allows for future expansion without breaking existing workflows.</p>"}, {"location": "philosophy/#storage-flexibility", "title": "Storage Flexibility", "text": "<p>Integration with major cloud storage providers ensures you can use existing infrastructure investments while maintaining data governance requirements.</p>"}, {"location": "philosophy/#automation-ready", "title": "Automation Ready", "text": "<p>Built-in support for CI/CD integration, Infrastructure as Code (Terraform), and development environment integration (VS Code extension) enables automated workflows from day one.</p>"}, {"location": "philosophy/#the-result-better-development-workflows", "title": "The Result: Better Development Workflows", "text": "<p>These principles and design decisions work together to deliver tangible benefits for development teams:</p> <ul> <li>Faster Development: Realistic test data accelerates feature development and bug identification</li> <li>Improved Quality: Testing against production-like data improves application reliability</li> <li>Enhanced Security: Built-in sanitization and encryption protect sensitive information</li> <li>Reduced Costs: Use existing infrastructure and avoid vendor lock-in</li> <li>Compliance Confidence: Automated de-identification supports regulatory requirements</li> </ul>"}, {"location": "philosophy/#getting-started", "title": "Getting Started", "text": "<p>Ready to experience these principles in action?</p> <ol> <li>Install DBSnapper to get started with the CLI tool</li> <li>Follow the Quick Start Guide to create your first sanitized snapshot</li> <li>Explore DBSnapper Cloud for team collaboration features</li> </ol> <p>Learn more about how DBSnapper works or dive into the configuration options to customize the tool for your specific needs.</p>"}, {"location": "quick-start/", "title": "Quick Start Guide", "text": "<p>This guide will walk you through creating your first database snapshot with DBSnapper in just a few minutes. You'll learn how to set up the agent, configure a target database, and create a sanitized snapshot for development use.</p>"}, {"location": "quick-start/#what-youll-accomplish", "title": "What You'll Accomplish", "text": "<p>By the end of this guide, you will have:</p> <ul> <li>\u2705 Configured the DBSnapper Agent</li> <li>\u2705 Connected to a database (source)</li> <li>\u2705 Created your first database snapshot</li> <li>\u2705 Loaded the snapshot into a development database</li> </ul>"}, {"location": "quick-start/#prerequisites", "title": "Prerequisites", "text": "<p>Before starting, ensure you have:</p> <ul> <li>DBSnapper Agent installed - See the installation guide for setup instructions</li> <li>A source database - PostgreSQL or MySQL database with sample data</li> <li>A destination database - Where you'll load the snapshot (can be empty)</li> <li>Database tools - Either installed locally or Docker available for containerized tools</li> </ul> <p>Need a Test Database?</p> <p>If you don't have a database ready, consider using the PostgreSQL <code>dvdrental</code> sample database or MySQL <code>sakila</code> database for testing.</p>"}, {"location": "quick-start/#option-1-container-based-quick-start", "title": "Option 1: Container-Based Quick Start", "text": "<p>The fastest way to get started is using the DBSnapper container with DBSnapper Cloud:</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock \\\n  -e DBSNAPPER_SECRET_KEY=your_secret_key \\\n  -e DBSNAPPER_AUTHTOKEN=your_auth_token \\\n  --rm --network dbsnapper --pull always \\\n  ghcr.io/dbsnapper/dbsnapper:latest \\\n  dbsnapper build your-cloud-target\n</code></pre> <p>This approach requires:</p> <ol> <li>A DBSnapper Cloud account with configured targets</li> <li>Your secret key and auth token from the cloud dashboard</li> </ol>"}, {"location": "quick-start/#option-2-local-configuration-setup", "title": "Option 2: Local Configuration Setup", "text": "<p>For local development or when you want full control over configuration:</p>"}, {"location": "quick-start/#step-1-initialize-configuration", "title": "Step 1: Initialize Configuration", "text": ""}, {"location": "quick-start/#initialize-the-configuration-file", "title": "Initialize the configuration file", "text": "<p>Run the <code>config init</code> command to create an example configuration at <code>~/.config/dbsnapper/dbsnapper.yml</code></p> <pre><code>dbsnapper config init\n</code></pre> <p>Configuration file initialized to default values</p> <pre><code>secret_key: d3d234bc83dd4efe7b7329855ba0acc2\nworking_directory: /Users/snappy/.dbsnapper\ndocker:\n  images:\n    postgres: postgres:latest\n</code></pre>"}, {"location": "quick-start/#check-the-configuration-and-environment", "title": "Check the configuration and environment", "text": "<p>Next, we can check our configuration and required dependencies. This runs some checks to verify the configuration file is valid and reports on the database tools found in the path as well as Docker engine and database image availability.</p> <pre><code>dbsnapper config check\n</code></pre> <p><code>dbsnapper config check</code> output</p> <pre><code>Checking DBSnapper Configuration\n  \u2705 Config file ( /Users/snappy/app/dbsnapper/cli/dbsnapper.yml ) found and loaded\n  \ud83d\udd35 Postgres Local Engine (pglocal)\n    \u2705 psql found at /Applications/Postgres.app/Contents/Versions/latest/bin/psql\n    \u2705 pg_dump found at /Applications/Postgres.app/Contents/Versions/latest/bin/pg_dump\n    \u2705 pg_restore found at /Applications/Postgres.app/Contents/Versions/latest/bin/pg_restore\n  \ud83d\udd35 MySQL Local Engine (mylocal)\n    \u2705 mysqldump found at /opt/homebrew/bin/mysqldump\n    \u2705 mysql found at /opt/homebrew/bin/mysql\n  \ud83d\udd35 Postgres Docker Engine (pgdocker)\n    \u2705 Docker client connected\n    \u2705 docker.images set in config file\n    \u2705 docker.images.postgres set in config file\n      \u2705 Found Docker image: postgres:latest\n  \ud83d\udd35 Mysql Docker Engine (mydocker)\n    \u2705 Docker client connected\n    \u2705 docker.images set in config file\n    \u2705 docker.images.mysql set in config file\n      \u2705 Found Docker image: mysql:8.0-oracle\n  \u2705 All supported database engines configured\n  \u2705 DBSnapper Cloud connected\n\n  \u2705 Configuration OK\n</code></pre>"}, {"location": "quick-start/#step-3-configure-database-targets", "title": "Step 3: Configure Database Targets", "text": "<p>Add database connection details to your configuration. A \"target\" defines both source and destination databases.</p> <p>Open your configuration file (<code>~/.config/dbsnapper/dbsnapper.yml</code>) and add a target definition:</p> <pre><code>targets:\n  my_app:\n    snapshot:\n      src_url: postgresql://postgres:postgres@localhost:5432/production_app?sslmode=disable\n      dst_url: postgresql://postgres:postgres@localhost:5432/dev_app?sslmode=disable\n</code></pre> <p>Database Safety</p> <p>The destination database (<code>dst_url</code>) will be completely dropped and recreated when loading a snapshot. Never use a production database as a destination.</p> <p>Connection Examples</p> <ul> <li>PostgreSQL: <code>postgresql://user:password@host:port/database?sslmode=disable</code></li> <li>MySQL: <code>mysql://user:password@host:port/database</code></li> </ul>"}, {"location": "quick-start/#step-4-verify-target-configuration", "title": "Step 4: Verify Target Configuration", "text": "<p>List all configured targets and verify connectivity:</p> <pre><code>dbsnapper targets\n</code></pre> <p>This command displays all targets with their connection status and database sizes. The DBSnapper UI provides a clear overview:</p> <p></p>"}, {"location": "quick-start/#step-5-create-your-first-snapshot", "title": "Step 5: Create Your First Snapshot", "text": "<p>Build a snapshot of your source database:</p> <pre><code>dbsnapper build my_app\n</code></pre> <p>This command:</p> <ul> <li>Connects to your source database using the configured <code>src_url</code></li> <li>Creates a database dump using native tools (<code>pg_dump</code> for PostgreSQL, <code>mysqldump</code> for MySQL)</li> <li>Stores the snapshot in your working directory</li> <li>Optionally uploads to cloud storage if configured</li> </ul> <p>Snapshot Naming</p> <p>Snapshots are automatically timestamped and indexed. You can also add custom tags or descriptions.</p>"}, {"location": "quick-start/#step-6-view-available-snapshots", "title": "Step 6: View Available Snapshots", "text": "<p>List all snapshots for a specific target:</p> <pre><code>dbsnapper target my_app\n</code></pre> <p>This shows detailed information about each snapshot including size, creation time, and status:</p> <p></p>"}, {"location": "quick-start/#step-7-load-a-snapshot", "title": "Step 7: Load a Snapshot", "text": "<p>Load the most recent snapshot (index 0) into your destination database:</p> <pre><code>dbsnapper load my_app 0\n</code></pre> <p>Destructive Operation</p> <p>The destination database will be completely dropped and recreated. Ensure you're not targeting a production database.</p>"}, {"location": "quick-start/#congratulations", "title": "Congratulations! \ud83c\udf89", "text": "<p>You've successfully created and loaded your first database snapshot with DBSnapper. Your development database now contains a sanitized copy of your production data.</p>"}, {"location": "quick-start/#next-steps", "title": "Next Steps", "text": "<p>Now that you have the basics working, explore these advanced features:</p>"}, {"location": "quick-start/#data-sanitization", "title": "Data Sanitization", "text": "<p>Learn how to remove sensitive information from your snapshots:</p> <ul> <li>Sanitization Overview - Understanding data sanitization concepts</li> <li>Configure Sanitization Rules - Set up automated PII removal</li> </ul>"}, {"location": "quick-start/#data-subsetting", "title": "Data Subsetting", "text": "<p>Reduce snapshot size by including only relevant data:</p> <ul> <li>Subset Configuration - Create smaller, focused snapshots</li> </ul>"}, {"location": "quick-start/#team-collaboration", "title": "Team Collaboration", "text": "<p>Share snapshots securely with your team:</p> <ul> <li>DBSnapper Cloud - Central snapshot management</li> <li>Storage Profiles - Configure cloud storage</li> </ul>"}, {"location": "quick-start/#automation-integration", "title": "Automation &amp; Integration", "text": "<p>Integrate DBSnapper into your workflows:</p> <ul> <li>GitHub Actions - Automated CI/CD snapshots</li> <li>VS Code Extension - In-editor snapshot management</li> <li>Terraform Provider - Infrastructure as Code</li> </ul>"}, {"location": "quick-start/#common-issues-troubleshooting", "title": "Common Issues &amp; Troubleshooting", "text": "<p>Connection Problems: If <code>dbsnapper targets</code> shows connection errors, verify:</p> <ul> <li>Database credentials and network connectivity</li> <li>Database server is running and accepting connections</li> <li>SSL/TLS settings match your database configuration</li> </ul> <p>Missing Tools: If <code>dbsnapper config check</code> reports missing tools:</p> <ul> <li>Install database tools locally (<code>postgresql-client</code>, <code>mysql-client</code>)</li> <li>Or use Docker-based engines (recommended for consistency)</li> </ul> <p>Permission Errors: Ensure the DBSnapper user has:</p> <ul> <li><code>SELECT</code> permissions on source database</li> <li><code>CREATE DATABASE</code> permissions on destination server</li> </ul>"}, {"location": "quick-start/#get-help", "title": "Get Help", "text": "<ul> <li>Configuration Reference - Complete configuration options</li> <li>Command Reference - Full CLI documentation</li> <li>GitHub Issues - Report bugs or request features</li> <li>Community Support - Get help from the community</li> </ul>"}, {"location": "release-notes/", "title": "Release Notes", "text": "<p>Full release notes</p> <p>For a complete list of changes, see the DBSnapper Releases Page which will include all changes, bug fixes, and enhancements.</p>"}, {"location": "release-notes/#v270-team-sharing-for-sso-groups", "title": "v2.7.0 - Team Sharing for SSO Groups", "text": "<p>This release introduces the ability to share targets and snapshots with your team members via their assigned groups in their SSO provider (currently Okta). You can specify the groups on the Targets page and the agent will include these targets and snapshots for any members of the group.</p> <p>Other Changes:</p> <ul> <li>Using presigned URLs for snapshot uploads and downloads from cloud storage.</li> <li>Added flags <code>--original</code> and <code>--destdb</code> to the <code>targets</code> and <code>target</code> commands so these can be used when <code>pull</code>ing or <code>load</code>ing snapshots in the terminal UI.</li> <li>Specify a default destination database (used for loading shared targets) in the configuration file like so:</li> </ul> <pre><code>defaults:\n  shared_target_dst_db_url: postgres://postgres:postgres@localhost:5432/dbsnapper_dst_db_default\n</code></pre>"}, {"location": "release-notes/#v260-overrides-sanitization-query", "title": "v2.6.0 - Overrides: Sanitization Query", "text": "<p>You may now specify a sanitization query that can be used to override all sanitization operations. This can be provided in the configuration file or as an environment variable and must be base-64 encoded:</p> <pre><code>override:\n  san_query: RFJPUCBUQUJMRSBJRiBFWElTVFMgZGJzbmFwcGVyX2luZm87CkNSRUFURSBUQUJMRSBkYnNuYXBwZXJfaW5mbyAoY3JlYXRlZF9hdCB0aW1lc3RhbXAsIHRhZ3MgdGV4dCBbXSk7CklOU0VSVCBJTlRPIGRic25hcHBlcl9pbmZvIChjcmVhdGVkX2F0LCB0YWdzKQpWQUxVRVMgKE5PVygpLCAne3F1ZXJ5OnNhbl9xdWVyeV9vdmVycmlkZSwgbG9jYXRpb246Y2xvdWR9Jyk7\n</code></pre> <p>Or via an environment variable:</p> <pre><code>DBSNAPPER_OVERRIDE__SAN_QUERY=RFJPUCBUQUJMRSBJRiBFWElTVFMgZGJzbmFwcGVyX2luZm87CkNSRUFURSBUQUJMRSBkYnNuYXBwZXJfaW5mbyAoY3JlYXRlZF9hdCB0aW1lc3RhbXAsIHRhZ3MgdGV4dCBbXSk7CklOU0VSVCBJTlRPIGRic25hcHBlcl9pbmZvIChjcmVhdGVkX2F0LCB0YWdzKQpWQUxVRVMgKE5PVygpLCAne3F1ZXJ5OnNhbl9xdWVyeV9vdmVycmlkZV9FTlYsIGxvY2F0aW9uOmNsb3VkfScpOw== dbsnapper sanitize dvdrental-san -n\n</code></pre> <p>Note: Remember to base-64 encode the query before providing it in the configuration file or as an environment variable.</p>"}, {"location": "release-notes/#v253-detect-terminal-tty", "title": "v2.5.3 - Detect Terminal (TTY)", "text": "<ul> <li>TTY detection has been added to the <code>targets</code> and <code>target</code> commands. If a TTY is detected, the interactive Terminal UI will load, otherwise a non-interactive ascii table will be displayed.</li> </ul>"}, {"location": "release-notes/#v252-bugfixes", "title": "v2.5.2 - Bugfixes", "text": "<ul> <li>Fix parsing snapshot filename with multiple underscores.</li> </ul>"}, {"location": "release-notes/#v251-decrypt-authtoken-when-provided-via-environment-bugfixes", "title": "v2.5.1 - Decrypt Authtoken When Provided via Environment + bugfixes", "text": "<ul> <li>Will now attempt to decrypt the authtoken if an encrypted authtoken is provided via the DBSNAPPER_AUTHTOKEN environment variable.</li> </ul>"}, {"location": "release-notes/#v250-docker-in-docker-changes-environment-variables-destination-db-override-flag", "title": "v2.5.0 - Docker in Docker Changes + Environment Variables + Destination DB Override Flag", "text": "<p>This release fixes some issues when running DBSnapper agent in a Docker-in-Docker (DinD) environment and adds some additional nice features.</p>"}, {"location": "release-notes/#docker-in-docker-ephemeral-sanitization-changes", "title": "Docker in Docker Ephemeral Sanitization Changes", "text": "<p>This was caused by limitations in the way Docker manages DinD containers and directory mounts. In summary - when creating a container to do dumps and restores in a DinD environment, the mountpoint was relative to the Host and not the DinD container. This caused the container to not be able to access the mounted directory. We now use the local database tools which are installed in the dbsnapper agent container to do the dumps and restores, avoiding this issue.</p> <p>The ephemeral container is still started as normal in a DinD environment and will be placed in the <code>dbsnapper</code> network. Using a non-default Docker network is necessary to ensure the Docker internal DNS resolver is used to resolve container hostnames correctly.</p>"}, {"location": "release-notes/#notes-ephemeral-sanitization-in-dind-environments", "title": "Notes - Ephemeral sanitization in DinD environments:", "text": "<ul> <li>DBSnapper, by default, specifies <code>dbsnapper</code> for the docker network for any containers created.</li> <li>If you are running the DBSnapper docker image and planning to do an ephemeral sanitization, you'll need to</li> <li>Mount the docker socket to the container <code>-v /var/run/docker.sock:/var/run/docker.sock</code></li> <li>Specify the <code>dbsnapper</code> network for the container <code>--network dbsnapper</code></li> </ul>"}, {"location": "release-notes/#environment-variables", "title": "Environment Variables", "text": "<p>This release also improves handling of environment variables. You can now execute the docker container without the need to initialize the configuration file first. You can pass the environment variables directly to the docker run command. For example:</p>"}, {"location": "release-notes/#interactive-agent-example", "title": "Interactive Agent Example", "text": "<pre><code>docker run -it -e DBSNAPPER_SECRET=XXX -e DBSNAPPER_AUTHTOKEN=YYY -v /var/run/docker.sock:/var/run/docker.sock --rm --network dbsnapper --pull always ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n</code></pre> <p>This will start the DBSnapper agent container in interactive mode with the following settings:</p> <ul> <li>With the <code>DBSNAPPER_SECRET</code> and <code>DB_SNAPPER_AUTHTOKEN</code> environment variables set. Both are required to execute the agent without a configuration file.</li> <li>With the docker socket mounted to the container <code>-v /var/run/docker.sock:/var/run/docker.sock</code> (for ephemeral sanitization)</li> <li>In the <code>dbsnapper</code> network <code>--network dbsnapper</code></li> <li>Will pull the latest DBSnapper image <code>--pull always</code></li> <li>Will remove the container after it exits <code>--rm</code></li> </ul>"}, {"location": "release-notes/#one-liner-snapshot-example", "title": "One-Liner Snapshot Example", "text": "<p>A one-liner that will launch the DBSnapper agent container, build a snapshot, upload it to the cloud, and exit:</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -e DBSNAPPER_SECRET_KEY=XXX -e DBSNAPPER_AUTHTOKEN=YYY --rm --network dbsnapper  --pull always ghcr.io/dbsnapper/dbsnapper:latest dbsnapper build dvdrental-cloud\n</code></pre> <p>This command will output:</p> <pre><code>Creating working directory at: /root/.dbsnapper\nDBSnapper Agent - Version: 2.5.0\nDBSnapper Cloud: Enabled\n\nSTART: Build Snapshot for target: dvdrental-cloud with engine: postgres-local\n--&gt; Zipping snapshot to /root/.dbsnapper/1715957602_dvdrental-cloud.zip\n--&gt; Uploading snapshot to Cloud Storage Profile: cloudflare-r2 - r2://dbsnapper-r2/dbs-production-bucket/e9b33f60-d791-4d73-b9d2-30f9adad2fde.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: e9b33f60-d791-4d73-b9d2-30f9adad2fde\nFINISH: Building DB Snapshot for target: dvdrental-cloud\n</code></pre>"}, {"location": "release-notes/#one-liner-sanitize-example", "title": "One-Liner Sanitize Example", "text": "<p>A one-liner that will launch the DBSnapper agent container, create a new original and sanitized snapshot set, upload them to cloud storage, and exit:</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -e DBSNAPPER_SECRET_KEY=XXX -e DBSNAPPER_AUTHTOKEN=YYY  --rm --network dbsnapper --pull always ghcr.io/dbsnapper/dbsnapper:latest dbsnapper sanitize dvdrental-cloud -n\n</code></pre> <p>Which outputs:</p> <pre><code>latest: Pulling from dbsnapper/dbsnapper\nDigest: sha256:2d0b3c054942bdea88bde16e8b49a69d1249a0ea20b78b69508b23e4ee23f92d\nStatus: Image is up to date for ghcr.io/dbsnapper/dbsnapper:latest\nCreating working directory at: /root/.dbsnapper\nDBSnapper Agent - Version: 2.5.0 (c2d2ec052fe4) Build Date: 2024-05-17T16:21:43Z\nDBSnapper Cloud: Enabled\n\nRunning ephemeral sanitization\nCreating a new snapshot set\n--&gt; Building NEW original snapshot\n--&gt; Zipping snapshot to /root/.dbsnapper/1715963288_dvdrental-san.zip\n--&gt; Created ephemeral container: animal-belief, database: pgdocker://dbsnapper:dbsnapper@nutrition/troupe\n--&gt; Restoring snapshot to Sanitization DB\n--&gt; Sanitizing Snapshot\n--&gt; Building sanitized snapshot\n--&gt; Pushing original snapshot to cloud\n--&gt; Uploading snapshot to Cloud Storage Profile: R2-Original - r2://dbsnapper/original/0bd58a2a-4db1-49a8-8917-f513d829c31b.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: 0bd58a2a-4db1-49a8-8917-f513d829c31b\n--&gt; Pushing sanitized snapshot to cloud\n--&gt; Uploading snapshot to Cloud Storage Profile: R2-Sanitized - r2://dbsnapper/sanitized/cb062d1f-cf2d-4bae-a1b0-489853e5a500.san.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: cb062d1f-cf2d-4bae-a1b0-489853e5a500\nSanitize Complete\n</code></pre>"}, {"location": "release-notes/#environment-variables-for-non-docker-environments", "title": "Environment Variables for Non-Docker Environments", "text": "<p>Of course, you can always use envirnoment variables when running the DBSnapper agent in a non-Docker environment as well.</p> <pre><code>DBSNAPPER_SECRET_KEY=XXX DBSNAPPER_AUTHTOKEN=YYY dbsnapper targets\n</code></pre> <p>This will list all targets using the specified secret key and authtoken. If you don't have a configuration file, it will end up listing all targets created in the DBSnapper cloud.</p>"}, {"location": "release-notes/#destination-database-override-flag", "title": "Destination Database Override Flag", "text": "<p>The <code>load</code> command now supports a <code>--destdb</code> flag that can be used to override the snapshot destination database specified in the target configuration. The following example overrides the database set for the <code>target-name</code> target with the <code>postgres://postgres:postgres@localhost:5432/database_snapshot_override</code> database:</p> <pre><code>dbsnapper load target-name --destdb=\"postgres://postgres:postgres@localhost:5432/database_snapshot_override\"\n</code></pre>"}, {"location": "release-notes/#v240-ephemeral-sanitization-support", "title": "v2.4.0 - Ephemeral Sanitization Support", "text": "<p>This release is bringing back the ability to use ephemeral containers for sanitization. This streamlines the sanitization process, leveraging containers to spin up a temporary database that can be used to sanitize the unsanitized snapshot data.</p> <p>The <code>sanitize</code> command now behaves as follows:</p> <ol> <li>It will create a new unsanitized and sanitized snapshot set if no snapshots exist for a target or the <code>-n</code> flag is set.</li> <li>Will use an ephemeral container to sanitize the data if the <code>sanitize: dst_url</code> is not specified in the configuration file, or the <code>-e</code> flag is set. </li> </ol> <p>You can combine both the <code>-n</code> and <code>-e</code> flags to create a new snapshot set and use an ephemeral container for sanitization.</p>"}, {"location": "release-notes/#v230-new-user-interface-share-targets-storage-engines-improvements-and-more", "title": "v2.3.0 - New User Interface, Share Targets, Storage Engines Improvements, and More", "text": "<p>A Terminal User Interface (TUI) has been added to the DBSnapper Agent, making it even easier to use. See all your targets, drill down into their snapshots, and load them all from the new UI.</p> <p> DBSnapper Agent User Interface - All Targets </p> <p>Sharing Targets have been added to DBSnapper. Leveraging the ability to specify different storage profiles for original and sanitized snapshots, you can now create a share target in your configuration file, that will allow you to list and load sanitized snapshots from a shared storage location. This is useful for sharing sanitized snapshots with developers, testers, and other stakeholders.</p> <p>New Storage Engines have been added. In addition to our support for AWS S3 and CloudFlare R2, we have added support for Minio and Digital Ocean Spaces</p> <p>Storage Engines now support retrieving credentials from  the AWS CLI shared configuration. It is now possible to retrieve S3 compatible storage engine credentials from environment variables, or you can specify an <code>awscli_profile</code> in your storage profile configuration to use the credentials from the specified AWS CLI profile. More information on this can be found in the Storage Engine Configuration documentation.</p>"}, {"location": "release-notes/#v220-separate-storage-profiles-for-unsanitized-and-sanitized-snapshots", "title": "v2.2.0 - Separate Storage Profiles for Unsanitized and Sanitized Snapshots", "text": "<p>You can now specify different storage profiles for unsanitized (original) and sanitized snapshots, allowing you to store them in different buckets or cloud providers if desired.  This will allow sharing only the sanitized snapshot cloud storage buckets with developers, while keeping the unsanitized snapshots private.</p> <p>Up next is additional sharing functionality for accessing and loading the sanitized snapshots.</p> <p>Download the v2.2.0 release for your platform.</p>"}, {"location": "release-notes/#v210-connection-string-url-templates", "title": "v2.1.0 - Connection String URL Templates", "text": "<p>All connection string URLs now support templating. This allows you to access environment variables in the connection string URLs. For example, you can now use the following connection string URL for a Postgres database:</p> <pre><code>snapshot:\n  src_url: postgres://{{`DB_USER` | env}}:{{`DB_PASSWORD` | env}}@localhost:5432/{{`DB_NAME` | env}}\n</code></pre> <p>In this example we are indicating we want the username, password, and database name to be read from the <code>DB_USER</code>, <code>DB_PASSWORD</code>, and <code>DB_NAME</code> environment variables, respectively.</p> <p>Templates conform to Go Templates syntax. Specify the <code>env</code> function to read the value from the environment.</p> <pre><code>{{`ENV_VAR` | env}} # substitute the value of the ENV_VAR environment variable\n{{`CONSTANT`}} # substitute the supplied `CONSTANT` value\n</code></pre>"}, {"location": "release-notes/#v200-subsetting", "title": "v2.0.0 - Subsetting!", "text": "<p>We're excited to announce the release of DBSnapper v2.0, which introduces a major new feature: Database Subsetting. This feature allows you to create a relationally consitent copy of your database that contains only a subset of the data. This is useful for creating smaller, more manageable datasets for development and testing.</p> <p>Backwards Compatibility</p> <p>This release introduces a new configuration file format and options. If you are upgrading from a previous version, you will need to update your configuration file to the new format. See the Configuration Settings documentation for more information.</p>"}, {"location": "release-notes/#additional-improvements", "title": "Additional Improvements", "text": "<ul> <li>Improved support for MySQL databases.</li> <li>Support for PostgreSQL COPY protocol for fast data copy operations.</li> <li>Simplified the sanitization command, eliminating the use of ephemeral database containers,</li> <li>Released Docker images for easier installation and use.</li> <li>An extensive refactoring and testing of the codebase to improve performance, quality, and maintainability.</li> <li>Improved documentation and examples.</li> </ul>"}, {"location": "requirements/", "title": "Requirements", "text": "<p>Below are the requirements for the DBSnapper Agent.</p>"}, {"location": "requirements/#supported-platforms", "title": "Supported Platforms", "text": "<p>You can find the latest builds at the DBSnapper Agent Releases page.</p> <ul> <li> <p>MacOS: Latest versions of MacOS are supported.</p> </li> <li> <p>Linux: Full compatibility with various distributions of Linux, catering to a wide range of Linux-based environments.</p> </li> </ul>"}, {"location": "requirements/#supported-databases", "title": "Supported Databases", "text": "<p>DBSnapper leverages the native database utilities for snapshot creation and restoration. It can access these tools either directly from the host system's path or through Docker container images, offering flexibility in how you manage your database environments.</p> <p>The following databases are supported at this time:</p> <ul> <li> <p>PostgreSQL: DBSnapper supports both local and Docker-based PostgreSQL databases.</p> </li> <li> <p>MySQL: DBSnapper supports both local and Docker-based MySQL databases.</p> </li> </ul> <p>See the Database Engines documentation for more information on how to configure and use these database engines.</p>"}, {"location": "requirements/#supported-cloud-storage-providers", "title": "Supported Cloud Storage Providers", "text": "<p>The DBSnapper Cloud extends the capabilities of the DBSnapper Agent by providing a support for cloud storage in a \"Bring Your Own Cloud Storage\" arrangement. The DBSnapper Agent supports the following cloud storage providers:</p> <ul> <li> <p>Amazon S3: DBSnapper supports the Amazon Simple Storage Service (S3) cloud storage platform.</p> </li> <li> <p>Cloudflare R2: DBSnapper supports Cloudflare R2 cloud storage platform.</p> </li> </ul>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/", "title": "Backup Amazon RDS Databases with DBSnapper", "text": "<p>DBSnapper is a powerful tool for creating and managing backups of your databases. It is designed to be easy to use, and can be installed and configured in minutes. In this tutorial, we'll show you how to use DBSnapper to backup an Amazon RDS database deployed in a private VPC.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#overview", "title": "Overview", "text": "<p>In this tutorial we'll be using the following tools to backup the Amazon RDS database:</p> <ul> <li>DBSnapper Agent to facilitate the backup process</li> <li>DBSnapper Cloud to provide the target configuration and snapshot storage</li> <li>Docker to launch a DBSnapper container with the necessary database client tools.</li> <li>Amazon EC2 instance to connect to the Amazon RDS database</li> </ul> <p>Here's our target definition on the DBSnapper Cloud. We've simply provided the connection string of the Amazon RDS database as the source URL, and a Cloudflare R2 storage profile as the destination URL.</p> <p></p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#tldr-backup-in-four-commands", "title": "TL;DR Backup in Four Commands", "text": "<p>Update: You can now do this in a SINGLE command</p> <p>(copied from the DBSnapper Quick Start Guide)</p> <p>Just use the latest version of the DBSnapper container image, provide the minimum required <code>DBSNAPPER_SECRET_KEY</code> and <code>DBSNAPPER_AUTHTOKEN</code> environment variables necessary to run DBSnapper in Cloud mode, without a configuration file.</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock -e DBSNAPPER_SECRET_KEY=XXX -e DBSNAPPER_AUTHTOKEN=YYY --rm --network dbsnapper --pull always ghcr.io/dbsnapper/dbsnapper:latest dbsnapper build dvdrental-cloud\n</code></pre> <p>This command does the following:</p> <ol> <li>It pulls the latest version of the DBSnapper Agent container image from the GitHub Container Registry,</li> <li>It mounts the host Docker socket. This is optional, but necessary for ephemeral sanitization operations requiring Docker-in-Docker (DinD) support.</li> <li>Passes the <code>DBSNAPPER_SECRET_KEY</code> and <code>DBSNAPPER_AUTHTOKEN</code> environment variables to the container</li> <li>Runs the <code>dbsnapper build dvdrental-cloud</code> command to build a snapshot of the <code>dvdrental-cloud</code> target defined in the DBSnapper Cloud.</li> <li>If a storage profile is defined in the DBSnapper Cloud, the snapshot will be uploaded to the configured storage provider.</li> </ol> Amazon RDS Backup in four commands<pre><code># Launch interactive DBSnapper Container from EC2 instance\ndocker run -it ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n\n# Create our configuration and connect to the DBSnapper Cloud\ndbsnapper config init\ndbsnapper auth token &lt;YOUR_DBSNAPPER_CLOUD_AUTH_TOKEN&gt;\n\n# Build the snapshot and upload to object storage\ndbsnapper build amazon-rds-postgres\n</code></pre>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#launch-an-ec2-instance", "title": "Launch an EC2 Instance", "text": "<p>To easily gain access to the Amazon RDS database, we'll launch an EC2 instance in the same VPC as the RDS database. This will allow us to run DBSnapper and connect to the RDS database without exposing the database to the public internet.</p> <p>For this instance I am using the <code>Canonical, Ubuntu, 22.04 LTS, arm64 (ami-05d47d29a4c2d19e1)</code> image. But you can use other Linux distributions as well.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#launch-an-interactive-dbsnapper-container", "title": "Launch an Interactive DBSnapper Container", "text": "<p>The DBSnapper Docker Images include the necessary database client tools for PostgreSQL and MySQL, so it is a quick way to get started with all the necessary dependencies.</p> <p>Docker Dependency</p> <p>This tutorial uses Docker to launch the container - if you need to install Docker, see the Detour: Install Docker Community Edition section below.</p> Launch DBSnapper Container &amp; Setup DBSnapper Agent<pre><code># Launch interactive DBSnapper Container\ndocker run -it ghcr.io/dbsnapper/dbsnapper:latest /bin/bash\n\n# Create our configuration and connect to the DBSnapper Cloud\ndbsnapper config init\ndbsnapper auth token &lt;YOUR_DBSNAPPER_CLOUD_AUTH_TOKEN&gt;\ndbsnapper targets\n</code></pre> <p>Using the commands above, we will launch an interactive DBSnapper container, initialize the DBSnapper configuration file, and authenticate with the DBSnapper Cloud using the authentication token provided on the Get Started page, and finally list the available targets.</p> Command output<pre><code>1c09173ca85b:~# dbsnapper config init\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Standalone Mode\n\nInitializing dbsnapper config at       : /root/.config/dbsnapper/dbsnapper.yml\nCreating dbsnapper working directory at: /root/.dbsnapper\n\n1c09173ca85b:~# dbsnapper auth token &lt;YOUR_DBSNAPPER_CLOUD_AUTH_TOKEN&gt;\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Standalone Mode\n\nCloud API token written to configuration file: /root/.config/dbsnapper/dbsnapper.yml\n\n1c09173ca85b:~# dbsnapper targets\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Enabled\n\nListing all targets\n+---------------------+----------+--------+------------------------------------------------------+------+-----+-----------------+-------+----------+\n|        NAME         | LOCATION | STATUS |                         SRC                          | SIZE | DST | STORAGE PROFILE | QUERY | MESSAGES |\n+---------------------+----------+--------+------------------------------------------------------+------+-----+-----------------+-------+----------+\n| amazon-rds-postgres | cloud    |        | pgl://dbsna...amazonaws.com:5432/rds_backup_tutorial | 0 B  |     | Cloudflare R2   | No    |          |\n+---------------------+----------+--------+------------------------------------------------------+------+-----+-----------------+-------+----------+\n</code></pre> <p>With three simple DBSnapper commands (highlighted) we've initialized the configuration file, authenticated with the DBSnapper Cloud, and listed the available targets. We see our Amazon RDS database listed as the <code>amazon-rds-postgres</code> target.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#backup-the-amazon-rds-database", "title": "Backup the Amazon RDS Database", "text": ""}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#build-the-snapshot", "title": "Build the Snapshot", "text": "<p>Now that we have the DBSnapper container running and have authenticated with the DBSnapper Cloud, we can simply use the <code>build</code> command to create a snapshot of the Amazon RDS database.</p> Backup Amazon RDS Database<pre><code>dbsnapper build amazon-rds-postgres\n</code></pre> Command Output<pre><code>1c09173ca85b:~# dbsnapper build amazon-rds-postgres\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Enabled\n\nSTART: Build Snapshot for target: amazon-rds-postgres with engine: postgres-local\n--&gt; Cloud target, Cloud storage, non-localhost DB.\n--&gt; Zipping snapshot 1708621936_amazon-rds-postgres to /root/.dbsnapper/1708621936_amazon-rds-postgres.zip\n--&gt; Uploading snapshot to Cloud Storage Profile: Cloudflare R2 - r2://dbsnapper-r2/dbsnapper-playground/cc6e21cb-e591-4a0e-8166-eba3c2a98dca.zip\n--&gt; Upload complete.\n--&gt; Local snapshot entry stored in cloud, ID: cc6e21cb-e591-4a0e-8166-eba3c2a98dca\nFINISH: Building DB Snapshot for target: amazon-rds-postgres\n</code></pre> <p>And there you have it. We've backed up the Amazon RDS database and it is now available in your cloud storage and can be retrieved and loaded by the DBSnapper Agent on any machine with access to the your DBSnapper Cloud account.</p>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#list-the-snapshot", "title": "List the Snapshot", "text": "<p>We can verify that the snapshot was created by listing the snapshots for the target.</p> List Snapshots for the Target<pre><code>1c09173ca85b:~# dbsnapper target amazon-rds-postgres\nDBSnapper Agent - Version: 2.0.0 (60302b36df4d08b7c0a983c435d683e67373b235) Build Date: 2024-02-22T00:13:23Z\nDBSnapper Cloud: Enabled\n\nTables in target:  amazon-rds-postgres\n+---------------+------------+--------+\n|     NAME      | EST'D ROWS |  SIZE  |\n+---------------+------------+--------+\n| address       |        603 | 131 kB |\n| users         |          4 | 49 kB  |\n| store         |          2 | 41 kB  |\n| staff         |          2 | 33 kB  |\n| payment       |     14,596 | 1.9 MB |\n| film          |      1,000 | 934 kB |\n| inventory     |      4,581 | 451 kB |\n| film_category |      1,000 | 90 kB  |\n| language      |          6 | 25 kB  |\n| country       |        109 | 25 kB  |\n| film_actor    |      5,462 | 500 kB |\n| customer      |        599 | 188 kB |\n| city          |        600 | 90 kB  |\n| actor         |        200 | 49 kB  |\n| category      |         16 | 25 kB  |\n| rental        |     16,044 | 2.4 MB |\n+---------------+------------+--------+\nListing ALL snapshots for target: amazon-rds-postgres\n+-------+-------------------------+--------------------------------+------------------------------------------+--------+------------+-------+\n| INDEX |         CREATED         |              NAME              |                 FILENAME                 |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+--------------------------------+------------------------------------------+--------+------------+-------+\n|     0 | 2024-Feb-22 @ 17:12:17Z | 1708621936_amazon-rds-postgres | cc6e21cb-e591-4a0e-8166-eba3c2a98dca.zip | 613 kB | false      |       |\n+-------+-------------------------+--------------------------------+------------------------------------------+--------+------------+-------+\n</code></pre>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#optional-test-ec2-rds-connection", "title": "Optional: Test EC2 &lt;-&gt; RDS Connection", "text": "<p>To ensure we're able to connect to the Amazon RDS database from our EC2 Instance, we can use the <code>psql</code> command to connect to the database and list the available tables.</p> Connect to Amazon RDS Database<pre><code>psql -d postgres://postgres:&lt;PASSWORD&gt;dbsnapper-playground.&lt;RDS_HOSTNAME...&gt;:5432/rds_backup_tutorial\n\npsql (16.2, server 16.1)\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off)\nType \"help\" for help.\n\nrds_backup_tutorial=&gt; \\dt\n             List of relations\n Schema |     Name      | Type  |  Owner\n--------+---------------+-------+----------\n public | actor         | table | postgres\n public | address       | table | postgres\n public | category      | table | postgres\n public | city          | table | postgres\n public | country       | table | postgres\n public | customer      | table | postgres\n public | film          | table | postgres\n public | film_actor    | table | postgres\n public | film_category | table | postgres\n public | inventory     | table | postgres\n public | language      | table | postgres\n public | payment       | table | postgres\n public | rental        | table | postgres\n public | staff         | table | postgres\n public | store         | table | postgres\n public | users         | table | postgres\n(16 rows)\n\nrds_backup_tutorial=&gt;\n</code></pre>"}, {"location": "articles/backup-amazon-rds-with-dbsnapper/#optional-install-docker-community-edition", "title": "Optional: Install Docker Community Edition", "text": "<p>We're using Docker for this tutorial so if your instance doesn't have it installed, the following commands will install and setup Docker community edition in rootless mode on your instance.</p> Install Docker Community Edition on Ubuntu 22.04 LTS<pre><code>curl -fsSL https://get.docker.com -o get-docker.sh &amp;&amp; \\\nsudo sh ./get-docker.sh\n\nsudo sh -eux &lt;&lt;EOF\n# Install newuidmap &amp; newgidmap binaries\napt-get install -y uidmap\nEOF\n\ndockerd-rootless-setuptool.sh install\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/", "title": "Creating Sanitized Database Snapshots with DBSnapper", "text": "<p>Updated Feb. 21 2024 for v2.0</p> <p>This tutorial has been updated to reflect the changes in DBSnapper v2.0.</p> <p>In this example, we're going to create a simple database with User account information. Our goal is to be able to create sanitized snapshot of this database that we can use in our development</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#create-a-database-and-add-some-data", "title": "Create a database and add some data", "text": "<p>Let's create a database and call it <code>example_app</code></p> <pre><code>psql -d 'postgres://postgres:postgres@localhost:15432?sslmode=disable' -c 'create database example_app;'\n</code></pre> <p>We'll create a simple users table with some basic user and authentication fields</p> <pre><code>DROP TABLE IF EXISTS users;\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    first_name text,\n    last_name text,\n    email character varying(110) unique not null,\n    password character varying(50) not null,\n    pin character varying(8)\n);\n</code></pre> <p>And we'll load some sample data</p> <pre><code>INSERT INTO users (first_name, last_name, email, password, pin) VALUES\n('John', 'Doe', 'johndoe@example.com', 'secretpassword', '2468'),\n('Jane', 'Doe', 'janedoe@example.com', 'ubersecretpassword', '1357'),\n('Fred', 'Smith', 'fsmith@dbsnapper.com', 'opensesame', '7890'),\n('Sam', 'Jackson', 'sj@example.com', 'iamsam', '1234');\n</code></pre> <p>Our database so far</p> <pre><code>example_app=# select * from users;\n id | first_name | last_name |        email         |      password      | pin\n----+------------+-----------+----------------------+--------------------+------\n  1 | John       | Doe       | johndoe@example.com  | secretpassword     | 2468\n  2 | Jane       | Doe       | janedoe@example.com  | ubersecretpassword | 1357\n  3 | Fred       | Smith     | fsmith@dbsnapper.com | opensesame         | 7890\n  4 | Sam        | Jackson   | sj@example.com       | iamsam             | 1234\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#create-a-target-configuration", "title": "Create a target configuration", "text": "<p>Our goal is to create a snapshot of the production <code>example_app</code> database that we can copy locally to our development database. Let's do that:</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code>docker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:latest\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: /Users/joescharf/.dbsnapper\ntargets:\n  example_app:\n    name: example_app\n    snapshot:\n      src_url: postgres://postgres:postgres@localhost:15432/example_app?sslmode=disable\n      dst_url: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n    sanitize:\n      dst_url:\n      query_file:\n</code></pre> <p>As you can see in the highlighted section, we're specifying <code>example_app</code> as our source datbase and <code>example_app_snap</code> as our destination</p> <p>Danger: Destination database <code>dst_url</code> will be DROPPED and RECREATED</p> <p>Remember, the any database specified on a <code>dst_url</code> attribute will be DROPPED and a new empty database with the same name will be CREATED prior to loading the data from the snapshot!</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-our-target", "title": "Checking our target", "text": "<p>Let's make sure our target is configured properly and we can access it:</p> <pre><code>dbsnapper targets\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha-dev Build Date: 2024-02-21T16:20:40-07:00\nDBSnapper Cloud: Standalone Mode\n\nListing all targets\n+-------------+----------+--------+-----------------------------------+--------+----------------------------------------+-----------------+-------+----------+\n|    NAME     | LOCATION | STATUS |                SRC                |  SIZE  |                  DST                   | STORAGE PROFILE | QUERY | MESSAGES |\n+-------------+----------+--------+-----------------------------------+--------+----------------------------------------+-----------------+-------+----------+\n| example_app | local    | OK     | pgl://localhost:15432/example_app | 7.8 MB | pgl://localhost:15432/example_app_snap |                 | No    |          |\n+-------------+----------+--------+-----------------------------------+--------+----------------------------------------+-----------------+-------+----------+```\n</code></pre> <p>STATUS shows OK and no errors, so we're good to go.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#build-a-snapshot", "title": "Build a snapshot", "text": "<p>The first step in our journey is creating the snapshot. This can be done easily with the <code>build</code> command:</p> <pre><code>$ dbsnapper build example_app\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Build Snapshot for target: example_app with engine: postgres-local\n--&gt; Local target, Local storage, non-localhost DB.\n--&gt; Zipping snapshot 1708557745_example_app to /Users/joescharf/.dbsnapper/1708557745_example_app.zip\nFINISH: Building DB Snapshot for target: example_app\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-our-snapshot", "title": "Checking our snapshot", "text": "<p>Now that we've successfully built a snapshot of our <code>example_app</code> database, let's check the snapshots for this target:</p> <pre><code>$ dbsnapper target example_app\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nListing ALL snapshots for target: example_app\n+-------+-------------------------+------------------------+----------------------------+--------+------------+-------+\n| INDEX |         CREATED         |          NAME          |          FILENAME          |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+-------+\n|     0 | 2024-Feb-21 @ 16:32:11Z | 1708558331_example_app | 1708558331_example_app.zip | 1.4 kB | false      |       |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+-------+\n</code></pre> <p>Great, we have a single snapshot available. Note that the <code>SANFN</code> (Sanitized FileName) cell is empty, indicating that we don't have a sanitized version of this snapshot. That's ok, we'll deal with that later.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#load-our-snapshot", "title": "Load our snapshot", "text": "<p>Now that we have our snapshot, let's load it into our development database specified on <code>dst_url</code> of our target configuration.</p> <pre><code>$ dbsnapper load example_app 0\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Loading Snapshot\n-&gt; LOADING original Snapshot #0: Name: 1708558331_example_app, Snapshot File: 1708558331_example_app.zip, Dest DB URL: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n--&gt; Using engine: postgres-local\n--&gt; Using Target: example_app\n--&gt; Pulling to local file: /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Local snapshot already exists at /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Pulled snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Unzipping snapshot /Users/joescharf/.dbsnapper/1708558331_example_app.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-1871306128\n--&gt; Dropping and recreating database pgl://localhost:15432/example_app_snap\n-&gt; LOADING Snapshot Completed for Target: example_app\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-our-new-database", "title": "Checking our new database", "text": "<p>Another success. Let's switch to the database and take a look at the data in the <code>users</code> table:</p> <pre><code>postgres=# \\l\n                                                          List of databases\n             Name              |  Owner   | Encoding |  Collate   |   Ctype    | ICU Locale | Locale Provider |   Access privileges\n-------------------------------+----------+----------+------------+------------+------------+-----------------+-----------------------\n example_app                   | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n example_app_snap              | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n\npostgres=# \\c example_app_snap\npsql (15.6 (Postgres.app), server 15.2 (Debian 15.2-1.pgdg110+1))\nYou are now connected to database \"example_app_snap\" as user \"postgres\".\nexample_app_snap=# select * from users;\n id | first_name | last_name |        email         |      password      | pin\n----+------------+-----------+----------------------+--------------------+------\n  1 | John       | Doe       | johndoe@example.com  | secretpassword     | 2468\n  2 | Jane       | Doe       | janedoe@example.com  | ubersecretpassword | 1357\n  3 | Fred       | Smith     | fsmith@dbsnapper.com | opensesame         | 7890\n  4 | Sam        | Jackson   | sj@example.com       | iamsam             | 1234\n(4 rows)\n</code></pre> <p>Great. So far we've made an exact copy of our <code>example_app</code> source database which is now available in the <code>example_app_snap</code> database.</p> <p>But we have a problem. Passing around a database snapshot with all this personal information (PII) and sensitive authentication data (passwords, pins) is a security issue! Eventually something will git misplaced or misused, so we'll need to deal with that.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#sanitizing-and-de-identifying-the-snapshot", "title": "Sanitizing and de-identifying the snapshot", "text": "<p>The sanitization process takes a query and executes it against a database snapshot. The resulting changes to the snapshot are exported and stored in a sanitized snapshot file.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#create-the-query", "title": "Create the query", "text": "<p>Let's create a sanitization query that will</p> <ol> <li>Obfuscate <code>first_name</code> and <code>last_name</code></li> <li>Change the <code>email</code> to match the <code>first_name</code> and <code>last_name</code></li> <li>Change the <code>password</code> and <code>pin</code> for all users to a common password used for development</li> </ol> example_app.san.sql<pre><code>-- sanitize the first_name, last_name\nupdate users u\nset first_name = 'User',\nlast_name = 'Id' || id;\n\n-- sanitize the email\nupdate users u\nset email = first_name || '_' || last_name || '@example.com';\n\n-- sanitize password and pin\nupdate users u\nset password = 'genericpassword',\npin = '0000';\n</code></pre> <p>Let's also create a table that will record the time the sanitization was performed and the name of the query file used:</p> example_app.san.sql<pre><code>-- Add a dbsnapper_info table to record timestamp of sanitizaiton\n\nDROP TABLE IF EXISTS dbsnapper_info;\n\nCREATE TABLE dbsnapper_info (\n  created_at timestamp,\n  tags text[]\n);\n\nINSERT INTO dbsnapper_info (created_at, tags)\n  VALUES (NOW(), '{example_app.san.sql}');\n</code></pre> <p>And let's save this file as <code>example_app.san.sql</code> in our <code>working_directory</code> (which defaults to <code>~/.dbsnapper</code>)</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#update-the-target-configuration", "title": "Update the target configuration", "text": "<p>Now we need to update our target configuration and specify the <code>query_file</code> that will be used for sanitization along with the <code>dst_url</code> for the sanitized snapshot:</p> <p>Destination URLs and DBSnapper</p> <p>DBSnapper uses separate <code>dst_url</code> attributes for the <code>snapshot</code> <code>subset</code>, and <code>sanitize</code> operations. This allows you to specify different destination databases for the different DBSnapper operations.</p> ~/.config/dbsnapper/dbsnapper.yml<pre><code>docker:\n  images:\n    postgres: postgres:latest\nsecret_key: c614a689a559d1b517c28a5e4fcdc059\nworking_directory: /Users/joescharf/.dbsnapper\ntargets:\n  example_app:\n    name: example_app\n    snapshot:\n      src_url: postgres://postgres:postgres@localhost:15432/example_app?sslmode=disable\n      dst_url: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n    sanitize:\n      dst_url: postgres://postgres:postgres@localhost:15432/example_app_sanitized?sslmode=disable\n      query_file: \"example_app.san.sql\"\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#perform-the-sanitization", "title": "Perform the sanitization", "text": "<p>Now we're ready to run the <code>sanitize</code> command against the <code>example_app</code> target:</p> <pre><code>$ dbsnapper sanitize example_app 0\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Sanitize Snapshot #0, Name: 1708558331_example_app\n-&gt; LOADING original Snapshot #0: Name: 1708558331_example_app, Snapshot File: 1708558331_example_app.zip, Dest DB URL: postgres://postgres:postgres@localhost:15432/example_app_sanitized?sslmode=disable\n--&gt; Using engine: postgres-local\n--&gt; Using Target: example_app\n--&gt; Pulling to local file: /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Local snapshot already exists at /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Pulled snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.zip\n--&gt; Unzipping snapshot /Users/joescharf/.dbsnapper/1708558331_example_app.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-1231401554\n--&gt; Dropping and recreating database pgl://localhost:15432/example_app_sanitized\n-&gt; LOADING Snapshot Completed for Target: example_app\n--&gt; Executing sanitization query\n--&gt; Building sanitized snapshot\n--&gt; Zipping snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Sanitized snapshot '1708558331_example_app' created at /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n</code></pre> <p>This output shows the steps taken to sanitize the database which involve:</p> <ol> <li>Pulling the snapshot from the cloud or finding it locally on disk.</li> <li>Loading the snapshot into the database specified in the <code>dst_url</code> attribute.</li> <li>Running the sanitization query specified in <code>query_file</code> against the <code>dst_url</code> database.</li> <li>Dumping the sanitized database to a snapshot, compressing it, and uploading it to the cloud if configured.</li> </ol>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#check-the-sanitized-snapshot", "title": "Check the sanitized snapshot", "text": "<p>Now that we've sanitized the database, let's take another look at the snapshots available for the <code>example_app</code> target</p> <pre><code>$ dbsnapper target example_app\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nListing ALL snapshots for target: example_app\n+-------+-------------------------+------------------------+----------------------------+--------+------------+--------------------------------+\n| INDEX |         CREATED         |          NAME          |          FILENAME          |  SIZE  | SANITIZED? |             SANFN              |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+--------------------------------+\n|     0 | 2024-Feb-21 @ 16:32:11Z | 1708558331_example_app | 1708558331_example_app.zip | 1.4 kB | true       | 1708558331_example_app.san.zip |\n+-------+-------------------------+------------------------+----------------------------+--------+------------+--------------------------------+\n</code></pre> <p>The big difference here is that <code>1708558331_example_app.san.zip</code> is listed in the <code>SANFN</code> column, which indicates that we have a sanitized database snapshot available for this snapshot index.</p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#load-the-sanitized-snapshot", "title": "Load the sanitized snapshot", "text": "<p>Like we did above, we'll once again, load the snapshot to our <code>example_app_snap</code> snapshot database:</p> <pre><code>$ dbsnapper load example_app 0\n</code></pre> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha.2 (d534e0fcfacd632e5d117bed05a4d44520b6d388) Build Date: 2024-02-21T20:55:39Z\nDBSnapper Cloud: Standalone Mode\n\nSTART: Loading Snapshot\n-&gt; LOADING sanitized Snapshot #0: Name: 1708558331_example_app, Snapshot File: 1708558331_example_app.san.zip, Dest DB URL: postgres://postgres:postgres@localhost:15432/example_app_snap?sslmode=disable\n--&gt; Using engine: postgres-local\n--&gt; Using Target: example_app\n--&gt; Pulling to local file: /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Local snapshot already exists at /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Pulled snapshot 1708558331_example_app to /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip\n--&gt; Unzipping snapshot /Users/joescharf/.dbsnapper/1708558331_example_app.san.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-3818520887\n--&gt; Dropping and recreating database pgl://localhost:15432/example_app_snap\n-&gt; LOADING Snapshot Completed for Target: example_app\nFINISH: Loading Snapshot\n</code></pre> <p>In line 5 above we're loading the SANITIZED snapshot at index 0. The <code>load</code> command automatically loads the sanitized snapshot for an index if one exists.</p> <p>Note</p> <p>If a sanitized snapshot exists, but you'd like to load the original snapshot, you can use the <code>--original</code> flag to force this behavior:</p> <p><code>dbsnapper load example_app 0 --original</code></p>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#checking-the-sanitized-database", "title": "Checking the sanitized database", "text": "<p>Our sanitized database is now loaded, let's check it to see if it worked:</p> <pre><code>postgres=# \\l\n                                                          List of databases\n             Name              |  Owner   | Encoding |  Collate   |   Ctype    | ICU Locale | Locale Provider |   Access privileges\n-------------------------------+----------+----------+------------+------------+------------+-----------------+-----------------------\n example_app                   | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n example_app_sanitized         | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n example_app_snap              | postgres | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            |\n\n\npostgres=# \\c example_app_snap\npsql (15.6 (Postgres.app), server 15.2 (Debian 15.2-1.pgdg110+1))\nYou are now connected to database \"example_app_snap\" as user \"postgres\".\nexample_app_snap=#\nexample_app_snap=# \\dt\n             List of relations\n Schema |      Name      | Type  |  Owner\n--------+----------------+-------+----------\n public | dbsnapper_info | table | postgres\n public | users          | table | postgres\n(2 rows)\n\n example_app_snap=# select * from dbsnapper_info;\n         created_at         |         tags\n----------------------------+-----------------------\n 2024-02-21 23:47:17.454871 | {example_app.san.sql}\n(2 rows)\n</code></pre> <p>There are our tables, with the new <code>dbsnapper_info</code> table that holds the sanitization timestamp entry.</p> <p>And here is our user table with the sanitized <code>first_name</code>, <code>last_name</code>, <code>email</code>, <code>password</code> and <code>pin</code> fields.</p> <pre><code>example_app_snap=# select * from users;\n id | first_name | last_name |        email         |    password     | pin\n----+------------+-----------+----------------------+-----------------+------\n  1 | User       | Id1       | User_Id1@example.com | genericpassword | 0000\n  2 | User       | Id2       | User_Id2@example.com | genericpassword | 0000\n  3 | User       | Id3       | User_Id3@example.com | genericpassword | 0000\n  4 | User       | Id4       | User_Id4@example.com | genericpassword | 0000\n(4 rows)\n</code></pre>"}, {"location": "articles/creating-sanitized-database-snapshots-with-dbsnapper/#conclusion", "title": "Conclusion", "text": "<p>In this example, we created a simple database and demonstrated how you can use DBSnapper to copy it with the <code>build</code> and <code>load</code> commands, sanitize it with the <code>sanitize</code> command, and then <code>load</code> the sanitized version to your development database.</p> <p>We hope you find this example useful. If you have any questions about dbsnapper, you can contact us via email at contact@dbsnapper.com.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/", "title": "Part 1: Using DBSnapper with GitHub Actions and Amazon ECS", "text": ""}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#overview", "title": "Overview", "text": "<p>Note: This is Part 1 of a multi-part series on using DBSnapper with GitHub Actions and Amazon ECS. Go here to read Part 2 - A Simplified Approach with Third-Party Runners which includes the results of the workflow execution.</p> <p>Update: New DBSnapper GitHub Action</p> <p>We've updated this article to use our new Install DBSnapper Agent GitHub Action that makes it simple to install the DBSnapper Agent onto a GitHub Actions runner. These changes can be found in Step 4.</p> <p>The motivation behind this article is to describe an automated way to snapshot and sanitize database workloads running on private infrastructure. In this article, we'll use GitHub Actions and self-hosted runners to trigger a snapshot and sanitization of an Amazon RDS database using DBSnapper and Amazon ECS Fargate.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#services-and-components-used", "title": "Services and components used", "text": "<ul> <li>DBSnapper Agent to take the snapshot, sanitize it, and store it.</li> <li>DBSnapper Cloud providing target configuration and snapshot storage.</li> <li>GitHub Actions (GHA) as CI/CD provider to automate the environment setup and trigger the snapshot.</li> <li>GitHub Actions self-hosted runners to run the DBSnapper Agent in our private infrastructure.</li> <li>Amazon ECS Fargate to run an ephemeral GHA self-hosted runner</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#other-requirements", "title": "Other requirements", "text": "<ul> <li>We use GitHub's OIDC Provider to provide AWS credentials needed for the actions. We used the instructions given in the aws-actions/configure-aws-credentials OIDC Section to setup the federation between GitHub and AWS, using the CloudFormation Template to speed things up.</li> <li>We use the GitHub official <code>actions-runner</code> container for our self-hosted runner.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#github-secrets", "title": "GitHub Secrets:", "text": "<p>We will need the following GitHub Secrets configured for the GitHub Action:</p> <ul> <li><code>FG_PAT</code> - Fine-Grained GitHub Personal Access Token (PAT) with <code>repo:actions:(read/write)</code> and <code>org:self-hosted-runners:(read/write)</code> scopes.</li> <li><code>DBSNAPPER_AUTHTOKEN</code> - Your DBSnapper Cloud Auth Token used to authenticate to the DBSnapper API.</li> <li><code>DBSNAPPER_SECRET_KEY</code> - Your DBSnapper config secret key used to encrypt certain configuration values.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#overview-of-the-steps", "title": "Overview of the steps", "text": "<ol> <li>Setup the GitHub Actions workflow basics.</li> <li>Get a Registration token to register a self-hosted runner.</li> <li>Launch an Amazon ECS Fargate Task to run the self-hosted runner.</li> <li>Execute the DBSnapper Agent snapshot and sanitization commands.</li> <li>Cleanup the Amazon ECS Fargate Tasks.</li> </ol>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-1-setup-the-github-actions-workflow", "title": "Step 1 - Setup the GitHub Actions Workflow", "text": "Step 1 - Setup the GitHub Actions Workflow<pre><code>name: \"DBSnapper Build Snapshot\"\non: [push]\n\n# Permissions are needed for github OIDC provider - AWS federation\n# https://github.com/aws-actions/configure-aws-credentials?tab=readme-ov-file#oidc\npermissions:\n  id-token: write\n  contents: read\n  actions: write\n\n# Some necessary environment variables\nenv:\n  ORGANIZATION: dbsnapper # Your GitHub Organization\n  ORGANIZATION_URL: https://github.com/dbsnapper # Your GitHub Organization URL\n  AWS_REGION: us-east-1\n  AWS_IAM_ROLE: arn:aws:iam::&lt;acct_id&gt;:role/&lt;name of role&gt; # From OIDC Federation\n  ECS_CLUSTER: dbsnapper\n  ECS_SERVICE: github_runner\n  ECS_TASK_DEFINITION: .aws/ecs_task_definition_github_runner.json # Initial task definition file in the repo for the ECS Task\n</code></pre> <p>Here we setup the basic elements of the GitHub Actions workflow. We define the name of the workflow, the event that triggers it, and some environment variables that we will use throughout the workflow.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#iam-policy-for-the-aws_iam_role", "title": "IAM Policy for the AWS_IAM_ROLE", "text": "<p>The <code>permissions</code> section is needed for the GitHub OIDC provider to provide AWS credentials to the actions and the <code>AWS_IAM_ROLE</code> is the ARN of the role that the OIDC provider will assume to provide the AWS credentials. This role should have the necessary permissions to interact with the services we are using, specifically ECS in our case:</p> Example IAM Policy for the AWS_IAM_ROLE<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"RegisterTaskDefinition\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecs:RegisterTaskDefinition\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"PassRolesInTaskDefinition\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"iam:PassRole\"],\n      \"Resource\": \"*\"\n    },\n    {\n      \"Sid\": \"DeployService\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\"ecs:UpdateService\", \"ecs:DescribeServices\"],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Above is an example of the IAM policy that the <code>AWS_IAM_ROLE</code> should have to interact with ECS. You should set specific Resource ARNs for the <code>ecs:UpdateService</code> and <code>ecs:DescribeServices</code> actions to limit the access to the ECS cluster and service you are using. I've wildcarded them here for simplicity.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-2-get-registration-token-to-register-a-runner", "title": "Step 2 - Get Registration Token to Register a Runner", "text": "Step 2 - Get Registration Token<pre><code>jobs:\n  register-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      registration_token: ${{ steps.create-token.outputs.registration_token }}\n    steps:\n      - name: Create Registration Token\n        id: create-token\n        # https://docs.github.com/en/rest/actions/self-hosted-runners?apiVersion=2022-11-28#about-self-hosted-runners-in-github-actions\n        # use jq -r to output the raw version of the .token field\n        run: |\n          token=$(curl -L \\\n          -X POST \\\n          -H \"Accept: application/vnd.github+json\" \\\n          -H \"Authorization: Bearer ${{ secrets.FG_PAT }}\" \\\n          -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n          \"https://api.github.com/orgs/${{ env.ORGANIZATION }}/actions/runners/registration-token\" | jq -r '.token')\n\n          echo \"registration_token=$token\" &gt;&gt; $GITHUB_OUTPUT\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs", "title": "Inputs", "text": "<ul> <li><code>secrets.FG_PAT</code>- The Fine-Grained GitHub Personal Access Token (PAT) used in the <code>Authentication</code> header for the API call</li> <li><code>env.ORGANIZATION</code> - The GitHub Organization that the runner will be registered with, used in the API URL.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#outputs", "title": "Outputs", "text": "<ul> <li><code>create_token.registration_token</code> - The registration token that will be used to register the self-hosted runner</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description", "title": "Description", "text": "<p>Step two gives us a Registration Token that we can pass to the self-hosted runner configuration script to register the runner with the GitHub Actions service. We use the GitHub REST API to get the registration token and output it to a file that we can access in the next step.</p> <p>We use the <code>jq</code> utility to parse the JSON response from the API and extract the <code>token</code> field from the API response. We use the <code>-r</code> flag to output the raw version of the <code>token</code> field, otherwise it would be quoted and cause issues later in the workflow.</p> <p>in line 19 we save the output of the <code>token</code> to the $GITHUB_OUTPUT variable which is the new way to set outputs in GitHub Actions. This is assigned to the <code>registration_token</code> output of the job in line 5 for use later in the workflow.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-3-launch-amazon-ecs-task", "title": "Step 3 - Launch Amazon ECS Task", "text": "Step 3 - Launch Amazon ECS Task<pre><code>runner-ecs:\n  runs-on: ubuntu-latest\n  needs: register-runner\n  steps:\n    - name: Checkout for access to task definition\n      uses: actions/checkout@v4\n\n    # Uses OIDC connector for auth\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-region: ${{ env.AWS_REGION }}\n        role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n    - name: Update Task Definition JSON\n      id: update-task-def\n      uses: restackio/update-json-file-action@main\n      with:\n        file: ${{ env.ECS_TASK_DEFINITION }}\n        fields: |\n          {\n            \"containerDefinitions[0].command\": [\n            \"sh\",\n            \"-c\",\n            \"./config.sh --unattended --ephemeral --labels x64,linux,ecs --name ECS_Ephemeral_Runner --url ${{ env.ORGANIZATION_URL }} --token ${{ needs.register-runner.outputs.registration_token }} &amp;&amp; ./run.sh\"\n            ]\n          }\n\n    - name: Deploy Amazon ECS task definition\n      uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n      with:\n        task-definition: ${{ env.ECS_TASK_DEFINITION }}\n        service: ${{ env.ECS_SERVICE }}\n        cluster: ${{ env.ECS_CLUSTER }}\n        wait-for-service-stability: false\n\n    - name: ECS Service Set desired tasks to 1\n      run: |\n        aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 1\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs_1", "title": "Inputs", "text": "<ul> <li><code>env.AWS_REGION</code> - The AWS region where the ECS cluster is located</li> <li><code>env.AWS_IAM_ROLE</code> - The ARN of the IAM role that the OIDC provider will assume to provide the AWS credentials.</li> <li><code>env.ECS_CLUSTER</code> - The name of the ECS cluster where the task will be run.</li> <li><code>env.ECS_SERVICE</code> - The name of the ECS service that will run the task.</li> <li><code>env.ECS_TASK_DEFINITION</code> - The path and filename of the task definition file in the repository.</li> <li><code>.aws/ecs_task_definition_github_runner.json</code> - The initial task definition file in the repository for the ECS Task.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#actions", "title": "Actions", "text": "<ul> <li><code>actions/checkout@v4</code> - This action checks out the repository so that we can access the ECS task definition file.</li> <li><code>aws-actions/configure-aws-credentials@v4</code> - This action configures the AWS credentials for the actions to use. It uses the OIDC connector to authenticate with AWS.</li> <li><code>restackio/update-json-file-action@main</code> - This action updates the ECS task definition JSON file with the container command that provides the <code>--url</code> of the GitHub organization and the <code>--token</code> registration token to register the self-hosted runner. The main branch includes an updated version of node used in the action. We use this instead of <code>amazon-ecs-render-task-definition</code> because it improperly parses the provided command string, causing issues with the runner registration per this issue comment.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description_1", "title": "Description", "text": ""}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#checkout-and-configure-aws-credentials", "title": "Checkout and Configure AWS Credentials", "text": "<p>In this step, we're defining the <code>runner-ecs</code> job (that depends on successful execution of the <code>register-runner</code> job) that will launch an Amazon ECS Fargate Task to run the self-hosted runner. The first two steps are straightforward. We use the <code>actions/checkout@v4</code> action on line 6 to checkout the repository so that we can access the ECS task definition file. We then use the <code>aws-actions/configure-aws-credentials@v4</code> action on line 10 to configure the AWS credentials for the actions to use. This action uses the OIDC connector to authenticate with AWS.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#update-ecs-task-definition-json", "title": "Update ECS Task Definition JSON", "text": "<p>The next step starting on line 15, we update the ECS task definition file in our repository to reflect the command we'll run to configure the self-hosted runner. In this command we use the <code>config.sh</code> script to configure the runner with the <code>--url</code> and <code>--token</code> arguments, which are needed to register the runner in the GitHub Actions service. We can find our GHA runners in the organization actions settings URL: <code>https://github.com/organizations/&lt;your_organization&gt;/settings/actions/runners</code></p> <p>GHA Runner Docs</p> <p>It wasn't immediately obvious how to run the GHA runner container, so there was a bit of trial and error locally to get the commands right. Documentation can be found in the repository, and the Automate Configuring Self-Hosted Runners is especially useful. Two  scripts in the runner container are used to configure and run the runner:</p> <ul> <li><code>config.sh</code> - This script configures the runner with the provided <code>--url</code> and <code>--token</code> arguments. The <code>--ephemeral</code> flag is used to run the runner in ephemeral mode, meaning it will be removed when the task is stopped, and the <code>--unattended</code> flag is used to run the runner in unattended mode, meaning it won't prompt for user input.</li> <li><code>run.sh</code> - This script starts the runner, registering it with GitHub Actions, starting the runner service and waiting for jobs to run. This script is run after the runner is configured with the <code>config.sh</code> script.</li> </ul> <p>Both scripts will display help and available command line options by passing <code>--help</code> as an argument. The <code>config.sh</code> script is the only one that requires arguments, and the <code>run.sh</code> script will run the runner with the configuration provided by the <code>config.sh</code> script.</p> <p>The <code>--ephemeral</code> flag is used to run the runner in ephemeral mode, meaning the runner will be removed from the organization after it has completed a single job. This ensures we don't have any lingering runners in the organization.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#deploy-amazon-ecs-task-definition", "title": "Deploy Amazon ECS Task Definition", "text": "<p>The next step on line 29 uses the <code>aws-actions/amazon-ecs-deploy-task-definition@v1</code> action to deploy the updated task definition to the ECS cluster. This action takes the path to the task definition file, the cluster, and service name as inputs. We set the <code>wait-for-service-stability</code> input to <code>false</code> to avoid waiting for the service to stabilize before continuing with the workflow, since Step 4 - Execute the DBSnapper Agent Commands won't proceed until the runner is registered, running, and able to execute the job.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#ecs-service-set-desired-tasks-to-1", "title": "ECS Service: Set Desired Tasks to 1", "text": "<p>Finally, on line 39, we use the <code>aws ecs update-service</code> command (from the aws-cli) to set the desired task count for the service to 1. This will start a task in the service to run the self-hosted runner. The updated task definition will not start without this command. We use a similar command in Step 5 - Cleanup the Amazon ECS Task to set the desired task count to 0 to stop the runner after the job is complete.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#task-definition-file", "title": "Task Definition File", "text": "<p>To launch an ECS task via github actions and the <code>aws-actions/amazon-ecs-deploy-task-definition@v1</code> action, we need to provide a task definition file. This file is a JSON file that defines the properties of the task that we want to run. We can create this file using the ECS task definition wizard in the AWS console and then copy the task definition JSON to a file in our repository, or we can just use the sample proided below modified for your environment. In our workflow example, we're using the <code>ECS_TASK_DEFINITION</code> environment variable to specify the path to the task definition file.</p> ecs_task_definition_github_runner.json<pre><code>{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"github_runner\",\n      \"image\": \"ghcr.io/actions/actions-runner:2.317.0\",\n      \"cpu\": 0,\n      \"portMappings\": [],\n      \"essential\": true,\n      \"command\": [],\n      \"mountPoints\": [],\n      \"volumesFrom\": [],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/github_runner\",\n          \"awslogs-create-group\": \"true\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      },\n      \"systemControls\": []\n    }\n  ],\n  \"family\": \"github_runner\",\n  \"taskRoleArn\": \"arn:aws:iam::&lt;account_id&gt;:role/&lt;task_execution_role&gt;\",\n  \"executionRoleArn\": \"arn:aws:iam::&lt;account_id&gt;:role/&lt;task_execution_role&gt;\",\n  \"networkMode\": \"awsvpc\",\n  \"volumes\": [],\n  \"placementConstraints\": [],\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"512\",\n  \"memory\": \"2048\",\n  \"runtimePlatform\": {\n    \"cpuArchitecture\": \"X86_64\",\n    \"operatingSystemFamily\": \"LINUX\"\n  },\n  \"tags\": []\n}\n</code></pre> <p>In this task definition, we set the default GHA runner image on line 5. If needed, we can update this image in the workflow using the <code>restackio/update-json-file-action@main</code> action. We also set the <code>cpu</code> and <code>memory</code> values for this task on lines 31 and 32 to the minimum values needed to run the runner and any additional services.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-4-execute-the-dbsnapper-agent-commands", "title": "Step 4 - Execute the DBSnapper Agent Commands", "text": "Step 4 - Execute the DBSnapper Agent Commands<pre><code>dbsnapper:\n  runs-on: self-hosted\n  needs: runner-ecs\n  env:\n    DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n    DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n  steps:\n    # Updated to use the new DBSnapper GitHub Action\n    - name: Install DBSnapper Agent\n      uses: dbsnapper/install-dbsnapper-agent-action@v1\n      with:\n        version: latest\n    - name: Install Database Utilities\n      # Get the latest version of the postgres client\n      # https://www.postgresql.org/download/linux/ubuntu/\n      run: |\n        sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-common &amp;&amp; sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-client-16\n\n    - name: Run DBSnapper build command\n      run: dbsnapper build dvdrental-prod\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs_2", "title": "Inputs", "text": "<ul> <li><code>secrets.DBSNAPPER_SECRET_KEY</code> - Your DBSnapper config secret key used to encrypt certain configuration values.</li> <li><code>secrets.DBSNAPPER_AUTHTOKEN</code> - Your DBSnapper Cloud Auth Token used to authenticate to the DBSnapper API.</li> <li><code>DBSnapper Debian Package</code> - The latest release of the DBSnapper Agent. Download the <code>.deb</code> package that matches your architecture. We use the <code>dbsnapper_linux_x86_64.deb</code> package in this example.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description_2", "title": "Description", "text": "<p>The steps prior to this one were necessary to setup a self-hosted runner in our private infrastructure with access to the workloads in our network. In this step, we're finally able to run the DBSnapper Agent commands to snapshot and sanitize the database.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#runs-on-self-hosted", "title": "Runs-on: Self-Hosted", "text": "<p>On line 2, we specify the <code>runs-on</code> property as <code>self-hosted</code> to run the job on the self-hosted runner we registered in the previous step. This property can take additional labels to be more specific about the runner that should run the job, in cases where you have many different types of runners registered in your organization. In line 3 we specify that this job depends on the <code>runner-ecs</code> job to ensure the ecs jobs are run before running the dbsnapper job.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#environment-variables", "title": "Environment Variables", "text": "<p>We set our environment variables starting on line 4. In this example, we set the minimum required <code>DBSNAPPER_SECRET_KEY</code> and <code>DBSNAPPER_AUTHTOKEN</code> environment variables necessary to run DBSnapper without a configuration file.</p> <p>DBSnapper Configuration via Environment Variables</p> <p>DBSnapper can be configured exclusively through environment variables if you don't want to rely on a configuration file. All the configuration options can be represented as environment variables through a specific naming convention involving prefixing the environment variable with <code>DBSNAPPER</code> and replacing periods with two underscores <code>__</code>. Some examples from the DBSnapper Configuration Documentation include:</p> <ul> <li><code>docker.images.postgres</code> -&gt; <code>DBSNAPPER_DOCKER__IMAGES__POSTGRES: postgres:latest</code> # Sets the docker image to use for the postgres containers</li> <li><code>defaults.shared_target_dst_db_url</code> -&gt; <code>DBSNAPPER_DEFAULTS__SHARED_TARGET_DST_DB_URL: &lt;connstring&gt;</code> # Sets the default destination database URL for shared targets</li> <li><code>override.san_query</code> -&gt; <code>DBSNAPPER_OVERRIDE__SAN_QUERY: &lt;base-64-encoded-value&gt;</code> # Sets a query to use for sanitization overriding any existing queries.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#install-and-run-dbsnapper", "title": "Install and Run DBSnapper", "text": "<p>Starting on line 8 we use the new DBSnapper GitHub Action to install the latest version of the DBSnapper Agent. This action takes into account the operating system and architecture of the runner and installs the appropriate version of the DBSnapper Agent.</p> <p>Starting on line 8, we run commands to update our apt repository, install <code>curl</code>, and use <code>curl</code> to download the latest release of the DBSnapper Agent. We then use <code>dpkg</code> to install the <code>.deb</code> package.</p> <p>Why Not use the DBSnapper Docker Image?</p> <p>At this point, it would have been convenient to use the DBSnapper Docker image to run the DBSnapper commands. Unfortunately GitHub Actions... runner does not have Docker installed by default, so we would need to install Docker in the runner before we could use the Docker image. To avoid the additional complexity we decided to download and install the <code>.deb</code> package instead.</p> <p>...doesn't support using docker from a self-hosted runner at this time. See the following issues for more information:</p> <ul> <li>https://github.com/actions/runner/issues/406\u2060</li> <li>https://github.com/actions/runner/issues/367\u2060    </li> </ul> <p>Database Utilities Needed</p> <p>When using the DBSnapper container image, the Agent and database utilities are already included in the image. Since GitHub Actions doesn't support Docker containers, we need to install the tools by hand. In this case, we install the PostgreSQL client on line 17 to support the snapshot of our Postgresql RDS database. If you are using a different database, you will need to install the appropriate client.</p> <p>On line 13, we run the <code>dbsnapper build dvdrental-prod</code> command which will use the <code>DBSNAPPER_AUTHTOKEN</code> to authenticate to the DBSnapper Cloud, create a snapshot of the database specified in the <code>dvdrental-prod</code> target, and store it in the cloud storage specified in the target configuration. Once this is complete and no additional steps are needed, the task will be cleaned up in the next step.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#step-5-cleanup-the-amazon-ecs-task", "title": "Step 5 - Cleanup the Amazon ECS Task", "text": "Step 5 - Cleanup the Amazon ECS Task<pre><code>deprovision:\n  runs-on: ubuntu-latest\n  if: ${{ always() }}\n  needs: dbsnapper\n  steps:\n    # Uses OIDC connector for auth\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-region: ${{ env.AWS_REGION }}\n        role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n    - name: ECS Add desired tasks to 0\n      run: |\n        aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 0\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#inputs_3", "title": "Inputs", "text": "<ul> <li><code>env.AWS_REGION</code> - The AWS region where the ECS cluster is located</li> <li><code>env.AWS_IAM_ROLE</code> - The ARN of the IAM role that the OIDC provider will assume to provide the AWS credentials.</li> <li><code>env.ECS_CLUSTER</code> - The name of the ECS cluster where the task will be run.</li> <li><code>env.ECS_SERVICE</code> - The name of the ECS service that will run the task.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#actions_1", "title": "Actions", "text": "<ul> <li><code>aws-actions/configure-aws-credentials@v4</code> - This action configures the AWS credentials for the actions to use. It uses the OIDC connector to authenticate with AWS.</li> </ul>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#description_3", "title": "Description", "text": "<p>In this final step, we need to use the <code>aws-actions/configure-aws-credentials@v4</code> action to configure the AWS credentials so we can use the AWS CLI to run the <code>aws ecs update-service</code> command. We use this command to set the desired task count for the service to 0, which will stop all ECS runner tasks, ensuring we don't get billed for unnecessary resources. Because we configured the runners with the <code>--ephemeral</code> flag, they will be automatically removed from the GitHub organization when they are terminated.</p> <p>Note: By using the <code>if: ${{ always() }}</code> condition on line 3, we ensure this step runs even if the previous steps fail. This is important to ensure that the ECS service is cleaned up even other steps fail, which under normal circumstances would not be the case.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#conclusion", "title": "Conclusion", "text": "<p>In this article, we've shown how to use GitHub Actions to trigger a snapshot and sanitization of an Amazon RDS database using DBSnapper and Amazon ECS Fargate. We've covered the setup of the GitHub Actions workflow, the registration of a self-hosted runner, the launch of an Amazon ECS Fargate task to run the runner, the execution of the DBSnapper Agent commands, and the cleanup of the Amazon ECS task. This workflow can be used to automate the snapshot and sanitization of database workloads running on private infrastructure, providing an automated and reliable manage database backups.</p>"}, {"location": "articles/dbsnapper-github-actions-amazon-ecs/#full-workflow", "title": "Full Workflow", "text": "<p>Here is the full GitHub Actions workflow that we've described in this article. Be sure to replace any placeholders with your own values.</p> DBSnapper Build Snapshot - Full Workflow<pre><code>name: \"DBSnapper Build Snapshot\"\non: [push]\n\n# Permissions are needed for github OIDC provider - AWS federation\n# https://github.com/aws-actions/configure-aws-credentials?tab=readme-ov-file#oidc\npermissions:\n  id-token: write\n  contents: read\n  actions: write\n\n# Some necessary environment variables\nenv:\n  ORGANIZATION: dbsnapper # Your GitHub Organization\n  ORGANIZATION_URL: https://github.com/dbsnapper # Your GitHub Organization URL\n  AWS_REGION: us-east-1\n  AWS_IAM_ROLE: arn:aws:iam::&lt;acct_id&gt;:role/&lt;name of role&gt; # From OIDC Federation\n  ECS_CLUSTER: dbsnapper\n  ECS_SERVICE: github_runner\n  ECS_TASK_DEFINITION: .aws/ecs_task_definition_github_runner.json # Initial task definition file in the repo for the ECS Task\n\njobs:\n  register-runner:\n    runs-on: ubuntu-latest\n    outputs:\n      registration_token: ${{ steps.create-token.outputs.registration_token }}\n    steps:\n      - name: Create Registration Token\n        id: create-token\n        # https://docs.github.com/en/rest/actions/self-hosted-runners?apiVersion=2022-11-28#about-self-hosted-runners-in-github-actions\n        # use jq -r to output the raw version of the .token field\n        run: |\n          token=$(curl -L \\\n          -X POST \\\n          -H \"Accept: application/vnd.github+json\" \\\n          -H \"Authorization: Bearer ${{ secrets.FG_PAT }}\" \\\n          -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n          \"https://api.github.com/orgs/${{ env.ORGANIZATION }}/actions/runners/registration-token\" | jq -r '.token')\n\n          echo \"registration_token=$token\" &gt;&gt; $GITHUB_OUTPUT\n\n  runner-ecs:\n    runs-on: ubuntu-latest\n    needs: register-runner\n    steps:\n        - name: Checkout for access to task definition\n          uses: actions/checkout@v4\n\n        # Uses OIDC connector for auth\n        - name: Configure AWS credentials\n          uses: aws-actions/configure-aws-credentials@v4\n          with:\n            aws-region: ${{ env.AWS_REGION }}\n            role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n        - name: Update Task Definition JSON\n          id: update-task-def\n          uses: restackio/update-json-file-action@main\n          with:\n            file: ${{ env.ECS_TASK_DEFINITION }}\n            fields: |\n            {\n                \"containerDefinitions[0].command\": [\n                \"sh\",\n                \"-c\",\n                \"./config.sh --unattended --ephemeral --labels x64,linux,ecs --name ECS_Ephemeral_Runner --url ${{ env.ORGANIZATION_URL }} --token ${{ needs.register-runner.outputs.registration_token }} &amp;&amp; ./run.sh\"\n                ]\n            }\n\n        - name: Deploy Amazon ECS task definition\n          uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n          with:\n            task-definition: ${{ env.ECS_TASK_DEFINITION }}\n            service: ${{ env.ECS_SERVICE }}\n            cluster: ${{ env.ECS_CLUSTER }}\n            wait-for-service-stability: false\n\n        - name: ECS Service Set desired tasks to 1\n          run: |\n            aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 1\n\n  dbsnapper:\n    runs-on: self-hosted\n    needs: runner-ecs\n    env:\n      DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n      DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n    steps:\n      # Updated to use the new DBSnapper GitHub Action\n      - name: Install DBSnapper Agent\n        uses: dbsnapper/install-dbsnapper-agent-action@v1\n        with:\n          version: latest\n      - name: Install Database Utilities\n        # Get the latest version of the postgres client\n        # https://www.postgresql.org/download/linux/ubuntu/\n        run: |\n          sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-common &amp;&amp; sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-client-16\n\n      - name: Run DBSnapper build command\n        run: dbsnapper build dvdrental-prod\n\n  deprovision:\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    needs: dbsnapper\n    steps:\n      # Uses OIDC connector for auth\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n      - name: ECS Add desired tasks to 0\n        run: |\n          aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 0\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/", "title": "Part 2: Using DBSnapper, GitHub Actions, and ECS - A Simplified Approach", "text": ""}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#overview", "title": "Overview", "text": "<p>Note: This is Part 2 of a multi-part series on using DBSnapper with GitHub Actions and Amazon ECS. Go here to read Part 1.</p> <p>Third Party GitHub Runners</p> <p>This article provides an alternative approach to using DBSnapper with GitHub Actions and Amazon ECS. This article uses a GitHub runner built by a third party. There are actually several nice third-party runners that build upon the official GitHub Actions runner, and provide additional capabilities that the official runner doesn't yet support. You can find a list of these runners on the Awesome-runners list. For this article we are using the myoung34 GitHub Runner.</p> <p>The previous version of this article used the official GitHub Actions runner to run the DBSnapper Agent in an Amazon ECS Fargate Task. We've found that there are third-party runners that provide additional capabilities that the official runner doesn't yet support. We will present the simplified approach using the myoung34 GitHub Runner in this article. We will be brief in our explanation of the steps, as they are similar to the previous article.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#using-the-myoung34-github-runner", "title": "Using the myoung34 GitHub Runner", "text": "<p>The myoung34 GitHub Runner is a third-party runner that builds upon the official GitHub Actions runner. This runner makes use of environment variables to configure the runner and and most importantly supports an <code>ACCESS_TOKEN</code> environment variable that will take care of registering the runner with the GitHub Actions service. This eliminates the need for the first step in the previous article where we had to get a registration token to register the runner with GitHub.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#simplified-github-actions-workflow", "title": "Simplified GitHub Actions Workflow", "text": "<p>In this workflow we only have three jobs instead of the four in the previous article. We describe some of the important changes below:</p> Simplified GitHub Actions Workflow<pre><code>name: \"GHA-my-ECS-DBSnapper Build Snapshot\"\non: [push]\n\n# For github OIDC provider\n# https://github.com/aws-actions/configure-aws-credentials?tab=readme-ov-file#oidc\npermissions:\n  id-token: write\n  contents: read\nenv:\n  ORGANIZATION: &lt;Your_GitHub_Organization_Name&gt;\n  ORGANIZATION_URL: &lt;Your_GitHub Organization URL&gt;\n  GHR_IMAGE: myoung34/github-runner:latest\n  AWS_REGION: us-east-1\n  AWS_IAM_ROLE: arn:aws:iam::&lt;account_id&gt;:&lt;role_name&gt;\n  ECS_CLUSTER: &lt;ECS_Cluster_Here&gt;\n  ECS_SERVICE: &lt;ECS_Service_Here&gt;\n  ECS_TASK_DEFINITION: .aws/ecs_task_definition_my_github_runner.json\n  DBSNAPPER_TARGET: dbs-prod-rds\n\njobs:\n  runner-ecs:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout for access to task definition\n        uses: actions/checkout@v4\n\n      # Uses OIDC connector for auth\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n      - name: Update Task Definition JSON\n        id: update-task-def\n        uses: restackio/update-json-file-action@main\n        with:\n          file: ${{ env.ECS_TASK_DEFINITION }}\n          fields: |\n            {\n              \"containerDefinitions[0].image\": \"${{ env.GHR_IMAGE }}\",\n              \"containerDefinitions[0].environment\": [\n                {\n                  \"name\": \"ACCESS_TOKEN\",\n                  \"value\": \"${{ secrets.FG_PAT }}\"\n                },\n                {\n                  \"name\": \"RUNNER_SCOPE\",\n                  \"value\": \"org\"\n                },\n                {\n                  \"name\": \"ORG_NAME\",\n                  \"value\": \"${{ env.ORGANIZATION }}\"\n                },\n                {\n                  \"name\": \"EPHEMERAL\",\n                  \"value\": \"true\"\n                },\n                {\n                  \"name\": \"LABELS\",\n                  \"value\": \"ecs\"\n                },\n                {\n                  \"name\": \"RUNNER_NAME_PREFIX\",\n                  \"value\": \"ecs\"\n                }\n              ]\n            }\n\n      - name: Deploy Amazon ECS task definition\n        uses: aws-actions/amazon-ecs-deploy-task-definition@v1\n        with:\n          task-definition: ${{ env.ECS_TASK_DEFINITION }}\n          service: ${{ env.ECS_SERVICE }}\n          cluster: ${{ env.ECS_CLUSTER }}\n          wait-for-service-stability: false\n\n      - name: ECS Add desired tasks to 1\n        run: |\n          aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 1\n\n  dbsnapper:\n    runs-on: self-hosted\n    needs: runner-ecs\n    env:\n      DBSNAPPER_SECRET_KEY: ${{ secrets.DBSNAPPER_SECRET_KEY }}\n      DBSNAPPER_AUTHTOKEN: ${{ secrets.DBSNAPPER_AUTHTOKEN }}\n      # Using target templates so we provide the required env vars:\n      DATABASE_USERNAME: ${{ secrets.DATABASE_USERNAME }}\n      DATABASE_PASSWORD: ${{ secrets.DATABASE_PASSWORD }}\n      DATABASE_HOST: ${{ secrets.DATABASE_HOST }}\n\n    steps:\n      - name: Install DBSnapper Agent\n        uses: dbsnapper/install-dbsnapper-agent-action@v1\n        with:\n          version: latest\n\n      - name: Install Database Utilities\n        # Get the latest version of the postgres client\n        # https://www.postgresql.org/download/linux/ubuntu/\n        run: |\n          sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-common &amp;&amp; sudo /usr/share/postgresql-common/pgdg/apt.postgresql.org.sh -y &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install -y postgresql-client-16\n\n      - name: Run DBSnapper List Targets\n        run: dbsnapper targets\n\n      - name: Run Build Snapshot\n        run: dbsnapper build ${{ env.DBSNAPPER_TARGET }}\n\n  deprovision:\n    runs-on: ubuntu-latest\n    if: ${{ always() }}\n    needs: dbsnapper\n    steps:\n      # Uses OIDC connector for auth\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          aws-region: ${{ env.AWS_REGION }}\n          role-to-assume: ${{ env.AWS_IAM_ROLE }}\n\n      - name: ECS Add desired tasks to 0\n        run: |\n          aws ecs update-service --cluster ${{ env.ECS_CLUSTER }} --service ${{ env.ECS_SERVICE }} --desired-count 0\n</code></pre> <p>The first job in this workflow involves setting up and starting the ECS task definition that will launch the myoung34 GitHub Runner. The biggest difference here is in the Update Task Definition JSON step starting on line 33. Instead of creating a command to run the runner as we did in the previous article, we instead update the task definition JSON file to include the necessary environment variables for the runner on lines 42-67. The environment variables we set are:</p> <ul> <li>ACCESS_TOKEN: We provide our Fine-Grained Personal Access Token (FG_PAT) as a secret to the runner. This token is used to authenticate the runner with GitHub Actions, saving us the step of registering the runner with GitHub which was necessary in the previous article.</li> <li>RUNNER_SCOPE: We set the runner scope to <code>org</code> to allow the runner to access all repositories in the organization.</li> <li>ORG_NAME: We provide the organization name to the runner.</li> <li>EPHEMERAL: We set the runner to ephemeral mode, so it will be deregistered from the GitHub Actions service when the task is stopped.</li> <li>LABELS: We provide a label to the runner to identify it as an ECS runner - customize this as you see fit.</li> <li>RUNNER_NAME_PREFIX: We provide a prefix for the runner name to identify it in the GitHub Actions service.</li> </ul> <p>There are several other environment variables you can use to configure the runner. You can find more information on the myoung34 GitHub Runner page.</p> <p>Database Utilities Needed</p> <p>When using the DBSnapper container image, the Agent and database utilities are already included in the image. Since GitHub Actions doesn't support Docker containers, we need to install the tools by hand. In this case, we install the PostgreSQL client on line 103 to support the snapshot of our Postgresql RDS database. If you are using a different database, you will need to install the appropriate client.</p> <p>Since our target defintion is using connection string templates for added security, we've set the <code>DATABASE_USERNAME</code>, <code>DATABASE_PASSWORD</code>, and <code>DATABASE_HOST</code> environment variables to the appropriate secrets on lines 89-91.</p> <p>The rest of the workflow is similar to the previous article, with the <code>dbsnapper</code> job installing and running the DBSnapper Agent to build a snapshot and the <code>deprovision</code> job stopping the ECS task.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#task-definition", "title": "Task Definition", "text": "<p>The task definition JSON file is similar to the one in the previous article, we're including it here for completeness.</p> .aws/ecs_task_definition_my_github_runner.json<pre><code>{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"my_github_runner\",\n      \"image\": \"myoung34/github-runner:latest\",\n      \"cpu\": 0,\n      \"portMappings\": [],\n      \"essential\": true,\n      \"environment\": [],\n      \"mountPoints\": [],\n      \"volumesFrom\": [],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/github_runner\",\n          \"awslogs-create-group\": \"true\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs-my\"\n        }\n      },\n      \"systemControls\": []\n    }\n  ],\n  \"family\": \"my_github_runner\",\n  \"taskRoleArn\": \"&lt;task_role_arn&gt;\",\n  \"executionRoleArn\": \"&lt;execution_role_arn&gt;\",\n  \"networkMode\": \"awsvpc\",\n  \"volumes\": [],\n  \"placementConstraints\": [],\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"512\",\n  \"memory\": \"2048\",\n  \"runtimePlatform\": {\n    \"cpuArchitecture\": \"X86_64\",\n    \"operatingSystemFamily\": \"LINUX\"\n  },\n  \"tags\": []\n}\n</code></pre>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#target-settings", "title": "Target Settings", "text": "<p>The target we've configured in the DBSnapper Cloud is called <code>dbs-prod-rds</code>. This target uses a Source URL connection string template allowing us to store sensitive connection information in GitHub Secrets.</p> <p> Target settings for the dbs-prod-rds target. </p> <p>This target definition also includes a storage profile that uploads our snapshot to an S3 bucket in our AWS account. Since we aren't doing anything with Sanitization yet, we just leve the Sanitization Detais empty.</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#workflow-execution-output", "title": "Workflow Execution Output", "text": "<p>When the workflow runs successfully, you should see output similar to the following:</p> GitHub Actions Workflow Output<pre><code> Listing all targets, update = false\n +-----------------------------+-------+---------------------------------------------------------------+-------+-----------------------------------------------------------+-----------------+\n |            NAME             | TYPE  |                              SRC                              | SIZE  |                            DST                            | STORAGE PROFILE |\n +-----------------------------+-------+---------------------------------------------------------------+-------+-----------------------------------------------------------+-----------------+\n | dbs-prod-rds                | cloud | pgl://dbsna...amazonaws.com:5432/app_dbsnapper_com_production | 0 B   |                                                           |                 |\n | dvdrental-san               | cloud | pgl://host....ker.internal:15432/dvdrental                    | 0 B   |                                                           |                 |\n | myd-ghost                   | cloud | myd://10.4.20.22:3306/ghost                                   | 0 B   | myd://mysql8:3306/ghost_prod_snap                         |                 |\n | snappy-rag-vecdb            | cloud | pgl://raghost:5432/snappy_rag                                 | 0 B   | pgl://raghost:5432/snappy_rag_snapshot                    |                 |\n |                             | ----- |                                                               |       |                                                           |                 |\n |                             | ----- |                                                               |       |                                                           |                 |\n |                             | ----- |                                                               |       |                                                           |                 |\n +-----------------------------+-------+---------------------------------------------------------------+-------+-----------------------------------------------------------+-----------------+\n ##[group]Run dbsnapper build dbs-prod-rds\n\n-- snip --\n\n env:\n   ORGANIZATION: dbsnapper\n   ORGANIZATION_URL: https://github.com/dbsnapper\n   GHR_IMAGE: myoung34/github-runner:latest\n\n-- snip --\n\n   DATABASE_USERNAME: ***\n   DATABASE_PASSWORD: ***\n   DATABASE_HOST: ***\n ##[endgroup]\n DBSnapper Agent - Version: 2.7.1 (aab3b1292b3e) Build Date: 2024-06-08T18:52:54Z\n DBSnapper Cloud: Enabled\n\n START: Build Snapshot for target: dbs-prod-rds with engine: postgres-local\n --&gt; Zipping snapshot to /root/.dbsnapper/1717956554_dbs-prod-rds.zip\n --&gt; Uploading snapshot to Cloud Storage via Presigned URL\n --&gt; Upload complete.\n --&gt; Local snapshot /root/.dbsnapper/1717956554_dbs-prod-rds.zip stored in cloud with ID: af36c454-eab4-4623-8485-4a51732634e7\n FINISH: Building DB Snapshot for target: dbs-prod-rds\n</code></pre> <p>As we can see above, the workflow first listed our targets as we requested on line 105 of the workflow. Here' we see the <code>dbs-prod-rds</code> target of interest. On lines 28-36 we see the output of the <code>dbsnapper build</code> command indicating that the snapshot was successfully built and uploaded to the cloud storage location specified in the target definition (line 35).</p>"}, {"location": "articles/dbsnapper-github-actions-ecs-simplified/#cloud-storage-verification", "title": "Cloud Storage Verification", "text": "<p>We can verify that the snapshot was uploaded to our S3 bucket by checking the bucket in the AWS Console. Here we see the snapshot file <code>af36c454-eab4-4623-8485-4a51732634e7.zip</code> in the bucket, which corresponds to the cloud ID of the snapshot we saw in the workflow output on line 35.</p> <p> Listing of objects in our snapshot bucket on S3. </p> <p>Now that we have a working, automated way to snapshot our database, we can include this workflow in our CI/CD pipeline to ensure that we always have the latest snapshot available as a point-in-time backup. We can use DBSnapper to create sanitized versions of these snapshots that we can share with our DevOps and development team for development and testing purposes.</p>"}, {"location": "articles/useful-postgres-mysql-queries/", "title": "Useful Postgres and MySQL Queries", "text": "<p>Here are some of the queries we use in DBSnapper to get database schema and other information.</p>"}, {"location": "articles/useful-postgres-mysql-queries/#connect-and-list-databases-tables", "title": "Connect and List Databases &amp; Tables", "text": "<p>Connect and list databases and tables.</p> PostgresMySQL <pre><code>psql -d postgres://postgres:postgres@localhost:15432\n\\l # list databases\n\\c database_name # connect to database\n\\d # list tables\n\\d+ table_name # list columns\n</code></pre> <pre><code>mysql -u root -P13306 -hlocalhost --protocol tcp -pmysql\nshow databases;\nuse database_name;\nshow tables;\ndescribe table_name;\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#list-tables", "title": "List Tables", "text": "<p>Tables</p> PostgresMySQL Query<pre><code>SELECT concat(concat(nsp.nspname, '.'), cls.relname) as table_name\nFROM pg_class cls\n  JOIN pg_namespace nsp ON nsp.oid = cls.relnamespace\nWHERE nsp.nspname NOT IN ('information_schema', 'pg_catalog')\n  AND cls.relkind = 'r';\n</code></pre> Output<pre><code>      table_name\n-----------------------\npublic.store\npublic.payment\npublic.film_category\npublic.actor\npublic.staff\npublic.dbsnapper_info\npublic.category\npublic.country\npublic.language\npublic.customer\npublic.users\npublic.rental\npublic.inventory\npublic.film\npublic.address\npublic.film_actor\npublic.city\n(17 rows)\n</code></pre> Query<pre><code>SELECT t.table_schema as schema_name,\n  t.table_name as table_name,\n  t.table_rows AS row_count,\n  ROUND(data_length) AS data_size,\n  ROUND(index_length) AS index_size,\n  ROUND(data_length + index_length) AS total_size\nFROM information_schema.TABLES t\nWHERE t.table_schema = 'sakila' -- replace sakila with your database name\n  AND t.table_type = 'BASE TABLE';\n</code></pre> Output<pre><code>+-------------+-----------------+-----------+-----------+------------+------------+\n| schema_name | table_name      | row_count | data_size | index_size | total_size |\n+-------------+-----------------+-----------+-----------+------------+------------+\n| sakila      | actor           |       200 |     16384 |      16384 |      32768 |\n| sakila      | address         |       603 |     98304 |      16384 |     114688 |\n| sakila      | category        |        16 |     16384 |          0 |      16384 |\n| sakila      | city            |       600 |     49152 |      16384 |      65536 |\n| sakila      | country         |       109 |     16384 |          0 |      16384 |\n| sakila      | customer        |       599 |     81920 |      49152 |     131072 |\n| sakila      | dbs_rental_copy |     15831 |   1589248 |          0 |    1589248 |\n| sakila      | film            |      1000 |    196608 |      81920 |     278528 |\n| sakila      | film_actor      |      5462 |    196608 |      81920 |     278528 |\n| sakila      | film_category   |      1000 |     65536 |      16384 |      81920 |\n| sakila      | film_text       |      1000 |    180224 |      16384 |     196608 |\n| sakila      | inventory       |      4581 |    180224 |     196608 |     376832 |\n| sakila      | language        |         6 |     16384 |          0 |      16384 |\n| sakila      | payment         |     16086 |   1589248 |     638976 |    2228224 |\n| sakila      | rental          |     16419 |   1589248 |    1196032 |    2785280 |\n| sakila      | staff           |         2 |     65536 |      32768 |      98304 |\n| sakila      | store           |         2 |     16384 |      32768 |      49152 |\n+-------------+-----------------+-----------+-----------+------------+------------+\n17 rows in set (0.01 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#list-table-columns", "title": "List Table Columns", "text": "<p>Table Columns</p> PostgresMySQL Query<pre><code>SELECT c.table_schema,\n  c.table_name,\n  c.column_name,\n  typ.typname AS type_name,\n  CASE\n    WHEN c.data_type = 'USER-DEFINED' THEN c.udt_name\n    ELSE c.data_type\n  END AS data_type,\n  CASE\n    WHEN c.data_type IN ('character', 'character varying') THEN c.character_maximum_length\n    ELSE NULL\n  END AS data_type_char_max,\n  -- Added this line\n  tc.constraint_name AS constraint_name,\n  (\n    SELECT COUNT(*) &gt; 0\n    FROM information_schema.key_column_usage cu\n      LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\n    WHERE cu.column_name = c.column_name\n      AND cu.table_name = c.table_name\n      AND tc.constraint_type = 'PRIMARY KEY'\n  ) AS is_primary,\n  (\n    SELECT COUNT(*) &gt; 0\n    FROM information_schema.key_column_usage cu\n      LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\n    WHERE cu.column_name = c.column_name\n      AND cu.table_name = c.table_name\n      AND tc.constraint_type = 'FOREIGN KEY'\n  ) AS is_foreign,\n  (\n    SELECT COUNT(*) &gt; 0\n    FROM information_schema.key_column_usage cu\n      LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\n    WHERE cu.column_name = c.column_name\n      AND cu.table_name = c.table_name\n      AND tc.constraint_type = 'UNIQUE'\n  ) AS is_unique,\n  BOOL_OR(c.is_nullable = 'YES') AS is_nullable,\n  att.attgenerated AS att_generated,\n  att.attidentity AS att_identity,\n  COALESCE(\n    STRING_AGG(\n      enumlabel,\n      ','\n      ORDER BY enumsortorder\n    ),\n    ''\n  ) AS enum_values,\n  COALESCE(pd.description, '') AS comment\nFROM information_schema.columns c\n  LEFT JOIN pg_type typ ON c.udt_name = typ.typname\n  LEFT JOIN pg_enum enu ON typ.oid = enu.enumtypid\n  LEFT JOIN pg_class cls ON c.table_name = cls.relname\n  LEFT JOIN pg_namespace ns ON cls.relnamespace = ns.oid\n  LEFT JOIN pg_description pd ON cls.oid = pd.objoid\n  LEFT JOIN pg_attribute att ON cls.oid = att.attrelid\n  LEFT JOIN information_schema.key_column_usage cu ON cu.column_name = c.column_name\n  AND cu.table_name = c.table_name\n  LEFT JOIN information_schema.table_constraints tc ON tc.constraint_name = cu.constraint_name\nWHERE c.table_name = 'customer' -- REPLACE WITH YOUR TABLE NAME\n  AND c.table_schema = 'public' -- REPLACE WITH YOUR SCHEMA\nGROUP BY c.column_name,\n  c.table_schema,\n  c.table_name,\n  typ.typname,\n  c.data_type,\n  c.udt_name,\n  c.ordinal_position,\n  pd.description,\n  att.attgenerated,\n  att.attidentity,\n  tc.constraint_name,\n  c.character_maximum_length\nORDER BY c.ordinal_position;       \n</code></pre> Output<pre><code>table_schema | table_name | column_name | type_name |          data_type          | data_type_char_max |     constraint_name      | is_primary | is_foreign | is_unique | is_nullable | att_generated | att_identity | enum_values | comment\n--------------+------------+-------------+-----------+-----------------------------+--------------------+--------------------------+------------+------------+-----------+-------------+---------------+--------------+-------------+---------\npublic       | customer   | customer_id | int4      | integer                     |                    | customer_pkey            | t          | f          | f         | f           |               |              |             |\npublic       | customer   | store_id    | int2      | smallint                    |                    |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | first_name  | varchar   | character varying           |                 45 |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | last_name   | varchar   | character varying           |                 45 |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | email       | varchar   | character varying           |                 50 |                          | f          | f          | f         | t           |               |              |             |\npublic       | customer   | address_id  | int2      | smallint                    |                    | customer_address_id_fkey | f          | t          | f         | f           |               |              |             |\npublic       | customer   | activebool  | bool      | boolean                     |                    |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | create_date | date      | date                        |                    |                          | f          | f          | f         | f           |               |              |             |\npublic       | customer   | last_update | timestamp | timestamp without time zone |                    |                          | f          | f          | f         | t           |               |              |             |\npublic       | customer   | active      | int4      | integer                     |                    |                          | f          | f          | f         | t           |               |              |             |\n(10 rows)\n</code></pre> Query<pre><code>select c.table_schema as table_schema,\n  c.table_name as table_name,\n  c.column_name as column_name,\n  c.data_type as type_name,\n  c.data_type as data_type,\n    c.character_maximum_length AS data_type_char_max,\n  k.constraint_name AS constraint_name,\n  (\n    select count(*) &gt; 0\n    from information_schema.KEY_COLUMN_USAGE\n    where table_name = c.table_name\n      and column_name = c.column_name\n      and constraint_name = 'PRIMARY'\n  ) as is_primary,\n  (\n    select count(*) &gt; 0\n    from information_schema.key_column_usage cu\n      left join information_schema.table_constraints tc on tc.constraint_name = cu.constraint_name\n    where cu.column_name = c.column_name\n      and cu.table_name = c.table_name\n      and tc.constraint_type = 'FOREIGN KEY'\n  ) as is_foreign,\n  (\n    select count(*) &gt; 0\n    from information_schema.key_column_usage cu\n      left join information_schema.table_constraints tc on tc.constraint_name = cu.constraint_name\n    where cu.column_name = c.column_name\n      and cu.table_name = c.table_name\n      and tc.constraint_type = 'UNIQUE'\n  ) as is_unique,\n  IF(c.is_nullable = 'YES', 1, 0) as is_nullable,\n  case\n    when c.data_type = 'enum' then REPLACE(\n      REPLACE(\n        REPLACE(REPLACE(c.column_type, 'enum', ''), '\\'', ''),\n        '(',\n        ''\n      ),\n      ')',\n      ''\n    )\n    else ''\n  end as enum_values,\n  c.column_comment as comment\nfrom information_schema.columns c\nLEFT JOIN information_schema.key_column_usage k ON c.table_schema = k.table_schema\nAND c.table_name = k.table_name\n  AND c.column_name = k.column_name\nwhere c.table_name = 'customer' -- replace with your table name\n  and c.TABLE_SCHEMA = 'sakila' -- replace with your schema\norder by c.ordinal_position;\n</code></pre> Output<pre><code>+--------------+------------+-------------+-----------+-----------+--------------------+---------------------+------------+------------+-----------+-------------+-------------+---------+\n| table_schema | table_name | column_name | type_name | data_type | data_type_char_max | constraint_name     | is_primary | is_foreign | is_unique | is_nullable | enum_values | comment |\n+--------------+------------+-------------+-----------+-----------+--------------------+---------------------+------------+------------+-----------+-------------+-------------+---------+\n| sakila       | customer   | customer_id | smallint  | smallint  |               NULL | PRIMARY             |          1 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | store_id    | tinyint   | tinyint   |               NULL | fk_customer_store   |          0 |          1 |         0 |           0 |             |         |\n| sakila       | customer   | first_name  | varchar   | varchar   |                 45 | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | last_name   | varchar   | varchar   |                 45 | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | email       | varchar   | varchar   |                 50 | NULL                |          0 |          0 |         0 |           1 |             |         |\n| sakila       | customer   | address_id  | smallint  | smallint  |               NULL | fk_customer_address |          0 |          1 |         0 |           0 |             |         |\n| sakila       | customer   | active      | tinyint   | tinyint   |               NULL | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | create_date | datetime  | datetime  |               NULL | NULL                |          0 |          0 |         0 |           0 |             |         |\n| sakila       | customer   | last_update | timestamp | timestamp |               NULL | NULL                |          0 |          0 |         0 |           1 |             |         |\n+--------------+------------+-------------+-----------+-----------+--------------------+---------------------+------------+------------+-----------+-------------+-------------+---------+\n9 rows in set (0.28 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#show-primary-keys", "title": "Show Primary Keys", "text": "<p>Primary Keys</p> PostgresMySQL Query<pre><code>SELECT cl.relname as table_name,\n  a.attname AS column_name\nFROM pg_constraint c\n  JOIN pg_namespace ns ON ns.oid = c.connamespace\n  JOIN pg_class cl ON cl.oid = c.conrelid\n  JOIN pg_attribute a ON a.attnum = ANY(c.conkey)\n  AND a.attrelid = c.conrelid\nWHERE (\n    ('public' = '') IS NOT FALSE -- replace with your schema\n    OR ns.nspname = 'public' -- replace with your schema\n  )\n  AND cl.relname = 'address' -- replace with your table name\n  AND c.contype = 'p'\nORDER BY a.attnum;\n</code></pre> Output<pre><code>table_name | column_name\n------------+-------------\naddress    | address_id\n(1 row)\n</code></pre> Query<pre><code>SELECT DISTINCT(column_name) as attname,\n  table_name as table_name,\n  table_schema as table_schema,\n  constraint_name as constraint_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'sakila' -- replace with your schema\n  AND table_name = 'customer' -- replace with your table name\n  AND constraint_name = 'PRIMARY';\n</code></pre> Output<pre><code>+-------------+------------+--------------+-----------------+\n| attname     | table_name | table_schema | constraint_name |\n+-------------+------------+--------------+-----------------+\n| customer_id | customer   | sakila       | PRIMARY         |\n+-------------+------------+--------------+-----------------+\n1 row in set (0.00 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#database-relationships", "title": "Database Relationships", "text": "<p>All Database Relationships</p> PostgresMySQL Query<pre><code>SELECT fk_nsp.nspname || '.' || fk_table AS fk_table,\n  array_agg(\n    fk_att.attname\n    ORDER BY fk_att.attnum\n  ) AS fk_columns,\n  tar_nsp.nspname || '.' || ref_table AS ref_table,\n  array_agg(\n    tar_att.attname\n    ORDER BY fk_att.attnum\n  ) AS ref_columns\nFROM (\n    SELECT fk.oid AS fk_table_id,\n      fk.relnamespace AS fk_schema_id,\n      fk.relname AS fk_table,\n      unnest(con.conkey) as fk_column_id,\n      tar.oid AS target_table_id,\n      tar.relnamespace AS target_schema_id,\n      tar.relname AS ref_table,\n      unnest(con.confkey) as target_column_id,\n      con.connamespace AS constraint_nsp,\n      con.conname AS constraint_name\n    FROM pg_constraint con\n      JOIN pg_class fk ON con.conrelid = fk.oid\n      JOIN pg_class tar ON con.confrelid = tar.oid\n    WHERE con.contype = 'f'\n  ) sub\n  JOIN pg_attribute fk_att ON fk_att.attrelid = fk_table_id\n  AND fk_att.attnum = fk_column_id\n  JOIN pg_attribute tar_att ON tar_att.attrelid = target_table_id\n  AND tar_att.attnum = target_column_id\n  JOIN pg_namespace fk_nsp ON fk_schema_id = fk_nsp.oid\n  JOIN pg_namespace tar_nsp ON target_schema_id = tar_nsp.oid\nGROUP BY 1,\n  3,\n  sub.constraint_nsp,\n  sub.constraint_name;\n</code></pre> Output<pre><code>      fk_table       |     fk_columns     |    ref_table     |  ref_columns\n----------------------+--------------------+------------------+----------------\npublic.address       | {city_id}          | public.city      | {city_id}\npublic.city          | {country_id}       | public.country   | {country_id}\npublic.customer      | {address_id}       | public.address   | {address_id}\npublic.film          | {language_id}      | public.language  | {language_id}\npublic.film_actor    | {actor_id}         | public.actor     | {actor_id}\npublic.film_actor    | {film_id}          | public.film      | {film_id}\npublic.film_category | {category_id}      | public.category  | {category_id}\npublic.film_category | {film_id}          | public.film      | {film_id}\npublic.inventory     | {film_id}          | public.film      | {film_id}\npublic.payment       | {customer_id}      | public.customer  | {customer_id}\npublic.payment       | {rental_id}        | public.rental    | {rental_id}\npublic.payment       | {staff_id}         | public.staff     | {staff_id}\npublic.rental        | {customer_id}      | public.customer  | {customer_id}\npublic.rental        | {inventory_id}     | public.inventory | {inventory_id}\npublic.rental        | {staff_id}         | public.staff     | {staff_id}\npublic.staff         | {address_id}       | public.address   | {address_id}\npublic.store         | {address_id}       | public.address   | {address_id}\npublic.store         | {manager_staff_id} | public.staff     | {staff_id}\n(18 rows)\n</code></pre> Query<pre><code>SELECT concat(table_schema, '.', table_name) AS fk_table,\n  group_concat(\n    column_name\n    ORDER BY ordinal_position\n  ) AS fk_columns,\n  concat(\n    referenced_table_schema,\n    '.',\n    referenced_table_name\n  ) AS ref_table,\n  group_concat(\n    referenced_column_name\n    ORDER BY ordinal_position\n  ) AS ref_columns\nFROM information_schema.key_column_usage\nWHERE referenced_table_schema NOT IN (\n    'information_schema',\n    'performance_schema',\n    'sys',\n    'mysql',\n    'innodb',\n    'tmp'\n  ) AND table_schema = 'sakila' -- replace with your schema\n\nGROUP BY 1,\n  3,\n  constraint_schema,\n  constraint_name;\n</code></pre> Output<pre><code>+----------------------+----------------------+------------------+--------------+\n| fk_table             | fk_columns           | ref_table        | ref_columns  |\n+----------------------+----------------------+------------------+--------------+\n| sakila.address       | city_id              | sakila.city      | city_id      |\n| sakila.city          | country_id           | sakila.country   | country_id   |\n| sakila.customer      | address_id           | sakila.address   | address_id   |\n| sakila.customer      | store_id             | sakila.store     | store_id     |\n| sakila.film          | language_id          | sakila.language  | language_id  |\n| sakila.film          | original_language_id | sakila.language  | language_id  |\n| sakila.film_actor    | actor_id             | sakila.actor     | actor_id     |\n| sakila.film_actor    | film_id              | sakila.film      | film_id      |\n| sakila.film_category | category_id          | sakila.category  | category_id  |\n| sakila.film_category | film_id              | sakila.film      | film_id      |\n| sakila.inventory     | film_id              | sakila.film      | film_id      |\n| sakila.inventory     | store_id             | sakila.store     | store_id     |\n| sakila.payment       | customer_id          | sakila.customer  | customer_id  |\n| sakila.payment       | rental_id            | sakila.rental    | rental_id    |\n| sakila.payment       | staff_id             | sakila.staff     | staff_id     |\n| sakila.rental        | customer_id          | sakila.customer  | customer_id  |\n| sakila.rental        | inventory_id         | sakila.inventory | inventory_id |\n| sakila.rental        | staff_id             | sakila.staff     | staff_id     |\n| sakila.staff         | address_id           | sakila.address   | address_id   |\n| sakila.staff         | store_id             | sakila.store     | store_id     |\n| sakila.store         | address_id           | sakila.address   | address_id   |\n| sakila.store         | manager_staff_id     | sakila.staff     | staff_id     |\n+----------------------+----------------------+------------------+--------------+\n22 rows in set (0.02 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#forward-relationships", "title": "Forward Relationships", "text": "<pre><code>erDiagram\n\npayment {\n        numeric amount\n        smallint customer_id FK\n        timestamp_without_time_zone payment_date\n        integer payment_id PK\n        integer rental_id FK\n        smallint staff_id FK\n    }\n\ncustomer {\n        integer active\n        boolean activebool\n        smallint address_id FK\n        date create_date\n        integer customer_id PK\n        character_varying email\n        character_varying first_name\n        character_varying last_name\n        timestamp_without_time_zone last_update\n        smallint store_id\n    }\n\nrental {\n    smallint customer_id FK\n    integer inventory_id FK\n    timestamp_without_time_zone last_update\n    timestamp_without_time_zone rental_date\n    integer rental_id PK\n    timestamp_without_time_zone return_date\n    smallint staff_id FK\n}\n\nstaff {\n    boolean active\n    smallint address_id FK\n    character_varying email\n    character_varying first_name\n    character_varying last_name\n    timestamp_without_time_zone last_update\n    character_varying password\n    bytea picture\n    integer staff_id PK\n    smallint store_id\n    character_varying username\n}\n\npayment }o--|| customer : \"customer_id\"\nrental }o--|| customer : \"customer_id\"\npayment }o--|| rental : \"rental_id\"\npayment }o--|| staff : \"staff_id\"\nrental }o--|| staff : \"staff_id\"\n</code></pre> <p>Forward Relationships</p> PostgresMySQL Query<pre><code>SELECT\n  COALESCE(conname, '') AS constraint_name,\n  conrelid::regclass AS table_name,\n  a.attname AS column_name,\n  confrelid::regclass AS referenced_table_name,\n  COALESCE(af.attname, '') AS referenced_column_name\nFROM\n  pg_constraint c\nJOIN\n  pg_namespace n ON n.oid = c.connamespace\nJOIN\n  pg_class cl ON cl.oid = c.conrelid\nJOIN\n  pg_attribute a ON a.attnum = ANY(c.conkey) AND a.attrelid = c.conrelid\nJOIN\n  pg_class conf ON conf.oid = c.confrelid\nJOIN\n  pg_attribute af ON af.attnum = ANY(c.confkey) AND af.attrelid = c.confrelid\nWHERE\n  n.nspname = 'public' -- replace with your schema\n  AND cl.relname = 'payment' -- replace with your table name\n  AND c.contype = 'f'\nORDER BY\n  constraint_name;\n</code></pre> Output<pre><code>    constraint_name      | table_name | column_name | referenced_table_name | referenced_column_name\n--------------------------+------------+-------------+-----------------------+------------------------\npayment_customer_id_fkey | payment    | customer_id | customer              | customer_id\npayment_rental_id_fkey   | payment    | rental_id   | rental                | rental_id\npayment_staff_id_fkey    | payment    | staff_id    | staff                 | staff_id\n(3 rows)\n</code></pre> Query<pre><code>SELECT coalesce(table_name, '') as table_name,\n  coalesce (column_name, '') as column_name,\n  coalesce(referenced_column_name, '') as referenced_column_name,\n  coalesce(referenced_table_name, '') as referenced_table_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'sakila' -- replace with your schema\n  AND table_name = 'payment' -- replace with your table name\n  AND referenced_table_name != 'NULL';\n</code></pre> Output<pre><code>+------------+-------------+------------------------+-----------------------+\n| table_name | column_name | referenced_column_name | referenced_table_name |\n+------------+-------------+------------------------+-----------------------+\n| payment    | customer_id | customer_id            | customer              |\n| payment    | rental_id   | rental_id              | rental                |\n| payment    | staff_id    | staff_id               | staff                 |\n+------------+-------------+------------------------+-----------------------+\n3 rows in set (0.01 sec)\n</code></pre>"}, {"location": "articles/useful-postgres-mysql-queries/#reverse-relationships", "title": "Reverse Relationships", "text": "<p>ER Diagram</p> <pre><code>erDiagram\n\npayment {\n        numeric amount\n        smallint customer_id FK\n        timestamp_without_time_zone payment_date\n        integer payment_id PK\n        integer rental_id FK\n        smallint staff_id FK\n    }\n\ncustomer {\n        integer active\n        boolean activebool\n        smallint address_id FK\n        date create_date\n        integer customer_id PK\n        character_varying email\n        character_varying first_name\n        character_varying last_name\n        timestamp_without_time_zone last_update\n        smallint store_id\n    }\n\nrental {\n    smallint customer_id FK\n    integer inventory_id FK\n    timestamp_without_time_zone last_update\n    timestamp_without_time_zone rental_date\n    integer rental_id PK\n    timestamp_without_time_zone return_date\n    smallint staff_id FK\n}\n\n\npayment }o--|| customer : \"customer_id\"\nrental }o--|| customer : \"customer_id\"\npayment }o--|| rental : \"rental_id\"</code></pre> <p>Reverse Relationships</p> PostgresMySQL Query<pre><code>SELECT\n  COALESCE(conname, '') AS constraint_name,\n  conrelid::regclass AS table_name,\n  a.attname AS column_name,\n  confrelid::regclass AS referenced_table_name,\n  COALESCE(af.attname, '') AS referenced_column_name\nFROM\n  pg_constraint c\nJOIN\n  pg_namespace n ON n.oid = c.connamespace\nJOIN\n  pg_class cl ON cl.oid = c.conrelid\nJOIN\n  pg_attribute a ON a.attnum = ANY(c.conkey) AND a.attrelid = c.conrelid\nJOIN\n  pg_class conf ON conf.oid = c.confrelid\nJOIN\n  pg_attribute af ON af.attnum = ANY(c.confkey) AND af.attrelid = c.confrelid\nWHERE\n  n.nspname = 'public' -- replace with your schema\n  AND\n  confrelid::regclass::text = 'customer' -- replace with your table name\nORDER BY\n  constraint_name;\n</code></pre> Output<pre><code>    constraint_name      | table_name | column_name | referenced_table_name | referenced_column_name\n--------------------------+------------+-------------+-----------------------+------------------------\npayment_customer_id_fkey | payment    | customer_id | customer              | customer_id\nrental_customer_id_fkey  | rental     | customer_id | customer              | customer_id\n(2 rows)\n</code></pre> Query<pre><code>SELECT coalesce(table_name, '') as table_name,\n  coalesce (column_name, '') as column_name,\n  coalesce(referenced_column_name, '') as referenced_column_name,\n  coalesce(referenced_table_name, '') as referenced_table_name\nFROM information_schema.key_column_usage\nWHERE table_schema = 'sakila' -- replace with your schema\n  AND referenced_table_name = 'customer'; -- replace with your table name\n</code></pre> Output<pre><code>+------------+-------------+------------------------+-----------------------+\n| table_name | column_name | referenced_column_name | referenced_table_name |\n+------------+-------------+------------------------+-----------------------+\n| payment    | customer_id | customer_id            | customer              |\n| rental     | customer_id | customer_id            | customer              |\n+------------+-------------+------------------------+-----------------------+\n2 rows in set (0.00 sec)\n</code></pre>"}, {"location": "cloud-storage-engines/amazon-s3/", "title": "Amazon S3 Storage Engine", "text": ""}, {"location": "cloud-storage-engines/amazon-s3/#overview", "title": "Overview", "text": "<p>The Amazon S3 Storage Engine allows developers to store large amounts of unstructured data using the Amazon Web Services S3 service.</p>"}, {"location": "cloud-storage-engines/amazon-s3/#requirements", "title": "Requirements", "text": "<p>To use the Amazon S3 Storage engine, you must have a Amazon Web Services account. You will need to create a programmatic IAM user and provide the access key and secret key to the DBSnapper Cloud Storage Profile page. You will also need to provide the AWS region that you're using for your S3 storage.</p> <p>Create a bucket in your Amazon S3 account and note the bucket name. You can specify an optional bucket prefix that you can use to organize your snapshots.</p>"}, {"location": "cloud-storage-engines/cloudflare-r2/", "title": "Cloudflare R2 Storage Engine", "text": ""}, {"location": "cloud-storage-engines/cloudflare-r2/#overview", "title": "Overview", "text": "<p>The Cloudflare R2 Storage allows developers to store large amounts of unstructured data without the costly egress bandwidth fees associated with typical cloud storage services. It uses an Amazon S3 compatible API to store data with high durability and availability.</p>"}, {"location": "cloud-storage-engines/cloudflare-r2/#requirements", "title": "Requirements", "text": "<p>To use the Cloudflare R2 Storage engine, you must have a Cloudflare R2 account. You can sign up for a free Cloudflare account at https://dash.cloudflare.com/sign-up. You will then need to generate an API key that consists of an access key and secret key. Using these credentials and your cloudflare account id.</p> <p>Create a bucket in your Cloudflare R2 account and note the bucket name. You can specify an optional bucket prefix that you can use to organize your snapshots.</p>"}, {"location": "cloud-storage-engines/configuration/", "title": "Storage Engine Configuration", "text": "<p>You specify configurations for storage engines in the DBSnapper configuration file under the <code>storage_profiles</code> section. These storage engines can be used with sharing targets to retrieve shared snapshots from cloud storage.</p> <p>Several variations of <code>storage_profile</code> configurations are shown in the following example for different cloud storage providers and methods of retrieving credentials.</p>"}, {"location": "cloud-storage-engines/configuration/#example", "title": "Example", "text": "<p><code>storage profiles</code> configuration</p> <pre><code># Storage profiles for sharing configurations\nstorage_profiles:\n  # All Profiles:\n  # Leave access_key and secret_key to use a profile\n  # Leave the awscli_profile empty to use the default profile\n  # If the endpoint is specified in the profile it will override the\n  # endpoint specified in this configuration.\n\n  # S3\n  s3-from-credentials:\n    provider: s3\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    region: &lt;region&gt;\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  s3-from-awscli-profile:\n    provider: s3\n    awscli_profile: dbsnapper_credentials\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  # R2\n  # Uses the account ID to create the endpoint url.\n  # Can be omitted if the r2 endpoint_url is specified in the awscli config.\n  # Or set the following Env Variables and leave the awscli_profile empty:\n  # AWS_ACCESS_KEY\n  # AWS_SECRET_KEY\"\n  # AWS_ENDPOINT_URL\"\n  r2-from-credentials:\n    provider: r2\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    account_id: &lt;account_id&gt;\n    bucket: dbsnapper-test-r2\n    prefix:\n\n  r2-from-awscli-profile:\n    provider: r2\n    awscli_profile: r2_production\n    account_id: # optional if enpoint_url set in profile\n    bucket: dbsnapper-test-r2\n    prefix: \n\n  r2-from-env-or-default-awscli_profile:\n    provider: r2\n    bucket: dbsnapper-test-r2\n    prefix: sanitized\n\n  # Minio\n  minio-from-credentials:\n    provider: minio\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    endpoint: http://localhost:9000\n    bucket: dbsnapper-test-minio\n    prefix:\n\n  # Digital Ocean Spaces\n  # Endpoint shold be set to the endpoint of the spaces region i.e. nyc3 below\n  dospaces-from-credentials:\n    provider: dospaces\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    endpoint: https://nyc3.digitaloceanspaces.com\n    bucket: dbsnapper-test-do\n    prefix:\n</code></pre>"}, {"location": "cloud-storage-engines/configuration/#configuration-options", "title": "Configuration Options", "text": "<p>Storage profile configuration options essentially consist of a section to configure credentials and service endpoints, and another section for specifying the <code>bucket</code> and optional <code>prefix</code> for snapshots.</p> <p>Storage Profile configuration options (* denotes required)</p> <code>provider</code> * <p>The name of the cloud storage provider. Supported providers are <code>s3</code>, <code>r2</code>, <code>minio</code>, and <code>dospaces</code></p> <code>awscli_profile</code> <p>If you have configured the AWS CLI with named profiles, you can specify the profile name here. If this is not specified and the <code>access_key</code> and <code>secret_key</code> are omitted, environment variables or the default profile will be used.</p> <code>access_key</code> <p>The access key for the cloud storage provider. This can be omitted if the <code>awscli_profile</code> is specified or you have the appropriate environment variables set</p> <code>secret_key</code> <p>The secret key for the cloud storage provider. This can be omitted if the <code>awscli_profile</code> is specified or you have the appropriate environment variables set</p> <code>region</code> <p>The region for the cloud storage provider. This is required for AWS S3 storage profiles.</p> <code>account_id</code> <p>The account ID for the Cloudflare R2 storage provider. This is required for R2 storage profiles and is used in creating the endpoint URL.</p> <code>endpoint</code> <p>The endpoint URL for the cloud storage provider. This is required for Minio and Digital Ocean Spaces storage profiles, and is automatically generated for R2 storage profiles using the account ID.</p> <code>bucket</code> * <p>The name of the bucket in the cloud storage provider where snapshots will be stored.</p> <code>prefix</code> <p>An optional prefix to use for the snapshots in the cloud storage provider. This can be used to organize snapshots in the bucket.</p>"}, {"location": "cloud-storage-engines/configuration/#aws-shared-configuration", "title": "AWS Shared Configuration", "text": "<p>DBSnapper supports retrieving credentials from the AWS CLI shared configuration.</p>"}, {"location": "cloud-storage-engines/configuration/#credentials-retrieval", "title": "Credentials Retrieval", "text": "<p>Credentials are retrieved from the shared configuration in the following order:</p> <ol> <li>If the <code>access_key</code> and <code>secret_key</code> are specified in the storage profile configuration, those credentials will be used and the shared configuration will be ignored.</li> <li>The envirnoment variables <code>AWS_ACCESS_KEY</code> and <code>AWS_SECRET_KEY</code> will be used if they are set. Along with <code>AWS_REGION</code> and <code>AWS_ENDPOINT_URL</code> if they are needed for the storage provider.</li> <li>If the <code>awscli_profile</code> is specified in the storage profile configuration, the credentials for the specified profile will be used.</li> <li>If the <code>awscli_profile</code> is not specified and the above values are not present, the default profile (if it exists) will be used.</li> </ol>"}, {"location": "cloud-storage-engines/configuration/#environment-variables", "title": "Environment Variables", "text": "<p>If the <code>awscli_profile</code> is not specified and the <code>access_key</code> and <code>secret_key</code> are not set, the following environment variables can be used to set the credentials:</p> <ul> <li><code>AWS_ACCESS_KEY</code></li> <li><code>AWS_SECRET</code></li> <li><code>AWS_REGION</code></li> <li><code>AWS_ENDPOINT_URL</code></li> </ul> <p>See the AWS CLI shared configuration documentation for more information on supported environment variables.</p>"}, {"location": "cloud-storage-engines/configuration/#endpoint-url", "title": "Endpoint URL", "text": "<p>Non AWS S3 storage providers require the <code>endpoint</code> to be specified in the storage profile configuration. This is the URL for the storage provider's API. For R2 storage profiles, the endpoint URL is automatically generated using the account ID.</p> <p>You may also provide the endpoint URL in the AWS CLI shared configuration. If the endpoint is specified in the profile, it will override the endpoint specified in the configuration.</p>"}, {"location": "cloud-storage-engines/introduction/", "title": "Introduction", "text": "<p>DBSnapper provides support for several Cloud Storage Engines that allow you to use your own private cloud storage provider to store your database snapshots keeping them in your control and secure.</p> <p>Cloud storage engine configurations can be specified on the DBSnapper Cloud and in the configuration file under the <code>storage_profiles</code> section. These storage engines can be used with sharing targets to retrieve shared snapshots from cloud storage.</p> <p> </p>"}, {"location": "cloud-storage-engines/introduction/#supported-cloud-storage-engines", "title": "Supported Cloud Storage Engines", "text": "<p>Currently supported Cloud Storage engines include:</p> <ul> <li>Amazon S3</li> <li>Cloudflare R2</li> <li>Minio</li> <li>Digital Ocean Spaces</li> </ul>"}, {"location": "cloud-storage-engines/introduction/#getting-started", "title": "Getting Started", "text": "<p>To get started with using a cloud storage engine, you will need to configure a <code>storage_profile</code> in your DBSnapper configuration file. The <code>storage_profile</code> configuration specifies the cloud storage provider, credentials, and bucket information for the storage engine.</p> <p>AWS S3 configuration examples</p> <pre><code>storage_profiles:\n  s3-with-provided-credentials:\n    provider: s3\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    region: &lt;region&gt;\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  s3-from-awscli-shared-profile:\n    provider: s3\n    awscli_profile: dbsnapper_credentials\n    bucket: dbsnapper-test-s3\n    prefix:\n\ntargets:\n  # Share target\n  shared-s3:\n    name: shared-s3\n    share:\n      storage_profile: s3-from-awscli-shared-profile\n      dst_url: postgres://localhost:5432/dbsnapper_test\n</code></pre> <p>In the example above we have two <code>storage_profiles</code> configurations for AWS S3. The first configuration <code>s3-with-provided-credentials</code> explicitly specifies the access key, secret key, region, and bucket for an S3 storage engine. <code>s3-from-awscli-shared-profile</code>, on the other hand, indicates that we want to retrieve the credentials from the <code>dbsnapper_credentials</code> AWS shared configuration profile as specified in the <code>awscli_profile</code> field.</p> <p>We have also defined a <code>share target</code> configuration on line 19 that uses the <code>s3-from-awscli-shared-profile</code> storage profile to access shared snapshots from the <code>dbsnapper-test-s3</code> bucket. These shared snapshots will be loaded into the <code>dbsnapper_test</code> database as specified in the <code>dst_url</code> field on line 23.</p>"}, {"location": "cmd/dbsnapper/", "title": "dbsnapper", "text": "<p>Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</p>"}, {"location": "cmd/dbsnapper/#synopsis", "title": "Synopsis", "text": "<p>DBSnapper is a database snapshot creation and management tool for developers.      DBSnapper makes it easy to snapshot, subset, sanitize and share your database to speed up application development and testing with real-world data.</p> <pre><code>You can specify your database targets and other configuration properties in the `~/.config/dbsnapper/dbsnapper.yml` configuration file.\n</code></pre>"}, {"location": "cmd/dbsnapper/#options", "title": "Options", "text": "<pre><code>      --config string   config file (default is $HOME/.config/dbsnapper/dbsnapper.yml)\n</code></pre>"}, {"location": "cmd/dbsnapper_auth_token/", "title": "dbsnapper auth token", "text": ""}, {"location": "cmd/dbsnapper_auth_token/#dbsnapper-auth-token_1", "title": "dbsnapper auth token", "text": "<p>Add DBSnapper Cloud Api Token</p>"}, {"location": "cmd/dbsnapper_auth_token/#synopsis", "title": "Synopsis", "text": "<p>This comand adds the DBSnapper Cloud API token to your configuration file.      You can find this token on the Get Started page of the DBSnapper Cloud app.</p> <pre><code>dbsnapper auth token &lt;api_token&gt; [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_auth_token/#examples", "title": "Examples", "text": "<pre><code>dbsnapper auth token hNsdelkjcxgoiuwalkmcgoi...\n</code></pre>"}, {"location": "cmd/dbsnapper_auth_token/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for token\n</code></pre>"}, {"location": "cmd/dbsnapper_auth_token/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper auth    - Login to DBSnapper Cloud</li> </ul>"}, {"location": "cmd/dbsnapper_build/", "title": "dbsnapper build", "text": ""}, {"location": "cmd/dbsnapper_build/#dbsnapper-build_1", "title": "dbsnapper build", "text": "<p>Build a database snapshot</p>"}, {"location": "cmd/dbsnapper_build/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper build</code> command creates a database snapshot from a configured target. The command connects to the source database specified in the target configuration and creates a compressed snapshot file that can be loaded later using the <code>load</code> command.</p> <p>The <code>target_name</code> is the name of the target defined in the configuration file. The snapshot is built from the connection string specified in the <code>src_url</code> parameter of the target configuration.</p>"}, {"location": "cmd/dbsnapper_build/#snapshot-creation-process", "title": "Snapshot Creation Process", "text": "<p>The build command performs the following operations:</p> <ol> <li>Connects to Source Database - Uses the <code>src_url</code> from target configuration</li> <li>Creates Database Dump - Uses appropriate database tools (pg_dump, mysqldump)</li> <li>Applies Schema Filtering - PostgreSQL only, based on schema configuration</li> <li>Creates Compressed Archive - Saves as <code>.zip</code> file in working directory</li> <li>Uploads to Cloud Storage - If storage profile is configured</li> <li>Applies Sanitization - If sanitization queries are configured</li> </ol>"}, {"location": "cmd/dbsnapper_build/#output-files", "title": "Output Files", "text": "<p>Snapshots are created with timestamps and stored locally:</p> <ul> <li>Local Snapshot: <code>{timestamp}_{target_name}.zip</code></li> <li>Sanitized Snapshot: <code>{timestamp}_{target_name}.san.zip</code> (if sanitization configured)</li> <li>Cloud Storage: Uploaded with UUID filename if storage profile exists</li> </ul>"}, {"location": "cmd/dbsnapper_build/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>The build command supports advanced schema filtering for PostgreSQL databases to control which schemas are included in the snapshot:</p> <p>Schema Configuration Options:</p> <ul> <li>No Configuration: Defaults to <code>public</code> schema only</li> <li>Use Default Schema (<code>use_default_schema: true</code>): Builds only the database's default schema</li> <li>Use All Schemas (<code>use_default_schema: false</code>): Builds all available schemas in the database</li> <li>Include Specific Schemas (<code>include_schemas</code>): Builds only the specified schemas that exist</li> <li>Exclude Specific Schemas (<code>exclude_schemas</code>): Builds all schemas except the excluded ones</li> </ul> <p>Schema Analysis: - The build process dynamically analyzes available schemas in the source database - Only existing schemas will be included in the snapshot - Invalid or non-existent schemas are ignored with warnings - Schema filtering configuration is preserved for load operations</p> <p>PostgreSQL Only</p> <p>Schema filtering is only supported for PostgreSQL databases. MySQL databases use standard mysqldump without schema-level filtering.</p>"}, {"location": "cmd/dbsnapper_build/#storage-integration", "title": "Storage Integration", "text": "<p>If a target has a configured storage profile, the build command will:</p> <ol> <li>Create the local snapshot file</li> <li>Upload to cloud storage using the storage profile credentials</li> <li>Generate a cloud snapshot entry accessible via DBSnapper Cloud</li> <li>Maintain local copy for immediate use</li> </ol> <p>Supported Storage Providers: - Amazon S3 - Cloudflare R2 - MinIO - S3-Compatible providers</p>"}, {"location": "cmd/dbsnapper_build/#sanitization-integration", "title": "Sanitization Integration", "text": "<p>The build command integrates with sanitization functionality:</p> <p>Sanitization Priority (highest to lowest): 1. Target-level <code>override_query</code>  2. Global <code>override.san_query</code> 3. Target <code>query_file</code></p> <p>Sanitization Process: - Original snapshot is created first - Sanitization queries are applied to create <code>.san.zip</code> version - Both original and sanitized versions are available for loading - Base64-encoded queries are automatically detected and decoded</p> <p>Sanitization Required for Sharing</p> <p>If you plan to share snapshots or use cloud storage, ensure proper sanitization is configured to remove sensitive data.</p>"}, {"location": "cmd/dbsnapper_build/#configuration-examples", "title": "Configuration Examples", "text": "<p>Basic target configuration: <pre><code>targets:\n  production-db:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod.example.com/myapp\"\n      dst_url: \"postgresql://user:pass@localhost/myapp_dev\"\n</code></pre></p> <p>Schema filtering configuration: <pre><code>targets:\n  my-target:\n    snapshot:\n      src_url: \"postgresql://user:pass@localhost/mydb\"\n      dst_url: \"postgresql://user:pass@localhost/mydb_copy\"\n      schema_config:\n        use_default_schema: false         # Build all schemas\n        # OR\n        include_schemas: [\"public\", \"app_data\"]  # Build only these schemas\n        # OR  \n        exclude_schemas: [\"temp_schema\"]   # Build all except these schemas\n</code></pre></p> <p>With storage profile and sanitization: <pre><code>targets:\n  production-db:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod.example.com/myapp\"\n      dst_url: \"postgresql://user:pass@localhost/myapp_dev\"\n      storage_profile: \"s3-production\"\n    sanitize:\n      query_file: \"sanitize-queries.sql\"\n</code></pre></p>"}, {"location": "cmd/dbsnapper_build/#safety-features", "title": "Safety Features", "text": "<p>The build command includes several safety and reliability features:</p> <ul> <li>Connection Validation - Verifies database connectivity before starting</li> <li>Schema Validation - Checks that specified schemas exist (PostgreSQL)</li> <li>Storage Validation - Tests cloud storage credentials before upload</li> <li>Error Recovery - Maintains local snapshots even if cloud upload fails</li> <li>Atomic Operations - Ensures snapshots are complete before making available</li> </ul>"}, {"location": "cmd/dbsnapper_build/#examples", "title": "Examples", "text": "<p>Basic snapshot creation: <pre><code>dbsnapper build production-db           # Build snapshot of production-db target\ndbsnapper build staging-db              # Build snapshot of staging-db target\n</code></pre></p> <p>Build with custom configuration: <pre><code>dbsnapper build --config custom.yml production-db  # Use custom config file\n</code></pre></p> <p>Performance Considerations</p> <ul> <li>Large databases may take significant time to snapshot</li> <li>Consider running during off-peak hours for production databases</li> <li>Cloud upload time depends on file size and network connectivity</li> <li>Use schema filtering to reduce snapshot size when appropriate</li> </ul> <p>Database Permissions</p> <p>The build command requires appropriate database permissions:</p> <p>PostgreSQL: SELECT permissions on all tables, USAGE on schemas MySQL: SELECT permissions on all databases/tables being snapshotted</p> <pre><code>dbsnapper build &lt;target_name&gt; [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_build/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for build\n</code></pre>"}, {"location": "cmd/dbsnapper_build/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> <li>dbsnapper load - Load a target snapshot or SQL file to a database</li> <li>dbsnapper sanitize - Used to sanitize a snapshot</li> </ul>"}, {"location": "cmd/dbsnapper_config_check/", "title": "dbsnapper config check", "text": ""}, {"location": "cmd/dbsnapper_config_check/#dbsnapper-config-check_1", "title": "dbsnapper config check", "text": "<p>Check the configuration and required dependencies</p>"}, {"location": "cmd/dbsnapper_config_check/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper config check</code> command validates that you have the required dependencies to run dbsnapper. </p> <pre><code>dbsnapper config check [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_config_check/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for check\n</code></pre>"}, {"location": "cmd/dbsnapper_config_check/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper config    - Configuration commands</li> </ul>"}, {"location": "cmd/dbsnapper_config_init/", "title": "dbsnapper config init", "text": ""}, {"location": "cmd/dbsnapper_config_init/#dbsnapper-config-init_1", "title": "dbsnapper config init", "text": "<p>Generate dbsnapper configuration file and create working directory</p>"}, {"location": "cmd/dbsnapper_config_init/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper config init</code> command generates, a dbsnapper configuration file and working directory. </p> <p>It also generates a <code>secret_key</code>used to encrypt sensitive data in the configuration file.</p> <pre><code>dbsnapper config init [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_config_init/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for init\n</code></pre>"}, {"location": "cmd/dbsnapper_config_init/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper config    - Configuration commands</li> </ul>"}, {"location": "cmd/dbsnapper_load/", "title": "dbsnapper load", "text": ""}, {"location": "cmd/dbsnapper_load/#dbsnapper-load_1", "title": "dbsnapper load", "text": "<p>Load a target snapshot or SQL file to a database</p>"}, {"location": "cmd/dbsnapper_load/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper load</code> command loads a snapshot or SQL file to a database and takes a <code>target_name</code> and optional arguments.</p> <p>The <code>target_name</code> is the name of the target defined in the configuration file.</p>"}, {"location": "cmd/dbsnapper_load/#snapshot-loading", "title": "Snapshot Loading", "text": "<p>Load database snapshots created with the <code>build</code> command:</p> <ul> <li><code>dbsnapper load target_name</code> - Load most recent snapshot to dst_url</li> <li><code>dbsnapper load target_name 2</code> - Load snapshot by index to dst_url  </li> <li><code>dbsnapper load target_name --original</code> - Load original (non-sanitized) snapshot</li> <li><code>dbsnapper load target_name --destdb &lt;url&gt;</code> - Load to custom destination database</li> </ul> <p>The <code>snapshot_index</code> is the index number of the snapshot to load (0 = most recent).</p>"}, {"location": "cmd/dbsnapper_load/#sql-file-loading", "title": "SQL File Loading", "text": "<p>Load SQL files directly to target databases:</p> <ul> <li><code>dbsnapper load target_name file.sql</code> - Load SQL file to dst_url (default)</li> <li><code>dbsnapper load target_name file.sql --source</code> - Load SQL file to src_url</li> <li><code>dbsnapper load target_name file.sql --destdb &lt;url&gt;</code> - Load to custom destination</li> <li><code>dbsnapper load target_name file.sql --no-drop</code> - Execute against existing database</li> <li><code>dbsnapper load target_name file.sql --dry-run</code> - Preview execution without running</li> </ul> <p>SQL files are automatically detected by the <code>.sql</code> extension (case-insensitive).</p>"}, {"location": "cmd/dbsnapper_load/#execution-modes", "title": "Execution Modes", "text": "<p>Replace Mode (default): Drops and recreates the target database before loading.</p> <p>Append Mode (--no-drop): Executes SQL against the existing database without dropping it.</p>"}, {"location": "cmd/dbsnapper_load/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>When loading snapshots, the load command respects the schema configuration for PostgreSQL databases:</p> <p>Schema Configuration Options:</p> <ul> <li>No Configuration: Defaults to <code>public</code> schema only</li> <li>Use Default Schema (<code>use_default_schema: true</code>): Loads only the default schema</li> <li>Use All Schemas (<code>use_default_schema: false</code>): Loads all schemas present in the snapshot</li> <li>Include Specific Schemas (<code>include_schemas</code>): Loads only specified schemas that exist in the snapshot</li> <li>Exclude Specific Schemas (<code>exclude_schemas</code>): Loads all schemas except the excluded ones</li> </ul> <p>Important Notes:</p> <ul> <li>Only schemas that exist in the snapshot will be loaded (no empty schemas created)</li> <li>If requested schemas don't exist in the snapshot, warnings will be displayed</li> <li>The load operation analyzes snapshot contents to determine available schemas</li> <li>Schema filtering applies to PostgreSQL snapshot loading only, not SQL file loading or MySQL</li> <li>MySQL databases use standard restore without schema filtering</li> </ul>"}, {"location": "cmd/dbsnapper_load/#safety-features", "title": "Safety Features", "text": "<p>The command includes multiple safety features:</p> <ul> <li>Interactive Confirmation: Prompts before destructive operations</li> <li>Dry Run Mode: Preview SQL execution without making changes  </li> <li>File Validation: Validates SQL files before execution</li> <li>Transaction Support: Wraps SQL execution in transactions for rollback capability</li> </ul> <p>To skip confirmations in automated environments, set <code>CI=true</code> or <code>DBSNAPPER_NO_CONFIRM=true</code>.</p> <p>Sanitized Snapshots</p> <p>If a sanitized snapshot exists at the specified index, it will be loaded unless the <code>--original</code> flag is set.</p> <p>Destructive Operation</p> <p>This is a destructive operation and any database that exists at the target URL  will be dropped and recreated! Use <code>--no-drop</code> flag with SQL files to  execute against existing database.</p> <p>Database URL Priority</p> <p>Database URL resolution follows this priority: 1. <code>--destdb</code> flag (highest priority) 2. <code>--source</code> flag (determines src_url vs dst_url) 3. Configuration overrides 4. Target configuration defaults</p>"}, {"location": "cmd/dbsnapper_load/#examples", "title": "Examples", "text": "<p>Load snapshots: <pre><code>dbsnapper load production-db              # Load latest snapshot\ndbsnapper load production-db 5            # Load snapshot at index 5\ndbsnapper load production-db --original   # Load original snapshot\n</code></pre></p> <p>Load SQL files: <pre><code>dbsnapper load dev-db schema.sql          # Load schema to dst_url\ndbsnapper load dev-db data.sql --source   # Load data to src_url  \ndbsnapper load dev-db migration.sql --dry-run  # Preview migration\n</code></pre></p> <p>Safety examples: <pre><code>dbsnapper load prod-db update.sql --no-drop    # Execute without dropping\nDBSNAPPER_NO_CONFIRM=true dbsnapper load ...   # Skip confirmations\n</code></pre></p> <p>Schema filtering configuration example: <pre><code>targets:\n  my-target:\n    snapshot:\n      schema_config:\n        use_default_schema: false         # Load all schemas from snapshot\n        # OR\n        include_schemas: [\"public\", \"app_data\"]  # Load only these schemas\n        # OR  \n        exclude_schemas: [\"temp_schema\"]   # Load all except these schemas\n</code></pre></p> <pre><code>dbsnapper load target_name [snapshot_index|file.sql] [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_load/#options", "title": "Options", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will Overwrite any existing destination database!!!\n      --dry-run         Show execution plan without running (SQL files only)\n  -h, --help            help for load\n      --no-drop         Skip dropping/recreating database (SQL files only)\n      --original        Use the original snapshot instead of the sanitized version (snapshots only)\n      --source          Load SQL file to source database instead of destination\n</code></pre>"}, {"location": "cmd/dbsnapper_load/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> </ul>"}, {"location": "cmd/dbsnapper_pull/", "title": "dbsnapper pull", "text": ""}, {"location": "cmd/dbsnapper_pull/#dbsnapper-pull_1", "title": "dbsnapper pull", "text": "<p>Retrieve a snapshot and store it locally</p>"}, {"location": "cmd/dbsnapper_pull/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper pull</code> command downloads a snapshot from the DBsnapper Cloud or finds it locally. the <code>pull</code> command takes  <code>target_name</code> and <code>snapshot_index</code> as arguments.</p> <p>The <code>target_name</code> is the name of the target defined in the configuration file.</p> <p>The <code>snapshot_index</code> is the index number of the snapshot to load.</p> <p>Note</p> <p>If a sanitized snapshot exists at this <code>snapshot_index</code>, it will be loaded unless the <code>--original</code> flag is set.</p> <pre><code>dbsnapper pull &lt;target_name&gt; [snapshot_index] [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_pull/#options", "title": "Options", "text": "<pre><code>  -h, --help       help for pull\n  -o, --original   Use the original snapshot instead of the sanitized version.\n</code></pre>"}, {"location": "cmd/dbsnapper_pull/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> </ul>"}, {"location": "cmd/dbsnapper_sanitize/", "title": "dbsnapper sanitize", "text": ""}, {"location": "cmd/dbsnapper_sanitize/#dbsnapper-sanitize_1", "title": "dbsnapper sanitize", "text": "<p>Used to sanitize a snapshot</p>"}, {"location": "cmd/dbsnapper_sanitize/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper sanitize</code> command sanitizes the specified database snapshot and takes a <code>target_name</code> and <code>snapshot_index</code> as arguments.</p> <p>The <code>target_name</code> is the name of the target defined in the configuration file.</p> <p>The <code>snapshot_index</code> is the index number of the snapshot to load.</p> <p>Note</p> <p>The target configuration must specifiy a <code>query_file</code> configuration parameter that specifies the name of the file (located in the <code>working_directory</code>) that contains the sanitization query</p> <p>This command will use the database specified in the <code>sanitize: dst_url:</code> configuration parameter, load the specified snapshot into the database,  and then sanitize the database using the specified query. If <code>sanitize: dst_url:</code> is not specified, the command will use an ephemeral database via docker container.</p> <p>If you would like to create a new snapshot set (unsanitized and sanitized snapshots), you can use the <code>-n</code> flag.  This will create a new snapshot set for the target.</p> <p>If you want to force ephemeral sanitization, you can use the <code>-e</code> flag.  This will force the command to use an ephemeral database for sanitization, overriding the <code>sanitize: dst_url</code> configuration parameter.</p> <p>The resulting sanitized database will be dumped to a file in the <code>working_directory</code> specified in the conifiguration and will be associated with the snapshot.</p> <pre><code>dbsnapper sanitize [flags] target_name snapshot_index\n</code></pre>"}, {"location": "cmd/dbsnapper_sanitize/#options", "title": "Options", "text": "<pre><code>  -e, --ephemeral   Create a snapshot using an ephemeral database via docker containers\n  -h, --help        help for sanitize\n  -n, --new         Create a new snapshot set for the target\n</code></pre>"}, {"location": "cmd/dbsnapper_sanitize/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> </ul>"}, {"location": "cmd/dbsnapper_subset/", "title": "dbsnapper subset", "text": ""}, {"location": "cmd/dbsnapper_subset/#dbsnapper-subset_1", "title": "dbsnapper subset", "text": "<p>Used to subset a target based on the configuration in targets..subset"}, {"location": "cmd/dbsnapper_subset/#synopsis", "title": "Synopsis", "text": "<p>The <code>dbsnapper subset</code> command subsets the specified target resulting in a relationally-complete subset of the source database.</p> <p>The <code>target_name</code> is the name of the target defined in the configuration file.</p> <pre><code>dbsnapper subset target_name [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_subset/#options", "title": "Options", "text": "<pre><code>  -h, --help   help for subset\n</code></pre>"}, {"location": "cmd/dbsnapper_subset/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> </ul>"}, {"location": "cmd/dbsnapper_target/", "title": "dbsnapper target", "text": ""}, {"location": "cmd/dbsnapper_target/#dbsnapper-target_1", "title": "dbsnapper target", "text": "<p>Used to list snapshots for a target</p>"}, {"location": "cmd/dbsnapper_target/#synopsis", "title": "Synopsis", "text": "<p>The <code>target</code> command lists all snapshots for a <code>target_name</code>.</p> <p>The <code>target_name</code> is the name of the target defined in the configuration file.</p> <p>Specify the <code>--cloud</code> flag to only list cloud snapshots.</p> <pre><code>dbsnapper target target_name [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_target/#options", "title": "Options", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will Overwrite any existing destination database!!!\n  -h, --help            help for target\n      --info            Output database and table stats using\n      --noui            Disable the Terminal UI\n  -o, --original        Use the original snapshot instead of the sanitized version.\n  -u, --update          Update the db info for all targets (default true)\n</code></pre>"}, {"location": "cmd/dbsnapper_target/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> </ul>"}, {"location": "cmd/dbsnapper_targets/", "title": "dbsnapper targets", "text": ""}, {"location": "cmd/dbsnapper_targets/#dbsnapper-targets_1", "title": "dbsnapper targets", "text": "<p>Use to list all targets</p> <pre><code>dbsnapper targets [flags]\n</code></pre>"}, {"location": "cmd/dbsnapper_targets/#options", "title": "Options", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will Overwrite any existing destination database!!!\n  -h, --help            help for targets\n      --noui            Disable the Terminal UI\n  -o, --original        Use the original snapshot instead of the sanitized version.\n  -u, --update          Update the db info for all targets (default true)\n</code></pre>"}, {"location": "cmd/dbsnapper_targets/#see-also", "title": "SEE ALSO", "text": "<ul> <li>dbsnapper  - Simplifies the process of creating de-identified database snapshots that can be used for real-world development, testing, and AI model training.</li> </ul>"}, {"location": "commands/build/", "title": "Build Command", "text": "<p>The <code>build</code> command creates database snapshots from your configured targets. This is the primary command for capturing database state for later restoration, sharing, or sanitization.</p>"}, {"location": "commands/build/#overview", "title": "Overview", "text": "<p>The build command connects to a source database, creates a comprehensive snapshot of its structure and data, and stores it locally. If cloud storage is configured, the snapshot is also uploaded to your cloud storage provider for sharing and backup.</p>"}, {"location": "commands/build/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper build &lt;target_name&gt; [flags]\n</code></pre>"}, {"location": "commands/build/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> </ul>"}, {"location": "commands/build/#options", "title": "Options", "text": "<pre><code>  -h, --help   Show help for the build command\n</code></pre>"}, {"location": "commands/build/#how-it-works", "title": "How It Works", "text": "<ol> <li>Connection: DBSnapper connects to the source database using the <code>src_url</code> from your target configuration</li> <li>Schema Analysis: For PostgreSQL databases, analyzes available schemas and applies filtering based on <code>schema_config</code></li> <li>Snapshot Creation: Creates a complete backup including schema, data, indexes, and constraints</li> <li>Local Storage: Saves the snapshot as a compressed ZIP file in your working directory</li> <li>Cloud Upload: If a storage profile is configured, uploads the snapshot to your cloud storage</li> <li>Metadata: Records snapshot information for future reference</li> </ol>"}, {"location": "commands/build/#output-files", "title": "Output Files", "text": ""}, {"location": "commands/build/#local-snapshots", "title": "Local Snapshots", "text": "<ul> <li>Format: <code>{timestamp}_{target_name}.zip</code></li> <li>Location: Current working directory</li> <li>Example: <code>1752444532_dvdrental.zip</code></li> </ul>"}, {"location": "commands/build/#cloud-snapshots", "title": "Cloud Snapshots", "text": "<ul> <li>Name: <code>{timestamp}_{target_name}</code> (user-visible identifier)</li> <li>Storage: Unique UUID filename in your cloud bucket</li> <li>Access: Available through DBSnapper Cloud interface</li> </ul>"}, {"location": "commands/build/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>The build command supports advanced schema filtering for PostgreSQL databases, allowing you to control exactly which schemas are included in your snapshots.</p>"}, {"location": "commands/build/#schema-configuration-options", "title": "Schema Configuration Options", "text": "Configuration Behavior Use Case No <code>schema_config</code> Builds <code>public</code> schema only Default behavior for simple applications <code>use_default_schema: true</code> Builds default schema (<code>public</code>) Explicit default schema only <code>use_default_schema: false</code> Builds all available schemas Multi-schema applications <code>include_schemas: [\"public\", \"app\"]</code> Builds only specified schemas Selective schema inclusion <code>exclude_schemas: [\"temp\", \"logs\"]</code> Builds all schemas except excluded Exclude temporary/log schemas"}, {"location": "commands/build/#configuration-examples", "title": "Configuration Examples", "text": "<p>Include Specific Schemas: <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        include_schemas: [\"public\", \"app_data\", \"reports\"]\n</code></pre></p> <p>Exclude Temporary Schemas: <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        exclude_schemas: [\"temp_data\", \"debug_logs\", \"analytics\"]\n</code></pre></p> <p>Build All Schemas: <pre><code>targets:\n  myapp-prod:\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_snap\"\n      schema_config:\n        use_default_schema: false  # Include all available schemas\n</code></pre></p>"}, {"location": "commands/build/#important-notes", "title": "Important Notes", "text": "<ul> <li>PostgreSQL Only: Schema filtering only works with PostgreSQL databases. MySQL targets ignore <code>schema_config</code></li> <li>Dynamic Analysis: The build process analyzes available schemas in the source database at runtime</li> <li>Existing Schemas Only: Only schemas that actually exist in the source database will be included</li> <li>Load Consistency: The load command will respect the schemas that were captured during build</li> <li>Validation: Schemas cannot appear in both <code>include_schemas</code> and <code>exclude_schemas</code> lists</li> </ul> <p>MySQL Limitation</p> <p>Schema filtering is not supported for MySQL databases. MySQL targets will use standard mysqldump without schema filtering, regardless of <code>schema_config</code> settings.</p>"}, {"location": "commands/build/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/build/#basic-build", "title": "Basic Build", "text": "<pre><code>dbsnapper build myapp-prod\n</code></pre>"}, {"location": "commands/build/#build-with-schema-filtering", "title": "Build with Schema Filtering", "text": "<pre><code># Builds only the schemas configured in schema_config\ndbsnapper build postgres-target\n</code></pre>"}, {"location": "commands/build/#configuration-requirements", "title": "Configuration Requirements", "text": "<p>Your target must be properly configured in your <code>dbsnapper.yml</code> file:</p> <pre><code>targets:\n  myapp-prod:\n    name: \"Production Database\"\n    src_url: \"postgresql://user:pass@host:5432/myapp\"\n    storage_profile: \"aws-s3\"  # Optional: for cloud storage\n</code></pre>"}, {"location": "commands/build/#storage-profiles", "title": "Storage Profiles", "text": "<p>For cloud storage, configure a storage profile:</p> <pre><code>storage_profiles:\n  aws-s3:\n    type: \"s3\"\n    region: \"us-west-2\"\n    bucket: \"my-snapshots\"\n    # Additional S3 configuration...\n</code></pre>"}, {"location": "commands/build/#best-practices", "title": "Best Practices", "text": ""}, {"location": "commands/build/#timing", "title": "Timing", "text": "<ul> <li>Production Databases: Schedule builds during low-traffic periods</li> <li>Development: Build regularly to maintain fresh snapshots</li> <li>Testing: Build before major changes for quick rollback</li> </ul>"}, {"location": "commands/build/#security", "title": "Security", "text": "<ul> <li>Credentials: Use environment variables or secure credential management</li> <li>Network: Ensure secure network connections to production databases</li> <li>Access: Limit build permissions to authorized personnel</li> </ul>"}, {"location": "commands/build/#performance", "title": "Performance", "text": "<ul> <li>Large Databases: Monitor build times and storage requirements</li> <li>Network: Consider bandwidth impact for cloud uploads</li> <li>Resources: Ensure adequate disk space for local snapshots</li> </ul>"}, {"location": "commands/build/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/build/#common-issues", "title": "Common Issues", "text": "<p>Connection Failed <pre><code>Error: failed to connect to database\n</code></pre> - Verify <code>src_url</code> connection string - Check network connectivity - Confirm database credentials</p> <p>Schema Configuration Errors <pre><code>Error: schema 'analytics' cannot be in both include and exclude lists\n</code></pre> - Remove the schema from one of the lists - Use either <code>include_schemas</code> OR <code>exclude_schemas</code> for any given schema</p> <p>Schema Not Found Warnings <pre><code>Warning: schema 'non_existent' specified in include_schemas but not found in database\n</code></pre> - Verify schema names are correct - Check that schemas exist in the source database - Review available schemas in your database</p> <p>Insufficient Storage <pre><code>Error: no space left on device\n</code></pre> - Free up disk space in working directory - Consider cleanup of old snapshots</p> <p>Cloud Upload Failed <pre><code>Error: failed to upload to cloud storage\n</code></pre> - Verify storage profile configuration - Check cloud credentials and permissions - Confirm bucket exists and is accessible</p>"}, {"location": "commands/build/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>load</code> - Load snapshots into databases</li> <li><code>sanitize</code> - Create sanitized versions of snapshots</li> <li><code>targets</code> - List available targets</li> <li><code>pull</code> - Download cloud snapshots</li> </ul>"}, {"location": "commands/build/#see-also", "title": "See Also", "text": "<ul> <li>Snapshot Configuration</li> <li>Cloud Storage Setup</li> <li>Target Configuration</li> </ul>"}, {"location": "commands/load/", "title": "Load Command", "text": "<p>The <code>load</code> command restores database snapshots and executes SQL files in target databases. This versatile command supports both snapshot restoration and direct SQL file execution with comprehensive safety features and execution modes.</p>"}, {"location": "commands/load/#overview", "title": "Overview", "text": "<p>The load command handles two primary operations: 1. Snapshot Loading: Restores database snapshots created with the <code>build</code> command 2. SQL File Loading: Executes SQL files directly against target databases</p> <p>Both operations support advanced configuration options, safety features, and schema filtering for PostgreSQL databases.</p>"}, {"location": "commands/load/#syntax", "title": "Syntax", "text": "<pre><code>dbsnapper load &lt;target_name&gt; [snapshot_index|file.sql] [flags]\n</code></pre>"}, {"location": "commands/load/#arguments", "title": "Arguments", "text": "<ul> <li><code>&lt;target_name&gt;</code>: The name of the target defined in your configuration file</li> <li><code>[snapshot_index|file.sql]</code>: Optional snapshot index (default: 0 for most recent) or SQL file path</li> </ul>"}, {"location": "commands/load/#options", "title": "Options", "text": "<pre><code>      --destdb string   Destination Database URL Override - Will overwrite any existing destination database\n      --dry-run         Show execution plan without running (SQL files only)\n      --no-drop         Skip dropping/recreating database (SQL files only)\n      --original        Use the original snapshot instead of the sanitized version (snapshots only)\n      --source          Load SQL file to source database instead of destination\n  -h, --help           Show help for the load command\n</code></pre>"}, {"location": "commands/load/#snapshot-loading", "title": "Snapshot Loading", "text": "<p>Load database snapshots created with the <code>build</code> command:</p>"}, {"location": "commands/load/#basic-snapshot-operations", "title": "Basic Snapshot Operations", "text": "<pre><code>dbsnapper load target_name              # Load most recent snapshot to dst_url\ndbsnapper load target_name 2            # Load snapshot by index to dst_url\ndbsnapper load target_name --original   # Load original (non-sanitized) snapshot\ndbsnapper load target_name --destdb &lt;url&gt;  # Load to custom destination database\n</code></pre>"}, {"location": "commands/load/#how-snapshot-loading-works", "title": "How Snapshot Loading Works", "text": "<ol> <li>Snapshot Selection: Identifies the snapshot to load based on index (0 = most recent)</li> <li>Database Preparation: Drops and recreates the destination database</li> <li>Schema Analysis: For PostgreSQL, analyzes snapshot contents and applies schema filtering</li> <li>Data Restoration: Restores schema, data, indexes, and constraints from the snapshot</li> <li>Verification: Confirms successful restoration</li> </ol>"}, {"location": "commands/load/#snapshot-priority-system", "title": "Snapshot Priority System", "text": "<p>DBSnapper automatically prioritizes sanitized snapshots for safety:</p> <ol> <li>Sanitized Snapshot: Loads <code>{timestamp}_{target_name}.san.zip</code> if available</li> <li>Original Snapshot: Loads <code>{timestamp}_{target_name}.zip</code> if no sanitized version exists  </li> <li>Override: Use <code>--original</code> flag to force loading the original snapshot</li> </ol>"}, {"location": "commands/load/#sql-file-loading", "title": "SQL File Loading", "text": "<p>Load SQL files directly to target databases with advanced execution modes and safety features:</p>"}, {"location": "commands/load/#basic-sql-file-operations", "title": "Basic SQL File Operations", "text": "<pre><code>dbsnapper load target_name file.sql                    # Load SQL file to dst_url (default)\ndbsnapper load target_name file.sql --source           # Load SQL file to src_url\ndbsnapper load target_name file.sql --destdb &lt;url&gt;     # Load to custom destination\ndbsnapper load target_name file.sql --no-drop         # Execute against existing database\ndbsnapper load target_name file.sql --dry-run         # Preview execution without running\n</code></pre>"}, {"location": "commands/load/#sql-file-detection", "title": "SQL File Detection", "text": "<p>SQL files are automatically detected by the <code>.sql</code> extension (case-insensitive): - <code>schema.sql</code>, <code>data.SQL</code>, <code>migration.Sql</code> are all valid - Non-SQL files will be treated as snapshot indices</p>"}, {"location": "commands/load/#execution-modes", "title": "Execution Modes", "text": "<p>Replace Mode (default) - Drops and recreates the target database before loading - Ensures clean database state - Use for complete database refreshes</p> <p>Append Mode (<code>--no-drop</code>) - Executes SQL against the existing database without dropping it - Preserves existing data and structure - Use for incremental updates, migrations, or data additions</p>"}, {"location": "commands/load/#database-url-resolution-priority", "title": "Database URL Resolution Priority", "text": "<p>Database URL resolution follows this priority order:</p> <ol> <li><code>--destdb</code> flag (highest priority)</li> <li><code>--source</code> flag (determines src_url vs dst_url)</li> <li>Configuration overrides</li> <li>Target configuration defaults</li> </ol>"}, {"location": "commands/load/#schema-filtering-postgresql-only", "title": "Schema Filtering (PostgreSQL Only)", "text": "<p>When loading snapshots, the load command respects schema configuration for PostgreSQL databases:</p>"}, {"location": "commands/load/#schema-configuration-behavior", "title": "Schema Configuration Behavior", "text": "Configuration Load Behavior Notes No <code>schema_config</code> Loads <code>public</code> schema only Default behavior <code>use_default_schema: true</code> Loads default schema from snapshot Explicit default <code>use_default_schema: false</code> Loads all schemas present in snapshot Multi-schema applications <code>include_schemas: [\"public\", \"app\"]</code> Loads only listed schemas from snapshot Selective inclusion <code>exclude_schemas: [\"temp\", \"logs\"]</code> Loads all schemas except excluded ones Exclude temporary data"}, {"location": "commands/load/#schema-configuration-example", "title": "Schema Configuration Example", "text": "<pre><code>targets:\n  postgres-target:\n    snapshot:\n      schema_config:\n        # Option 1: Load all schemas from snapshot\n        use_default_schema: false\n\n        # Option 2: Load only specific schemas\n        # include_schemas: [\"public\", \"app_data\"]\n\n        # Option 3: Load all except excluded schemas  \n        # exclude_schemas: [\"temp_schema\", \"debug_logs\"]\n</code></pre>"}, {"location": "commands/load/#important-schema-notes", "title": "Important Schema Notes", "text": "<ul> <li>PostgreSQL Only: Schema filtering only applies to PostgreSQL snapshot loading</li> <li>Snapshot Contents: Only schemas that exist in the snapshot will be loaded</li> <li>No Empty Schemas: No empty schemas are created if they don't exist in the snapshot</li> <li>Warnings Displayed: If requested schemas don't exist in the snapshot, warnings are shown</li> <li>MySQL Limitation: MySQL databases use standard restore without schema filtering</li> <li>SQL Files Excluded: Schema filtering does not apply to SQL file loading</li> </ul>"}, {"location": "commands/load/#safety-features", "title": "Safety Features", "text": "<p>The load command includes comprehensive safety features:</p>"}, {"location": "commands/load/#interactive-confirmation", "title": "Interactive Confirmation", "text": "<ul> <li>Prompts before destructive operations</li> <li>Shows operation details and target information</li> <li>Can be disabled for automation</li> </ul>"}, {"location": "commands/load/#dry-run-mode", "title": "Dry Run Mode", "text": "<ul> <li>Preview SQL execution without making changes (<code>--dry-run</code>)</li> <li>Shows execution plan and affected objects</li> <li>Available for SQL file loading only</li> </ul>"}, {"location": "commands/load/#file-validation", "title": "File Validation", "text": "<ul> <li>Validates SQL files before execution</li> <li>Checks file existence and readability</li> <li>Prevents execution of invalid files</li> </ul>"}, {"location": "commands/load/#transaction-support", "title": "Transaction Support", "text": "<ul> <li>Wraps SQL execution in transactions for rollback capability</li> <li>Ensures atomicity of operations</li> <li>Provides rollback on errors</li> </ul>"}, {"location": "commands/load/#confirmation-control", "title": "Confirmation Control", "text": "<p>Skip confirmations in automated environments:</p> <pre><code># Environment variable (recommended for CI/CD)\nexport CI=true\ndbsnapper load target_name file.sql\n\n# Alternative environment variable\nexport DBSNAPPER_NO_CONFIRM=true\ndbsnapper load target_name file.sql\n</code></pre>"}, {"location": "commands/load/#example-usage", "title": "Example Usage", "text": ""}, {"location": "commands/load/#snapshot-loading-examples", "title": "Snapshot Loading Examples", "text": "<pre><code># Basic snapshot operations\ndbsnapper load production-db              # Load latest snapshot\ndbsnapper load production-db 5            # Load snapshot at index 5\ndbsnapper load production-db --original   # Load original (non-sanitized) snapshot\n\n# Custom destination\ndbsnapper load prod-db --destdb \"postgresql://user:pass@test-host:5432/test_db\"\n</code></pre>"}, {"location": "commands/load/#sql-file-loading-examples", "title": "SQL File Loading Examples", "text": "<pre><code># Basic SQL file operations\ndbsnapper load dev-db schema.sql          # Load schema to dst_url\ndbsnapper load dev-db data.sql --source   # Load data to src_url\ndbsnapper load dev-db migration.sql --dry-run  # Preview migration\n\n# Advanced SQL file operations\ndbsnapper load prod-db update.sql --no-drop    # Execute without dropping database\ndbsnapper load staging-db backup.sql --destdb \"postgres://user:pass@host/custom_db\"\n</code></pre>"}, {"location": "commands/load/#automation-examples", "title": "Automation Examples", "text": "<pre><code># CI/CD pipeline usage\nCI=true dbsnapper load production-db\nDBSNAPPER_NO_CONFIRM=true dbsnapper load staging-db migration.sql --no-drop\n\n# Batch processing\nfor file in migrations/*.sql; do\n  dbsnapper load dev-db \"$file\" --no-drop\ndone\n</code></pre>"}, {"location": "commands/load/#configuration-requirements", "title": "Configuration Requirements", "text": ""}, {"location": "commands/load/#basic-target-configuration", "title": "Basic Target Configuration", "text": "<pre><code>targets:\n  myapp-dev:\n    name: \"Development Database\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_dev\"\n</code></pre>"}, {"location": "commands/load/#advanced-target-configuration-with-schema-filtering", "title": "Advanced Target Configuration with Schema Filtering", "text": "<pre><code>targets:\n  postgres-target:\n    name: \"PostgreSQL with Schema Control\"\n    snapshot:\n      src_url: \"postgresql://user:pass@prod:5432/myapp\"\n      dst_url: \"postgresql://user:pass@dev:5432/myapp_dev\"\n      schema_config:\n        include_schemas: [\"public\", \"app_data\", \"reports\"]\n        # Exclude temporary and debug schemas from loading\n</code></pre>"}, {"location": "commands/load/#use-cases", "title": "Use Cases", "text": ""}, {"location": "commands/load/#development-environment-setup", "title": "Development Environment Setup", "text": "<pre><code># Refresh development database with latest production snapshot\ndbsnapper build myapp-prod\ndbsnapper load myapp-dev\n</code></pre>"}, {"location": "commands/load/#testing-with-clean-data", "title": "Testing with Clean Data", "text": "<pre><code># Load sanitized snapshot for testing\ndbsnapper load test-env 0\n</code></pre>"}, {"location": "commands/load/#database-migrations", "title": "Database Migrations", "text": "<pre><code># Preview migration before applying\ndbsnapper load staging-db migration_v2.sql --dry-run\n\n# Apply migration without dropping database\ndbsnapper load staging-db migration_v2.sql --no-drop\n</code></pre>"}, {"location": "commands/load/#feature-branch-testing", "title": "Feature Branch Testing", "text": "<pre><code># Use specific snapshot state for consistent testing\ndbsnapper load feature-branch 3\n</code></pre>"}, {"location": "commands/load/#schema-management", "title": "Schema Management", "text": "<pre><code># Load only application schemas, excluding logs and temp data\n# (configured via schema_config in target definition)\ndbsnapper load filtered-target\n</code></pre>"}, {"location": "commands/load/#troubleshooting", "title": "Troubleshooting", "text": ""}, {"location": "commands/load/#common-issues", "title": "Common Issues", "text": "<p>Snapshot Not Found <pre><code>Error: snapshot index 5 not found for target myapp-dev\n</code></pre> - List available snapshots with <code>dbsnapper target myapp-dev</code> - Use valid snapshot index (0 = most recent)</p> <p>Database Connection Failed <pre><code>Error: failed to connect to destination database\n</code></pre> - Verify <code>dst_url</code> or <code>--destdb</code> connection string - Check database server availability - Confirm credentials and permissions</p> <p>Permission Denied <pre><code>Error: insufficient privileges to drop/create database\n</code></pre> - Ensure database user has CREATE/DROP privileges - Check database server permissions - Verify connection string includes correct credentials</p> <p>SQL File Not Found <pre><code>Error: SQL file not found: missing.sql\n</code></pre> - Verify file path is correct - Check file exists and is readable - Use absolute or relative paths as appropriate</p> <p>Schema Configuration Errors <pre><code>Warning: schema 'non_existent' specified but not found in snapshot\n</code></pre> - Verify schema names in configuration - Check that schemas exist in the original snapshot - Review schema configuration syntax</p>"}, {"location": "commands/load/#recovery-steps", "title": "Recovery Steps", "text": "<ol> <li>Verify Configuration: Check target configuration and connection strings</li> <li>Test Connectivity: Ensure database servers are accessible</li> <li>Check Permissions: Verify database user privileges</li> <li>Validate Files: Confirm SQL files exist and are readable</li> <li>Use Dry Run: Preview operations with <code>--dry-run</code> flag</li> </ol> <p>Destructive Operation</p> <p>The load command will drop and recreate the destination database by default. All existing data in the destination database will be permanently lost unless using <code>--no-drop</code> flag with SQL files.</p> <p>Sanitized Snapshots Priority</p> <p>If a sanitized snapshot exists at the specified index, it will be loaded unless the <code>--original</code> flag is set. This ensures sensitive data protection by default.</p> <p>Database URL Priority</p> <p>Database URL resolution follows this priority: 1. <code>--destdb</code> flag (highest priority) 2. <code>--source</code> flag (determines src_url vs dst_url) 3. Configuration overrides 4. Target configuration defaults</p>"}, {"location": "commands/load/#related-commands", "title": "Related Commands", "text": "<ul> <li><code>build</code> - Create new database snapshots</li> <li><code>sanitize</code> - Create sanitized versions of snapshots</li> <li><code>target</code> - View target details and available snapshots</li> <li><code>targets</code> - List all available targets</li> </ul>"}, {"location": "commands/load/#see-also", "title": "See Also", "text": "<ul> <li>Configuration - Target and schema configuration</li> <li>Database Engines - Supported databases</li> <li>Snapshot Management - Snapshot concepts and workflows</li> <li>Sanitization - Data sanitization features</li> </ul>"}, {"location": "database-engines/introduction/", "title": "Introduction", "text": "<p>DBSnapper provides a library of Database Engines that ensure compatibility between different database vendors.</p> <p>Currently supported engines include:</p> <ul> <li>PostgreSQL Local installation</li> <li>MySQL Local installation</li> <li>PostgreSQL Docker Image</li> <li>Mysql Docker Image</li> </ul> <p>More Database Engines are being created and will be added to the application in the future.</p>"}, {"location": "database-engines/mysql-docker/", "title": "MySQL Docker (<code>mydocker://</code>)", "text": ""}, {"location": "database-engines/mysql-docker/#overview", "title": "Overview", "text": "<p>The MySQL Docker engine allows you to create snapshots of your MySQL databases running in Docker containers.</p>"}, {"location": "database-engines/mysql-docker/#requirements", "title": "Requirements", "text": "<p>The MySQL Docker engine has been tested with <code>mysql:8-oracle</code> Docker image. But should be compatible with any recent version of the MySQL Docker image that includes the <code>mysqlsh</code> MySQL Shell client.</p>"}, {"location": "database-engines/mysql-docker/#usage-notes", "title": "Usage Notes", "text": ""}, {"location": "database-engines/mysql-docker/#specifying-the-mysql-docker-engine", "title": "Specifying the MySQL Docker Engine", "text": "<p>To use the MySQL Docker engine to create or load a database snapshot, use the <code>mydocker://</code> scheme for the source or destination target url as needed. For example:</p> <p><code>mydocker://username:password@host/database</code></p> <p>The <code>mydocker</code> scheme is abbreviated as <code>myd</code> in the CLI and DBSnapper cloud</p>"}, {"location": "database-engines/mysql-local/", "title": "MySQL Local (<code>mysql://</code>)", "text": ""}, {"location": "database-engines/mysql-local/#overview", "title": "Overview", "text": "<p>The MySQL Local engine allows you to create snapshots of your MySQL databases running in Docker containers.</p>"}, {"location": "database-engines/mysql-local/#requirements", "title": "Requirements", "text": "<p>The MySQL Local engine has been tested with Mysql 8.3 and uses the <code>mysqldump</code> and <code>mysql</code> command line tools to create, sanitize, and load database snapshots.</p>"}, {"location": "database-engines/mysql-local/#usage-notes", "title": "Usage Notes", "text": ""}, {"location": "database-engines/mysql-local/#specifying-the-mysql-local-engine", "title": "Specifying the MySQL Local Engine", "text": "<p>To use the MySQL Local engine to create or load a database snapshot, use the default <code>mysql://</code> scheme for the source or destination target url as needed. For example:</p> <p><code>mysql://username:password@host/database</code></p> <p>The <code>mysql</code> scheme is abbreviated as <code>myl</code> in the CLI and DBSnapper cloud</p>"}, {"location": "database-engines/postgresql-docker/", "title": "PostgreSQL Docker (<code>pgdocker://</code>)", "text": ""}, {"location": "database-engines/postgresql-docker/#overview", "title": "Overview", "text": "<p>The PostgreSQL Docker engine allows you to create snapshots of your PostgreSQL databases running in Docker containers. Using the PostgreSQL Docker engine is an easy way to get access to the <code>pg_dump</code>, <code>pg_restore</code>, and <code>psql</code> tools that are required to create, sanitize, and load database snapshots.</p>"}, {"location": "database-engines/postgresql-docker/#requirements", "title": "Requirements", "text": "<p>The PostgreSQL Docker engine has been tested with <code>postgres:16-alpine</code> Docker image. But should be compatible with any recent version of PostgreSQL Docker image &gt;= PostgreSQL 9.2.</p>"}, {"location": "database-engines/postgresql-docker/#usage-notes", "title": "Usage Notes", "text": ""}, {"location": "database-engines/postgresql-docker/#specifying-the-postgresql-docker-engine", "title": "Specifying the PostgreSQL Docker Engine", "text": "<p>To use the PostgreSQL Docker engine to create or load a database snapshot, use the <code>pgdocker://</code> scheme for the source or destination target url as needed. For example:</p> <p><code>pgdocker://username:password@host/database?sslmode=[require | disable]</code></p> <p>The <code>pgdocker</code> scheme is abbreviated as <code>pgd</code> in the CLI and DBSnapper cloud</p>"}, {"location": "database-engines/postgresql-local/", "title": "PostgreSQL Local (<code>postgres://</code>)", "text": ""}, {"location": "database-engines/postgresql-local/#overview", "title": "Overview", "text": "<p>The PostgreSQL Local engine allows you to create snapshots of your PostgreSQL databases running in Docker containers.</p>"}, {"location": "database-engines/postgresql-local/#requirements", "title": "Requirements", "text": "<p>The PostgreSQL Local engine has been tested with Postgres.app and should be compatible with any recent version of PostgreSQL installed on a host machine ( &gt;= PostgreSQL 9.2 ) that includes <code>pg_dump</code>, <code>pg_restore</code>, and <code>psql</code> in the path.</p>"}, {"location": "database-engines/postgresql-local/#usage-notes", "title": "Usage Notes", "text": ""}, {"location": "database-engines/postgresql-local/#specifying-the-postgresql-local-engine", "title": "Specifying the PostgreSQL Local Engine", "text": "<p>To use the PostgreSQL Local engine to create or load a database snapshot, use the default <code>postgres://</code> scheme for the source or destination target url as needed. For example:</p> <p><code>postgres://username:password@host/database?sslmode=[require | disable]</code></p> <p>The <code>postgres</code> scheme is abbreviated as <code>pgl</code> in the CLI and DBSnapper cloud</p>"}, {"location": "database-engines/postgresql-local/#permissions", "title": "Permissions", "text": "<p>Permissions for various operations:</p> <ul> <li><code>build</code></li> <li><code>sanitize</code></li> <li><code>load</code></li> </ul>"}, {"location": "dbsnapper-cloud/introduction/", "title": "Introduction to DBSnapper Cloud", "text": "<p>DBSnapper Cloud extends your database management capabilities beyond the local CLI, providing a secure, collaborative platform for managing and sharing database snapshots across your organization.</p>"}, {"location": "dbsnapper-cloud/introduction/#why-dbsnapper-cloud", "title": "Why DBSnapper Cloud?", "text": ""}, {"location": "dbsnapper-cloud/introduction/#key-benefits", "title": "Key Benefits", "text": "<ul> <li>Centralized Management: Streamline database snapshot management across teams and environments</li> <li>Enhanced Collaboration: Share sanitized database snapshots securely with team members</li> <li>Automated Workflows: Seamlessly integrate with CI/CD pipelines and development processes</li> <li>Enterprise-Grade Security: Built-in SSO integration and granular access controls</li> </ul>"}, {"location": "dbsnapper-cloud/introduction/#feature-comparison", "title": "Feature Comparison", "text": "Feature CLI Only DBSnapper Cloud Local Snapshots \u2705 \u2705 Database Subsetting \u2705 \u2705 Data Sanitization \u2705 \u2705 Team Sharing \u274c \u2705 SSO Integration \u274c \u2705 Cloud Storage Integration Limited Full Access Control Basic Advanced Audit Logging \u274c \u2705"}, {"location": "dbsnapper-cloud/introduction/#common-use-cases", "title": "Common Use Cases", "text": "<ol> <li> <p>Development Environments</p> <ul> <li>Quickly provision development databases with sanitized production data</li> <li>Maintain consistency across team environments</li> </ul> </li> <li> <p>Testing and QA</p> <ul> <li>Create reproducible test environments</li> <li>Share consistent datasets across QA teams</li> </ul> </li> <li> <p>CI/CD Integration</p> <ul> <li>Automate database provisioning in automated CI/CD pipelines</li> <li>Ensure consistent and up-to-date test data across builds</li> </ul> </li> <li> <p>Compliance and Security</p> <ul> <li>Maintain GDPR and CCPA compliance with sanitized data</li> <li>Control access through role-based SSO permissions</li> </ul> </li> </ol>"}, {"location": "dbsnapper-cloud/introduction/#getting-started", "title": "Getting Started", "text": "<ol> <li>Sign up for DBSnapper Cloud for DBSnapper Cloud</li> <li>Install the DBSnapper Agent on your local system</li> <li>Configure Storage Profiles and Targets</li> <li>Set up your SSO provider for team access (optional)</li> <li>Create your first cloud-managed snapshot</li> </ol>"}, {"location": "dbsnapper-cloud/introduction/#dbsnapper-cloud-processing-pipeline", "title": "DBSnapper Cloud Processing Pipeline", "text": "<p>The DBSnapper Cloud brings everything together and provides a processing pipeline to create, store, and share sanitized database snapshots.</p>"}, {"location": "dbsnapper-cloud/storage_profiles/", "title": "DBSnapper Cloud Platform - Storage Profiles Page", "text": ""}, {"location": "dbsnapper-cloud/storage_profiles/#all-storage-profiles", "title": "All Storage Profiles", "text": "<p>A Storage Profile specifies the access credentials and storage bucket / prefix for storing a snapshot in a cloud storage provider.</p> <p> DBSnapper Cloud: Storage Profiles page. </p> <p>The Storage Profiles page displays all the storage profiles you've defined. These storage profiles are used in the create/edit Targets page, where you can specify different storage profiles for the unsanitized and sanitized snapshots.</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#create-edit-storage-profile", "title": "Create / Edit Storage Profile", "text": "<p>To create a new Storage Profile, click the <code>New Storage Profile</code> button in the top right of the page. That will display a form similar to the Edit Storage Profile page below where you can define the settings for the Storage Profile.</p> <p> DBSnapper Cloud: Edit Storage Profile page. </p> <p>On this page, we are editing the storage profile settings for an Amazon S3 bucket. The following settings are available:</p>"}, {"location": "dbsnapper-cloud/storage_profiles/#profile-name", "title": "Profile Name", "text": "<ul> <li>Name: A unique name for the storage profile.</li> </ul>"}, {"location": "dbsnapper-cloud/storage_profiles/#storage-provider-info", "title": "Storage Provider Info", "text": "<ul> <li>Provider: This is a dropdown that provides a list of supported cloud storage providers. Currently, the supported providers are Amazon S3 and Cloudflare R2.</li> <li>Region / Account ID: Depending on the provider you select, you will need to provide the region (AWS S3) or your account ID (Cloudflare R2) for the storage provider.</li> <li>Access Key ID: The access key ID for the storage provider IAM account.</li> <li>Secret Access Key: The secret access key for the storage provider IAM account.</li> </ul>"}, {"location": "dbsnapper-cloud/storage_profiles/#storage-location", "title": "Storage Location", "text": "<ul> <li>Bucket the name of the bucket where the snapshot will be stored.</li> <li>Prefix (Optional) - A prefix that will be added to the snapshot name when it is stored in the bucket. This can be useful for organizing snapshots in the bucket.</li> </ul>"}, {"location": "dbsnapper-cloud/targets/", "title": "Targets", "text": "<p>Targets are the foundation of DBSnapper Cloud's database snapshot management. A target defines the complete configuration for creating, sanitizing, storing, and sharing snapshots for a specific database workload.</p>"}, {"location": "dbsnapper-cloud/targets/#overview", "title": "Overview", "text": "<p>Each target encapsulates:</p> <ul> <li>Source database connection - Where to snapshot data from</li> <li>Destination database connection - Where to load snapshots (optional)</li> <li>Storage configurations - Cloud storage for both raw and sanitized snapshots</li> <li>Sanitization rules - Data privacy and security transformations</li> <li>Team sharing settings - Access control for collaborative workflows</li> </ul> <p>Targets enable teams to standardize database snapshot processes while maintaining security and compliance requirements.</p>"}, {"location": "dbsnapper-cloud/targets/#managing-targets", "title": "Managing Targets", "text": ""}, {"location": "dbsnapper-cloud/targets/#viewing-all-targets", "title": "Viewing All Targets", "text": "<p>The Targets page displays all your configured targets along with any shared team targets you have access to. Shared team targets are available when your organization uses SSO integration, leveraging SSO groups to manage access permissions.</p> <p></p> <p>Each target shows:</p> <ul> <li>Target name and database type</li> <li>Connection status and last snapshot date</li> <li>Storage configuration and sharing permissions</li> <li>Quick actions for building snapshots or viewing details</li> </ul>"}, {"location": "dbsnapper-cloud/targets/#creating-and-editing-targets", "title": "Creating and Editing Targets", "text": "<p>Click the Add Target button to create a new target or select an existing target to modify its configuration.</p> <p></p>"}, {"location": "dbsnapper-cloud/targets/#target-configuration", "title": "Target Configuration", "text": "<p>When creating or editing a target, you'll configure several key components that define how snapshots are created, processed, and shared.</p>"}, {"location": "dbsnapper-cloud/targets/#basic-information", "title": "Basic Information", "text": "Target Name A unique, descriptive name for this target (e.g., \"production-users-db\", \"staging-analytics\")"}, {"location": "dbsnapper-cloud/targets/#database-connections", "title": "Database Connections", "text": "Source Database URL (Required) The connection string for the database you want to snapshot <pre><code>postgresql://username:password@host:port/database?sslmode=require\nmysql://username:password@host:port/database\n</code></pre> Destination Database URL (Optional) Where snapshots will be loaded for development/testing <p>Destructive Operation</p> <p>The destination database will be completely dropped and recreated when loading snapshots. Never use production databases as destinations.</p>"}, {"location": "dbsnapper-cloud/targets/#storage-configuration", "title": "Storage Configuration", "text": "Unsanitized Storage Profile (Required) Storage profile where raw database snapshots are stored. These contain original production data and should have restricted access. Sanitized Storage Profile (Optional) Storage profile where sanitized snapshots are stored. These can be shared more broadly since sensitive data has been removed. <p>Security Best Practice</p> <p>Use separate storage profiles for unsanitized and sanitized snapshots to implement proper access controls.</p>"}, {"location": "dbsnapper-cloud/targets/#data-sanitization", "title": "Data Sanitization", "text": "Sanitization Database URL (Optional) Temporary database used for sanitization processing. If not specified, DBSnapper uses ephemeral Docker containers. Sanitization Query (Optional) SQL commands to remove or mask sensitive data during the sanitization process <pre><code>-- Example sanitization query\nUPDATE users SET\n  email = CONCAT('user', id, '@example.com'),\n  phone = '555-0000',\n  ssn = NULL;\n\nDELETE FROM audit_logs WHERE created_at &lt; NOW() - INTERVAL 30 DAY;\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#team-collaboration", "title": "Team Collaboration", "text": "SSO Group Sharing (Optional) Comma-separated list of SSO groups that can access sanitized snapshots from this target <pre><code>developers,qa-team,data-analysts\n</code></pre> <p>SSO Required</p> <p>Team sharing requires your organization to have SSO integration configured.</p>"}, {"location": "dbsnapper-cloud/targets/#working-with-targets", "title": "Working with Targets", "text": ""}, {"location": "dbsnapper-cloud/targets/#using-the-dbsnapper-agent", "title": "Using the DBSnapper Agent", "text": "<p>Once targets are configured in DBSnapper Cloud, you can use them with the DBSnapper Agent CLI:</p> <pre><code># Build a snapshot using a cloud-configured target\ndbsnapper build my-production-target\n\n# Load a sanitized snapshot\ndbsnapper load my-production-target latest\n\n# Pull snapshots from cloud storage\ndbsnapper pull my-production-target\n</code></pre>"}, {"location": "dbsnapper-cloud/targets/#team-workflow-example", "title": "Team Workflow Example", "text": "<ol> <li>DevOps Engineer creates target with production database connection and sanitization rules</li> <li>Development Team uses the target to create sanitized snapshots for local development</li> <li>QA Team accesses shared sanitized snapshots for consistent testing environments</li> <li>Data Team uses sanitized snapshots for analytics without exposing sensitive data</li> </ol>"}, {"location": "dbsnapper-cloud/targets/#security-considerations", "title": "Security Considerations", "text": "<ul> <li>Connection Strings: Use environment variable templating to avoid storing credentials in target configurations</li> <li>Access Control: Leverage SSO groups to manage team access to sensitive targets</li> <li>Storage Separation: Use different storage profiles for unsanitized vs sanitized snapshots</li> <li>Audit Trail: Monitor target usage through DBSnapper Cloud's audit logging</li> </ul>"}, {"location": "dbsnapper-cloud/targets/#best-practices", "title": "Best Practices", "text": ""}, {"location": "dbsnapper-cloud/targets/#target-naming", "title": "Target Naming", "text": "<p>Use descriptive, consistent naming conventions:</p> <ul> <li>Include environment: <code>prod-users-db</code>, <code>staging-analytics</code></li> <li>Include purpose: <code>users-db-for-testing</code>, <code>analytics-weekly-snapshot</code></li> <li>Avoid spaces and special characters</li> </ul>"}, {"location": "dbsnapper-cloud/targets/#connection-security", "title": "Connection Security", "text": "<ul> <li>Use read-only users for source database connections when possible</li> <li>Enable SSL/TLS for all database connections</li> <li>Use connection pooling for high-frequency snapshot operations</li> <li>Test connectivity before saving target configurations</li> </ul>"}, {"location": "dbsnapper-cloud/targets/#sanitization-strategy", "title": "Sanitization Strategy", "text": "<ul> <li>Start simple with basic field masking, then iterate</li> <li>Preserve referential integrity when sanitizing related data</li> <li>Test sanitization queries thoroughly before production use</li> <li>Document sanitization rules for compliance audits</li> </ul>"}, {"location": "dbsnapper-cloud/targets/#next-steps", "title": "Next Steps", "text": "<ul> <li>Configure Storage Profiles - Set up cloud storage for your snapshots</li> <li>Set up SSO Integration - Enable team collaboration features</li> <li>Sanitization Guide - Learn advanced data sanitization techniques</li> <li>Agent CLI Reference - Complete command-line interface documentation</li> </ul>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/", "title": "DBSnapper Cloud - Single Sign-On (SSO) with Okta OIDC", "text": "<p>DBSnapper supports Single Sign-On (SSO) with Okta - this guide will walk you through the steps to configure Okta OIDC for your DBSnapper organization using the Okta Integration Network (OIN) catalog.</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#supported-features", "title": "Supported Features", "text": "<ul> <li>SP initiated Auth Flow: SSO Using OIDC initiated via DBSnapper Cloud</li> <li>Open ID Connect (OIDC): Yes</li> <li>Proof Key for Code Exchange (PKCE): Yes</li> <li>Just-in-Time Provisioning: Yes</li> </ul>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-1-add-the-dbsnapper-application-from-the-oin-catalog", "title": "Step 1: Add the DBSnapper Application from the OIN Catalog", "text": "<p>Sign into your Okta account and navigate to Admin/Applications. Click on the <code>Browse App Catalog</code> button.</p> <p> Step 1.1 - Okta SSO - Applications </p> <p>Search for and add the DBSnapper application from the catalog.</p> <p> Step 1.2 - Okta SSO - Add DBSnapper from Okta Integration Network (OIN) </p> <p>Take note of your application credentials which will be used in the DBSnapper Cloud SSO settings:</p> <ul> <li>Client ID - a public identifier for the client application</li> <li>Client Secret - Click to reveal the client secret and make note of it.</li> </ul>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#step-2-configure-dbsnapper-for-okta-oidc", "title": "Step 2: Configure DBSnapper for Okta OIDC", "text": "<p>Sign into your DBSnapper Cloud account and navigate to the <code>Settings -&gt; SSO</code> page. Click on the <code>No SSO Tenant Found - Add a Tenant</code> button.</p> <p>Add the settings from the Okta application you created in Step 1:</p> <ul> <li>Provider - Okta - OIDC (the default)</li> <li>Client ID - From the DBSnapper application settings in Okta</li> <li>Client Secret - From the DBSnapper application settings in Okta</li> <li>Issuer - Your Okta domain URL (e.g. <code>https://example.okta.com</code>)</li> </ul> <p>Note that the Organization Domain and Redirect URL are pre-filled for your organization. Make sure the Organization Domain matches the email domain of your users' email addresses.</p> <p> Step 2.1 - Okta SSO - Create new SSO Tenant in DBSnapper </p> <p>Click <code>Save</code> to save the new SSO Tenant for your organization. You should see your new SSO Tenant settings shown on the SSO page.</p> <p> Step 2.2 - Okta SSO - SSO Tenant Created in DBSnapper </p> <p>Congratulations you have successfully configured Okta OIDC for your DBSnapper organization.</p>"}, {"location": "dbsnapper-cloud/sso/sso-okta-oidc/#usage-instructions", "title": "Usage Instructions", "text": "<ul> <li>For SP Initiated Auth, visit the DBSnapper Cloud sign in page enter your email address, click Next, and then and click on the <code>Signn in with SSO</code> button.</li> </ul>"}, {"location": "sanitize/configuration/", "title": "Sanitize Configuration", "text": "<p>The DBSnapper Agent is configured using a YAML file, which is created when you run <code>dbsnapper config init</code> In this file you can specify multiple target configurations, each target being a set of options for a database you want to sanitize.</p> <p>Referring to our sample configuration file, the highlighted lines show the configuration options for a database sanitization:</p> <p><code>~/.config/dbsnapper/dbnsapper.yml</code> example</p> <pre><code>authtoken: 1234567890abcdef1234567890abcdef....\nworking_directory: /Users/snappy/.dbsnapper\ndocker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:16-alpine\nsecret_key: 1234567890abcdef1234567890abcdef\n# Target configurations\ntargets:\n  sakila:\n    name: sakila\n    # Snapshot configuration\n    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n    # Sanitization configuration\n    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: sakila-sanitize.sql\n    # Subsetting configuration\n    subset:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_subset?tls=false\n      subset_tables:\n        - table: sakila.film\n          where: \"film_id &lt; 20\"\n        - table: sakila.actor\n          percent: 20\n      copy_tables:\n        - sakila.store\n      excluded_tables:\n        - sakila.staff\n      added_relationships:\n        - fk_table: sakila.address\n          fk_columns: city_id\n          ref_table: sakila.city\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: sakila.store\n          ref_table: sakila.staff\n</code></pre>"}, {"location": "sanitize/configuration/#configuration-options", "title": "Configuration options", "text": "<p>Sanitization configuration options consist of a destination url <code>dst_url</code> that will be used to load a snapshot for sanitization and a query file <code>query_file</code> that contains the sanitization queries. When the sanitization is complete, a snapshot of the sanitized database will be created and stored in the working directory.</p> <p>Subset configuration options</p> <code>dst_url</code> <p>Connection string for the database where you will sanitize the snapshot (will be overwritten)</p> <code>query_file</code> <p>Name of the file containing the sanitization queries, located in your working directory (default: <code>~/.dbsnapper</code>)</p> <p>Danger: Destination database <code>dst_url</code> will be DROPPED and RECREATED</p> <p>Any connection string provided in the <code>dst_url</code> attribute will be overwritten when certain commands are used such as <code>load</code> and <code>sanitize</code> </p>"}, {"location": "sanitize/introduction/", "title": "Sanitization Introduction", "text": "<p>One of the main features of DBSnapper is the ability to sanitize a database. This allows you to remove sensitive information from your database, making it safe to use in development, testing, or analytics environments.</p>"}, {"location": "sanitize/introduction/#why-is-it-important", "title": "Why is it Important?", "text": "<ul> <li>Privacy Protection: In today's digital world, protecting personal information is not just ethical but also a legal requirement in many cases. Sanitization helps you respect and protect user privacy.</li> <li>Regulatory Compliance: Laws like GDPR and HIPAA require strict handling of personal data. Sanitization ensures that you stay on the right side of these regulations.</li> <li>Secure Development and Testing: Using real data in development and testing can be risky if it contains sensitive information. Sanitized data allows your team to work with realistic datasets without exposing personal information.</li> </ul>"}, {"location": "sanitize/introduction/#overview", "title": "Overview", "text": "<p>DBSnapper provides the <code>sanitize</code> command to sanitize a database, which applies a sanitization query to a snapshot of the database.</p>"}, {"location": "sanitize/introduction/#sanitize-command-steps", "title": "<code>sanitize</code> command steps", "text": "<ol> <li>Locate the desired snapshot in the working directory or pull it from the cloud (if configurec)</li> <li>Load the snapshot into the <code>dst_url</code> database</li> <li>Apply the sanitization query to the database</li> <li>Create a new snapshot of the sanitized database and store it in the working directory and push it to the cloud (if configured)</li> </ol> <p>Note</p> <p>In the near future we will be adding more capabilities to the sanitization process to make it even more powerful and flexible. Stay tuned for updates!</p>"}, {"location": "sanitize/sanitize/", "title": "Sanitize a Database", "text": "<p>Given the example configuration for the <code>sakila</code> target, we can sanitize the datbase with the snapshot we created in the Snapshot Build and Load example.</p> <p>Target: sakila - sanitize configuration</p> <pre><code>    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: \"sakila.san.sql\"\n</code></pre> <p>Here is our snapshot from the prior example:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nTables in target:  sakila\n...\nListing ALL snapshots for target: sakila\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n| INDEX |         CREATED         |       NAME        |       FILENAME        |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n|     0 | 2024-Feb-17 @ 23:14:50Z | 1708236890_sakila | 1708236890_sakila.zip | 981 kB | false      |       |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n</code></pre> <p>Now we're ready to sanitize this snapshot of the <code>sakila</code> target which can be done with the <code>sanitize</code> command:</p> <pre><code>dbsnapper sanitize sakila\n</code></pre> <p>Output:</p> <pre><code>DBSnapper Agent - Version: 2.0.0-alpha-dev Build Date: 2024-02-20T07:46:10-07:00\nDBSnapper Cloud: Enabled\n\nSTART: Sanitize Snapshot #0, Name: 1708236890_sakila\n-&gt; LOADING original Snapshot #0: Name: 1708236890_sakila, Snapshot File: 1708236890_sakila.zip, Dest DB URL: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n--&gt; Using engine: mysql-local\n--&gt; Using Target: sakila\n--&gt; Pulling to local file: /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Local snapshot already exists at /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Pulled snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Unzipping snapshot /Users/snappy/.dbsnapper/1708236890_sakila.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-1815162460\n--&gt; Dropping and recreating database myl://localhost:3306/sakila_sanitized\n-&gt; LOADING Snapshot Completed for Target: sakila\n--&gt; Executing sanitization query\n--&gt; Building sanitized snapshot\n--&gt; Zipping snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.san.zip\n--&gt; Sanitized snapshot '1708236890_sakila' created at /Users/snappy/.dbsnapper/1708236890_sakila.san.zip\nFINISHED Sanitizing snapshot\n</code></pre> <p>When this is finished you can list the <code>sakila</code> target snapshot:</p> <pre><code>dbsnapper target sakila\n</code></pre> <p>Output:</p> <pre><code>Listing ALL snapshots for target: sakila\n+-------+-------------------------+-------------------+-----------------------+--------+------------+---------------------------+\n| INDEX |         CREATED         |       NAME        |       FILENAME        |  SIZE  | SANITIZED? |           SANFN           |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+---------------------------+\n|     0 | 2024-Feb-17 @ 23:14:50Z | 1708236890_sakila | 1708236890_sakila.zip | 981 kB | true       | 1708236890_sakila.san.zip |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+---------------------------+\n</code></pre> <p>In this output, we now see that <code>SANITIZED?</code> column is <code>true</code> and the <code>SANFN</code> column shows the name of the sanitized snapshot file that is located in the working directory.</p>"}, {"location": "share/introduction/", "title": "Introduction", "text": "<p>One of the key DBSnapper features is the ability to easily share sanitized database snapshots with your team, developers, testers, and other stakeholders. Sharing snapshots is useful for providing sanitized data for development, testing, and other purposes without exposing sensitive information.</p> <p>By utilizing your own private cloud storage providers, you can securely store your snapshots in your own approved infrastructure and use the DBSnapper Agent and Cloud to facilitate easy sharing.</p>"}, {"location": "share/introduction/#sharing-architecture", "title": "Sharing Architecture", "text": "<p>The DBSnapper sharing architecture is designed to allow you to create snapshots of your database and share the unsanitized and sanitized snapshots in separate locations. You can then share the sanitized cloud storage location with your team and they can use the DBSnapper agent to access and load these snapshots.</p> <p> DBSnapper Architecture Overview </p>"}, {"location": "share/introduction/#getting-started", "title": "Getting Started", "text": "<p>To get started with using a cloud storage engine, you will need to configure a <code>storage_profile</code> in your DBSnapper configuration file. The <code>storage_profile</code> configuration specifies the cloud storage provider, credentials, and bucket information for the storage engine.</p> <p>AWS S3 configuration examples</p> <pre><code>storage_profiles:\n  s3-with-provided-credentials:\n    provider: s3\n    awscli_profile:\n    access_key: &lt;access_key&gt;\n    secret_key: &lt;secret_key&gt;\n    region: &lt;region&gt;\n    bucket: dbsnapper-test-s3\n    prefix:\n\n  s3-from-awscli-shared-profile:\n    provider: s3\n    awscli_profile: dbsnapper_credentials\n    bucket: dbsnapper-test-s3\n    prefix:\n\ntargets:\n  # Share target\n  shared-s3:\n    name: shared-s3\n    share:\n      storage_profile: s3-from-awscli-shared-profile\n      dst_url: postgres://localhost:5432/dbsnapper_test\n</code></pre> <p>In the example above we have two <code>storage_profiles</code> configurations for AWS S3. The first configuration <code>s3-with-provided-credentials</code> explicitly specifies the access key, secret key, region, and bucket for an S3 storage engine. <code>s3-from-awscli-shared-profile</code>, on the other hand, indicates that we want to retrieve the credentials from the <code>dbsnapper_credentials</code> AWS shared configuration profile as specified in the <code>awscli_profile</code> field.</p> <p>We have also defined a <code>share target</code> configuration on line 19 that uses the <code>s3-from-awscli-shared-profile</code> storage profile to access shared snapshots from the <code>dbsnapper-test-s3</code> bucket. These shared snapshots will be loaded into the <code>dbsnapper_test</code> database as specified in the <code>dst_url</code> field on line 23.</p>"}, {"location": "snapshot/build_and_load/", "title": "Build and Load", "text": "<p>Given the example configuration for the <code>sakila</code> target, we can build a snapshot and load it to the destination database.</p> <p>Target: sakila - snapshot configuration</p> <pre><code>    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n</code></pre>"}, {"location": "snapshot/build_and_load/#build-a-snapshot", "title": "Build a snapshot", "text": "<p>Now we're ready to create our first snapshot of the <code>sakila</code> target which can be done with the <code>build</code> command:</p> <pre><code>dbsnapper build sakila\n</code></pre> <p>Output:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nSTART: Build Snapshot for target: sakila with engine: mysql-local\n--&gt; Local target, Local storage, non-localhost DB.\n--&gt; Zipping snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.zip\nFINISH: Building DB Snapshot for target: sakila\n</code></pre> <p>When this is finished you can list all snapshots for the <code>sakila</code> target with:</p> <pre><code>dbsnapper target sakila\n</code></pre> <p>Output:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nTables in target:  sakila\n...\nListing ALL snapshots for target: sakila\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n| INDEX |         CREATED         |       NAME        |       FILENAME        |  SIZE  | SANITIZED? | SANFN |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n|     0 | 2024-Feb-17 @ 23:14:50Z | 1708236890_sakila | 1708236890_sakila.zip | 981 kB | false      |       |\n+-------+-------------------------+-------------------+-----------------------+--------+------------+-------+\n</code></pre> <p>And we see our new snapshot <code>1708236890_sakila</code> listed on line 10.</p>"}, {"location": "snapshot/build_and_load/#load-a-snapshot", "title": "Load a snapshot", "text": "<p>If a <code>dst_url</code> is defined in the target snapshot definition, you can load a snapshot to the destination using the index on the snapshot list. The <code>load</code> command will drop and recreate the destination database and restore the snapshot to the destination.</p> <p>Use the index of the snapshot to specify which snapshot to load. The default is snapshot index 0.</p> <pre><code>dbsnapper load sakila 0\n</code></pre> <p>Output:</p> <pre><code>DBSnapper CLI v2.0.0+35dda9f9.2024-02-16T06:24:43Z\nDBSnapper Cloud: Enabled\n\nSTART: Loading original Snapshot #0: Name: 1708236890_sakila, Snapshot File: 1708236890_sakila.zip, Dest DB URL: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n--&gt; Using engine: mysql-local\n--&gt; Using Target: sakila\n--&gt; Pulling to local file: /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Local snapshot already exists at /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Pulled snapshot 1708236890_sakila to /Users/snappy/.dbsnapper/1708236890_sakila.zip\n--&gt; Unzipping snapshot /Users/snappy/.dbsnapper/1708236890_sakila.zip to /var/folders/z5/n821ctqx34nb__xp15r69p9h0000gp/T/dbsnapper-2125850599\n--&gt; Dropping and recreating database myl://localhost:3306/sakila_snap\nFINISH: Loading Snapshot for Target: sakila\n</code></pre> <p>Here we see:</p> <ul> <li>In lines 4-6, we identify which snapshot we're using</li> <li>In lines 7-10, the snapshot is retrieved and unzipped to a temporary directory. Since it is already in the working directory, we don't have to <code>pull</code> it from the cloud</li> <li>In line 11, the destination database is dropped, recreated, and loaded with the snapshot</li> </ul>"}, {"location": "snapshot/configuration/", "title": "Snapshot Configuration", "text": "<p>The DBSnapper Agent is configured using a YAML file, which is created when you run <code>dbsnapper config init</code> In this file you can specify multiple target configurations, each target being a set of options for a database you want to snapshot.</p> <p>Referring to our sample configuration file, the highlighted lines show the configuration options for a snapshot:</p> <p><code>~/.config/dbsnapper/dbnsapper.yml</code> example</p> <pre><code>authtoken: 1234567890abcdef1234567890abcdef....\nworking_directory: /Users/snappy/.dbsnapper\ndocker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:16-alpine\nsecret_key: 1234567890abcdef1234567890abcdef\n# Target configurations\ntargets:\n  sakila:\n    name: sakila\n    # Snapshot configuration\n    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n    # Subsetting configuration\n    subset:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_subset?tls=false\n      subset_tables:\n        - table: sakila.film\n          where: \"film_id &lt; 20\"\n        - table: sakila.actor\n          percent: 20\n      copy_tables:\n        - sakila.store\n      excluded_tables:\n        - sakila.staff\n      added_relationships:\n        - fk_table: sakila.address\n          fk_columns: city_id\n          ref_table: sakila.city\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: sakila.store\n          ref_table: sakila.staff\n    # Sanitization configuration\n    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: sakila-sanitize.sql\n</code></pre>"}, {"location": "snapshot/configuration/#configuration-options", "title": "Configuration options", "text": "<p>As you can see, the configuration options are quite simple, consisting of a database source url <code>src_url</code> and a destination url <code>dst_url</code>.</p> <p>Subset configuration options</p> <code>src_url</code> <p>Connection string for the database you want to snapshot  </p> <code>dst_url</code> <p>Connection string for the database where you want to restore the snapshot.</p> <p>Danger: Destination database <code>dst_url</code> will be DROPPED and RECREATED</p> <p>Any connection string provided in the <code>dst_url</code> attribute will be overwritten when certain commands are used such as <code>load</code> which loads a snapshot to the destination database.</p>"}, {"location": "snapshot/introduction/", "title": "Introduction", "text": "<p>Snapshotting your database is a fundamental feature of DBSnapper, allowing you to create a point-in-time snapshot or backup of your database. Snapshotting takes a full copy of your database, including the schema and data, and stores it in a compressed file. This snapshot can then be used to restore your database to the state it was in when the snapshot was taken.</p>"}, {"location": "snapshot/introduction/#overview", "title": "Overview", "text": "<p>DBSnapper provides the <code>build</code> and <code>load</code> commands to create and restore a snapshot of your database.</p>"}, {"location": "snapshot/introduction/#build-command-steps", "title": "<code>build</code> command steps", "text": "<ol> <li>Connect to database specified by the <code>src_url</code> in the target definition</li> <li>Invoke the database vendor's database dump command to create a snapshot of the database</li> <li>Compress the snapshot file and store it in the working directory</li> </ol>"}, {"location": "snapshot/introduction/#load-command-steps", "title": "<code>load</code> command steps", "text": "<ol> <li>Find the requested snapshot file in the working directory</li> <li>Decompress the snapshot file</li> <li>Connect to the database specified by the <code>dst_url</code> in the target definition</li> <li>Drop and recreate the database</li> <li>Invoke the database vendor's database restore command to restore the snapshot to the database</li> </ol>"}, {"location": "snapshot/introduction/#required-dependencies", "title": "Required Dependencies", "text": "<p>DBSnapper uses database vendor tools to perform snapshot operations.</p> Database Vendor <code>build</code> command <code>load</code> command queries PostgreSQL <code>pg_dump</code> <code>pg_restore</code> <code>psql</code> MySQL <code>mysqldump</code> <code>mysql</code> <code>mysql</code>"}, {"location": "subset/configuration/", "title": "Subset Configuration", "text": "<p>The DBSnapper Agent is configured using a YAML file, which is created when you run <code>dbsnapper config init</code> In this file you can specify multiple target configurations, each target being a set of options for a database you want to subset.</p> <p>Referring to our sample configuration file, the highlighted lines show the configuration options for a database subsetting:</p> <p><code>~/.config/dbsnapper/dbnsapper.yml</code> example</p> <pre><code>authtoken: 1234567890abcdef1234567890abcdef....\nworking_directory: /Users/snappy/.dbsnapper\ndocker:\n  images:\n    mysql: mysql:8-oracle\n    postgres: postgres:16-alpine\nsecret_key: 1234567890abcdef1234567890abcdef\n# Target configurations\ntargets:\n  sakila:\n    name: sakila\n    # Snapshot configuration\n    snapshot:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_snap?tls=false\n    # Sanitization configuration\n    sanitize:\n      dst_url: mysql://root:mysql@localhost:3306/sakila_sanitized?tls=false\n      query_file: sakila-sanitize.sql\n    # Subsetting configuration\n    subset:\n      src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n      dst_url: mysql://root:mysql@localhost:3306/sakila_subset?tls=false\n      subset_tables:\n        - table: sakila.film\n          where: \"film_id &lt; 20\"\n        - table: sakila.actor\n          percent: 20\n      copy_tables:\n        - sakila.store\n      excluded_tables:\n        - sakila.staff\n      added_relationships:\n        - fk_table: sakila.address\n          fk_columns: city_id\n          ref_table: sakila.city\n          ref_columns: id\n      excluded_relationships:\n        - fk_table: sakila.store\n          ref_table: sakila.staff\n</code></pre>"}, {"location": "subset/configuration/#configuration-options", "title": "Configuration options", "text": "<p>The subset configuration for a target is rather involved and provides a number of options to control the subsetting process. The following is a list of the subset configuration options:</p>"}, {"location": "subset/configuration/#subset-connection-strings", "title": "Subset Connection Strings", "text": "<pre><code>subset:\n  src_url: mysql://root:mysql@localhost:13306/sakila?tls=false\n  dst_url: mysql://root:mysql@localhost:3306/sakila_subset?tls=false\n</code></pre> <code>subset</code> Subset configuration for the target <code>src_url</code> Connection string of source database <code>dst_url</code> Connection string for the database where the subset will be created (will be overwritten)"}, {"location": "subset/configuration/#subset-tables", "title": "Subset Tables", "text": "<pre><code>subset_tables:\n  - table: sakila.film\n    where: \"film_id &lt; 20\"\n  - table: sakila.actor\n    percent: 20\n</code></pre> <code>subset_tables</code> <p>List of tables to be subsetted. The subset tables are the initial tables of interest, and the <code>where</code> and <code>percent</code> clauses can be used to control the portion of each table to be included in the subset. Rows from other tables are included as needed to maintain referential integrity with the subset tables.</p> <code>table</code> The name of the table to be subsetted. All tables are specified in the format <code>schema.table</code>. <code>where</code> Providing a <code>where</code> clause will subset the table based on the condition specified. <code>percent</code> Specifying the <code>percent</code> clause will take a random sample of the table based on the percentage specified. <p><code>subset_tables</code> clauses</p> <p>One (and only one) of the <code>where</code> or <code>percent</code> clauses must be provided for each table in the <code>subset_tables</code> list.</p>"}, {"location": "subset/configuration/#tables-to-copy-or-exclude", "title": "Tables to Copy or Exclude", "text": "<pre><code>copy_tables:\n  - sakila.store\nexcluded_tables:\n  - sakila.staff\n</code></pre> <code>copy_tables</code> List of tables to be copied in whole to the subset database (<code>dst_url</code>). These tables are copied as-is from the source database to the subset database. <code>excluded_tables</code> List of tables to be excluded from the subset database (<code>dst_url</code>). These tables are not copied to the subset database."}, {"location": "subset/configuration/#defining-relationships", "title": "Defining Relationships", "text": "<pre><code>added_relationships:\n  - fk_table: sakila.address\n    fk_columns: city_id\n    ref_table: sakila.city\n    ref_columns: id\nexcluded_relationships:\n  - fk_table: sakila.store\n    ref_table: sakila.staff\n</code></pre> <code>added_relationships</code> <p>List of table relationships to be considered. These relationships are added to the configuration when a set of tables have a foreign key relationship that is not defined in the database schema via foreign key constraints.</p> <p>Each <code>added_relationships</code> entry is defined by the <code>fk_table</code>, <code>fk_columns</code>, <code>ref_table</code>, and <code>ref_columns</code> attributes.</p> <code>excluded_relationships</code> <p>List of table relationships to be excluded. A relationship should be excluded when a circular dependency is detected in the database schema. This can occur when a table has a foreign key relationship to another table that also has a foreign key relationship back to the original table.</p> <p>Each <code>excluded_relationships</code> entry is defined by the <code>fk_table</code> and <code>ref_table</code> attributes.</p>"}, {"location": "subset/introduction/", "title": "Introduction", "text": "<p>Database Subsetting is a new feature released in v2.0 of the DBSnapper Agent. Database subsetting is the process of creating a smaller, yet fully functional version of a large database. This smaller subset retains all the critical relationships and structures of the original database but is more manageable in size.</p> <p>While this would seem to be a simple task involving a simple SQL query to select the rows of interest, the main challenge is to ensure that the subset of data is relationally consistent such that all the relationships between the database tables are maintained. For example, if you have a table of <code>racers</code> and a table of <code>race_results</code>, you need to ensure that the relevant <code>race_results</code> are included for each <code>racer</code> record in the subset.</p>"}, {"location": "subset/introduction/#overview", "title": "Overview", "text": "<p>DBSnapper provides the <code>subset</code> command to create a subset of a database. The subset command relies on the subsetting configuration specified for the target database and will be dicussed in a following section.</p> <p>Database Subsetting Terminology</p> Database Subset A smaller, yet fully functional version of a large database. Subset Table The primary tables of interest in the subset. These are the tables that are the main focus of the subset and from which all other tables are derived. Upstream Table Also known as a <code>parent</code> table, the <code>upstream</code> table has a foreign_key column that references a primary key in a <code>downstream</code> or <code>child</code> table. Upstream tables, therefore have a dependency on the downstream table that they reference. Downstream Table Also known as a <code>child</code> table, the <code>downstream</code> table has a primary key column that is referenced by a foreign_key column in an <code>upstream</code> table. Copy Table A table that is copied in its entirety to the subset. This is useful for tables that are not directly related to the subset tables but are required for the subset to function correctly."}, {"location": "subset/introduction/#steps-to-create-a-database-subset", "title": "Steps to Create a Database Subset:", "text": "<p>The high-level steps to create a datbase subset are as follows:</p> <ol> <li> <p>Analyze the Database - DBSnapper will first analyze the database and determine the set of tables that should be included in the subset based on the selected Subset Tables and the relationships between the tables in the database schema as determined by the foreign key constraints defined in the database schema.</p> </li> <li> <p>Process the Subset Tables - DBSnapper will first create copies of the subset tables based on the criteria specified in the <code>percent</code> or <code>where</code> attributes of the subsetting configuration. In the case of <code>percent</code> we take a random sample of the specified percentage of the table. In the case of <code>where</code> we take a subset of the table based on the specified <code>where</code> clause.</p> </li> <li> <p>Process the Upstream Tables - For each upstream table that has a foreign key reference to a subset table, DBSnapper will copy the records from the upstream table that are related to the records copied in the \"Process the Subset Tables\" step. This process is then repeated for any new upstream tables that are created as a result of this step.</p> </li> <li> <p>Transfer the Copy Tables DBSnapper provides a configuration option to specify tables that you would like to have copied in their entirety to the subset. This is useful for tables that are not directly related to the subset tables but are required for the subset to function correctly. These tables are copied to the destination subset at this time.</p> </li> <li> <p>Process the Downstream Tables - For each <code>downstream</code> table that is refenced by a table in the subset, copy the records from the <code>downstream</code> table that are referenced by the upstream table into the destination subset.</p> </li> </ol>"}, {"location": "subset/introduction/#table-constraints", "title": "Table Constraints", "text": "<p>Identifying the relationships between the tables in the database is a critical part of the subsetting process. DBSnapper can automatically determine relationships if they are defined in the table (relation) schema. Depending on the database, These relationships are not always defined, so it is also possible to define the relationships manually in the DBSnapper configuration.</p> <p>The table relationship information is used to determine the order in which the tables are processed and to ensure that the subset is relationally consistent.</p>"}, {"location": "subset/introduction/#automatic-relationships", "title": "Automatic Relationships", "text": "<p>DBSnapper will automatically determine relationships between tables based on foreign key constraints defined in the table (relation) schema if they are defined.</p>"}, {"location": "subset/introduction/#specifying-relationships-explicitly", "title": "Specifying Relationships Explicitly", "text": "<p>If the relationships are not defined in the table schema, you can define the relationships manually in the DBSnapper configuration in the <code>added_relationships</code> section.</p> <pre><code>added_relationships:\n  - fk_table: sakila.address\n    fk_columns: city_id\n    ref_table: sakila.city\n    ref_columns: id\n</code></pre> <p>In this example we are definiing a relationship between the <code>sakila.address</code> and <code>sakila.city</code> tables.</p> <p>The <code>sakila.address</code> table is the upstream table with the foreign key reference to the <code>sakila.city</code> table. The <code>sakila.city</code> table is the downstream table with the primary key.</p>"}, {"location": "subset/introduction/#circular-references", "title": "Circular References", "text": "<p>Circular references are situations where a table has a foreign key relationship to another table that also has a foreign key relationship back to the original table, creating a cycle in the relationship graph and an infinite loop in the subsetting process.</p> <p>Self-Referencing Tables</p> <p>Circular references are also present when a table has a reference to itself. </p> <p>If a circular reference is detected, the subsetting process will stop and an error will be generated. You must then identify where the circular reference should be broken and exclude the relationship from the subsetting process. You can specify the relationships to be excluded in the <code>excluded_relationships</code> section of the DBSnapper configuration:</p> <pre><code>excluded_relationships:\n  - fk_table: sakila.store\n    ref_table: sakila.staff\n</code></pre> <p>In this example we are excluding the relationship between the <code>sakila.store</code> and <code>sakila.staff</code> tables. Any foreign_key values in the <code>sakila.store</code> table that reference the <code>sakila.staff</code> table will be set to <code>NULL</code> to break the circular reference.</p>"}]}